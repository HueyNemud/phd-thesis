\chapter{Construction du graphe géohistorique}

\renewcommand{\labelitemi}{\tiny$\blacksquare$}
\large \begin{mybox} \normalsize
\textbf{Objectifs~:} 
\vspace{11pt}
\begin{itemize}
\item  Proposer une méthode permettant de construire de façon semi-automatique un graphe géohistorique à partir d'un ensemble d'observations vectorielles localisées dans le temps.
\item  Implémenter et expérimenter la méthode sur les réseaux viaires de Paris extraits par vectorisation des atlas parisiens.
\end{itemize}
\end{mybox}
\renewcommand\labelitemi{\textbullet}% bullet
\newpage
\minitoc
\newpage
Nous disposons une base de données spatiale et temporelle stockant des observations géohistoriques sous forme vecteur, temporalisées et organisées par source, au dessus de laquelle est défini un modèle de graphe géohistorique permettant de représenter les transformations entre ces différentes observations. Ce graphe, issu d'un modèle de graphe spatio-temporel de la littérature, doit être instancié à partir des différentes observations. Nous avons expliqué dans le chapitre 5 que cette instanciation impliquait de connaître par avance les liens de filiation existant entre les différentes observations stockées. Or, dans notre cas, ceux-ci ne sont pas connus a priori et doivent être identifiés. Dans ce but, nous proposons dans ce chapitre une approche permettant de retrouver ces liens de filiation, ce qui permet ensuite d'instancier un graphe géohistorique. Cette approche, présentée de façon globale en section~\ref{section:appgen}, se compose de deux étapes.
\\
Tout d'abord, les observations dont les identités sont liées sont découvertes par un processus d'appariement de données géographiques, ce qui produit un ensemble de liens de filiations. L'approche est présentée en détails en section~\ref{section:decouverte}.
\\
Dans un second temps (cf. section \ref{section:tagging}) , le type de ces liens de filiation -dérivation ou continuation- est précisé à la volée à partir d'une définition de l'identité par l'utilisateur. Ainsi, cela nous permet de produire une instanciation du graphe géohistorique tel que nous l'avons défini dans le chapitre précédent.
\\
L'approche complète est appliquée aux réseaux viaires de Paris en section ~\ref{section:application} dans le but de reconstituer le graphe géohistorique retraçant les transformations des rues de la ville. Cette phase nous permet de valider l'approche et ainsi de finaliser l'ensemble des étapes permettant de créer une base de données spatio-temporelle sur l'espace urbain à partir de sources cartographiques anciennes.

\newpage
\section{Approche générale}
\label{section:appgen}
Le graphe spatio-temporel sur lequel s'appuie notre graphe géohistorique est une structure inférée à partir d'une base de données stockant des entités, leurs identités et les relations de dérivation entre entités. Dans notre cas, l'identité des observations n'est pas définie au moment de la création de la base de données spatiale et temporelle dans la mesure où cette identité dépend de la nature de l'observation et du point de vue de l'utilisateur qui conçoit le graphe géohistorique. Par conséquent, les relations de filiation (dérivation ou continuation) ne sont pas connues a priori et il n'est pas possible d'inférer directement un graphe géohistorique à partir d'une base de données spatiale et temporelle. Il est toutefois nécessaire de connaître ces relations de filiation pour détecter les processus spatio-temporels existants entre observations issues de diverses sources géohistoriques. \textbf{Afin de retrouver les relations de filiation, il faut détecter les observations dont les identités sont liées.} Pour atteindre cet objectif, nous avons besoin d'une façon d'exprimer l'identité de nos observations géohistoriques, ainsi que d'une approche permettant, à partir d'une définition de l'identité, de détecter les relations de dérivation et continuation existant entre ces observations.
\\
Nous présentons dans cette section les grandes étapes de l'approche que nous proposons pour atteindre ce but. Nous introduisons de plus quelques notions fondamentales sur lesquelles repose notre approche.

\subsection{Schéma général}
L'approche que nous proposons pour la construction d'un graphe géohistorique à partir d'un ensemble d'observations est résumée dans la figure~\ref{figure:schema_construction}. La présentation des deux étapes principales de l'approche qui apparaissent en rouge constitue le c\oe ur de ce chapitre. Nous décrivons ici l'enchaînement général des étapes ainsi que les interactions avec l'utilisateur lors de la construction du graphe.
\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{schema_general.png}
\caption{Schéma général décrivant l'instanciation d'un graphe géohistorique à partir d'un ensemble d'observations.}
\label{figure:schema_construction}
\end{figure}

\myparagraph{Sélection des observations à lier et alignement des schémas}
La base de données spatiale et temporelle stocke un ensemble d'observations géohistoriques pouvant être de nature variées. Ainsi, pour l'espace urbain, il peut s'agir d'observation de rues, de bâtiments, d'îlots, voire de données sociales géolocalisées (mouvements de foule, barricades, manifestations, etc.). Construire un graphe géohistorique  entre des observations de nature aussi variée n'a pas toujours de sens ou n'est pas toujours pertinent pour une étude particulière\footnote{Une rue n'est pas en relation de filiation avec une manifestation. Toutefois, on peut considérer qu'une rue peut être en relation de filiation avec un bâtiment (afin d'analyser le remembrement du bâti du aux percements par exemple).}. Cette étape de sélection correspond donc simplement à une sélection de données dans la base spatiale et temporelle. 
\\
Puisque la découverte de relations de filiation s'appuie sur la ressemblance entre les identités des observations qui composent le graphe, il faut que ces identités puissent être comparées. Celles-ci étant exprimées comme un ensemble d'attributs dépendant du \emph{feature type} de l'observation, comparer deux identités signifie que les \emph{feature types} des observations soient alignés. La plupart des approches proposées en géomatique pour aligner différents schémas de données géographiques reposent sur l'utilisation d'une ontologie pivot~\citep[p.50]{Abadie2012},\citep{Euzenat2007}. Cette ontologie support fournit un ensemble de concepts qui permettent de représenter les différents schémas et de déduire les relations de correspondance entre éléments de schémas à partir de ces annotations.
\\
Cette ontologie dépend donc largement de la nature des observations stockées. L'appariement de schémas est un domaine à part. Dans le cadre de cette thèse, nous avons choisi de ne pas considérer l'alignement de schémas pour nous concentrer sur les étapes de construction du graphe géohistorique. En effet, étant donnée les données vectorisées, cet alignement a pu être effectué manuellement sans difficultés. Pour cette raison, nous considérons toujours que les \emph{feature types} des observations sont alignés et donc que leurs identités sont comparables. 
\\
À la fin de cette étape, nous disposons d'un ensemble d'observations géohistoriques issues de sources différentes et dont les \emph{feature types} sont alignés.

\myparagraph{Découverte des relations de filiation}
Cette étape constitue la première partie du processus permettant la découverte des relations de filiation entre les observations sélectionnées. La découverte des relations de filiation, présentée en détail dans la section~\ref{section:decouverte}, est un processus automatique permettant de détecter les observations dont les identités sont liées qui sont membres d'un des processus spatio-temporels introduits dans le chapitre 5.
\\
Afin que la découverte des relations de filiation soit possible, il faut disposer d'une définition de l'identité des observations, c'est-à-dire d'un ensemble d'\emph{attribute types} définissant quels attributs de chaque observation sont membres de son identité. La définition de l'identité dépend de la nature des observations mais également du point de vue de l'utilisateur créant le graphe géohistorique\footnote{Un utilisateur peut par exemple considérer que la forme et la localisation d'une rue constituent son identité, tandis qu'un autre peut également inclure le nom de la rue comme membre de l'identité. Dans le second cas, une rue changeant de nom impliquera un changement d'identité et donc la présence de deux rues affiliées. Dans le premier cas, il s'agit de la même rue qui continue à exister}.
\\
L'étape de découverte des relations de filiation permet de créer un premier graphe de filiation dans lequel les observations ayant des identités liées sont identifiées. 

\myparagraph{Typage des relations de filiation}
Une fois les observations liées identifiées, il faut déterminer si cette relation implique un changement d'identité ou si les identités des observations sont suffisamment proches pour que l'on considère qu'il s'agit de deux observations de la même entité du monde réel. Cela revient donc, pour chaque relation de filiation découverte, à la typer selon qu'il s'agit d'une continuation ou d'une dérivation.
\\
Nous séparons cette étape de celle de la découverte des relations. En effet, les observations étant imparfaites, il peut être difficile de déterminer la différence entre continuation et dérivation\footnote{Par exemple, les erreurs planimétriques créent des décalages qui gênent l'identification des continuations.}. De plus, la limite entre continuation et dérivation dépend ici encore de l'utilisateur\footnote{Est-ce que deux observations d'une rue ayant été élargie de quelques mètres doivent être en relation de dérivation ou de continuation? Si l'utilisateur s'intéresse aux alignements des rues qu'il souhaite détecter et analyser, la relation doit être une dérivation. S'il s'intéresse aux transformations plus importantes, la relation peut être une continuation.}.
\\
Le typage des relations de filiation issues de l'étape de découverte nécessite de disposer d'une définition fine de l'identité pour distinguer continuations et dérivations. En entrée de cette phase de typage, il faut donc que l'utilisateur précise sa définition de l'identité afin de pouvoir créer le graphe géohistorique final.
\\
Une fois l'étape de typage terminée, nous disposons d'un graphe géohistorique complet sur l'ensemble des observations sélectionnées dans la base de données spatiale et temporelle.


\subsection{Notions fondamentales}
\myparagraph{Formalisation d'une observation géohistorique}
Pour la suite de ce chapitre, nous avons besoin de définir une observation géohistorique de façon formelle. Une observation est stockée dans la base de données spatiale et temporelle sous la forme d'un \emph{feature} géographique, c'est-à-dire un n-uplet d'attributs dont les natures sont définies par un ensemble d'\emph{attribute types}. Chaque observation est décrite par un \emph{feature type} qui contient la liste des \emph{attribute types} de l'observation.
\\
Notons $\mathcal{U}$ l'union des domaines de valeurs de tous les \emph{attribute types} de la base de données et $O$ un ensemble d'observations issues de la base de données spatiale et temporelle. Nous introduisons une fonction $U_i$ qui associe à toute observation la valeur de l'attribut correspondant à son i\textsuperscript{ème} \emph{attribute type}~:
\begin{table}[ht]
\centering
\begin{tabular}{r c l }
$U_i : O$& $\to$ & $\mathcal{U}$ \\
$U_i(o)$& $\mapsto$ &$ u_i$\\
\end{tabular}
\end{table}
Une observation géohistorique $o \in O$ peut donc être écrite
\begin{center}
 $o = (U_1(o),U_2(o),...U_n(o), U_g(o),U_t(o),U_s(o))$ 
\end{center}
où~:
\begin{itemize}
\item $\{U_1(o),U_2(o),...U_n(o)\}$ est un ensemble d'attributs thématiques quelconques pris dans $\mathcal{U}$,
\item $U_g(o)$ est sa représentation géométrique conforme au standard OGC \emph{Simple Feature Access}\footnote{Qui décrit les primitives géométriques simples du type point, courbes et surfaces ainsi que les types géométriques composés de plusieurs primitives. Voir~: \url{http://www.opengeospatial.org/standards/sfs}.},
\item $U_t(o)$ est le temps valide de l'observation représenté par un sous-ensemble flou,
\item $U_s(o)$ est la source géohistorique de laquelle provient l'observation.
\end{itemize}
$U_g, U_t$ et $U_s$ correspondent respectivement à $U_{n+1},U_{n+2}$ et $U_{n+3}$. Nous les notons $g,t,s$ pour des raisons de lisibilité. 
\myparagraph{Observations agrégées}
Nous introduisons ici la possibilité de créer des observations agrégées à partir de plusieurs autres observations. Ces observations agrégées sont utilisées dans l'étape de découverte des relations de filiation. \textbf{Une observation agrégée est constituée de l'agrégation des attributs des observations qui la composent}. Par souci de simplicité, nous considérons que ces observations sont également représentables dans la base de données, c'est-à-dire qu'elles appartiennent également à $O$.
\\
Pour donner une définition de ces observations, nous devons étendre la définition de $U_i$ à plusieurs observations. On pose~:
\begin{table}[ht]
\centering
\begin{tabular}{r c l }
$U_i : \bigcup\limits_{k=0}^{\infty}O^k$& $\to$ & $\mathcal{U}$ \\
$U_i(o_1,\dots,o_j)$& $\mapsto$ &$ \Psi(U_i(o_1),U_i(o_2),\dots,U_i(o_j))$\\
\end{tabular}
\end{table}
La fonction $\Psi$ est une fonction d'agrégation définie par~:
\begin{table}[ht]
\centering
\begin{tabular}{r c l }
$\Psi : \bigcup\limits_{k=0}^{\infty}U_i^k$& $\to$ & $\mathcal{U}$ \\
$\{U_{i_1},U_{i_2},\dots,U_{i_j}\}$& $\mapsto$ &$ u_i$\\
\end{tabular}
\end{table}
Le comportement de $\Psi$ dépend du \emph{feature attribute} correspondant à $U_i$. Par exemple, l'agrégation de géométries est une opération différente de l'agrégation de chaînes de caractères. Ces fonctions d'agrégation sont propres au type d'observation et aux choix de représentation adoptés pour les différents attributs de ces observations. Nous en conservons donc ici une notation générique.
\\
Finalement, une observation agrégée $o_{p\dots q} \in O$ formée des observations $\{o_p,\dots,o_q\}$ telle que $\forall o,o' \in \{o_p,\dots,o_q\}, U_s(o) = U_s(o')$ est de la forme~:
\begin{center}

$o_{p\dots q}=(U_1(o_p,\dots,o_q),\dots,U_n(o_p,\dots,o_q),U_g(o_p,\dots,o_q),U_t(o_p,\dots,o_q),U_s(o_p,\dots,o_q))$ 
\end{center} 
%Puisque les observations agrégées peuvent être construites à partir d'observations d'une seule source géohistorique, on écrira que toute observation agrégée $o_{p,\dots,q}$ formée d'observations d'une sources $S_i$ est issue de la source $\mathcal{S}_i = \bigcup\limits_{k=1}^{\infty}S_i^k$.  $\mathcal{S}_i$ représente toutes 
La figure \ref{figure:obs_agregee} présente un exemple simple d'une observation agrégée définie sur deux observations de parcelles cadastrales possédant comme attribut thématique un numéro.
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{obs_agregee.png}
\caption{Un exemple d'observation agrégée.}
\label{figure:obs_agregee}
\end{figure}

\myparagraph{Identité d'une observation géohistorique}
Nous avons déjà défini de façon générale l'identité d'une observation comme un ensemble de caractéristiques qui la distingue des autres. Ici, nous considérons que l'identité d'une observation géohistorique stockée comme une \emph{feature} géographique est \textbf{un sous-ensemble de ses attributs thématiques et géométriques qui la distingue des autres observations issues de la même source géohistorique}. Cette identité peut être mise à profit pour retrouver au sein d'autres sources géohistoriques des observations présentant la même identité et étant donc d'autres représentations de la même entité du monde réel. Ainsi, le temps valide n'est pas un élément constitutif de l'identité. En effet, deux sources géohistoriques de même temps valide peuvent décrire deux observations de la même entité du monde réel. Nous pouvons maintenant préciser la fonction d'identité $i$ posée dans le paragraphe \ref{par:identite} du chapitre 5. 
\\
L'identité d'une observation $o=(U_1(o),\dots,U_n(o), U_g(o),U_t(o),U_s(o)) \in O$, notée $I_o$, est définie par~:
\begin{table}[ht]
\centering
\begin{tabular}{r c l }
$i : O$& $\to$ & $\mathcal{I}$ \\
$i(o)$& $\mapsto$ &$ I_o \subseteq (U_1(o),\dots,U_n(o),U_g(o)) $\\
\end{tabular}
\end{table}
où $\mathcal{I}$ est l'ensemble des identités exprimables dans une base de données spatiale et temporelle.

\myparagraph{Hypergraphe de filiation}
L'existence d'une relation de filiation entre deux observations n'a de sens que parce qu'elle décrit une partie ou la totalité d'un des processus spatio-temporels que nous avons définis. Découvrir des relations de filiations revient finalement à découvrir les processus spatio-temporels à l'\oe uvre entre les différentes observations. Pour ce faire, il faut disposer d'une structure modélisant ces processus spatio-temporels. Pour cela, nous introduisons au dessus d'un graphe de filiation $G_f(O,E_f,S,\mathcal{T})$ un \textbf{hypergraphe orienté $H_f(O \cup o_\emptyset, E, S, \mathcal{T})$ nommé hypergraphe de filiation} où~:
\begin{itemize}
\item $o_\emptyset$ est l'observation nulle, décrite dans la suite du paragraphe,
\item $E$ est un ensemble d'hyperarcs décrivant les processus spatio-temporels que l'on peut extraire de $G_f$,
\item $S$ est l'ensemble des sources géohistoriques des observations de $O$,
\item $\mathcal{T}$ est le domaine temporel flou.
\end{itemize}
Chaque type de processus spatio-temporel peut être décrit par un hyperarc. Un hyperarc $e \in E $ est un couple formé de deux sous-ensembles disjoints d'observations géohistoriques $(Q,T)$. Le sous-ensemble $Q$ est la \textbf{queue} de l'hyperarc, et $T$ sa \textbf{tête}. Dans la suite de ce mémoire, nous écrirons $Q(e)$ et $T(e)$ les fonctions renvoyant respectivement la queue et la tête de l'hyperarc $e$.
\\
 Le tableau~\ref{table:hyperarcs} présente la modélisation sous forme d'hyperarc de chaque processus spatio-temporel. Les cas des processus d'apparition et de disparition posent problème car ils correspondent à l'absence de relation de filiation. Pour qu'ils soient modélisables également au sein de $H_f$, nous introduisons une \textbf{observation nulle} $o_\emptyset =\emptyset$ virtuelle. Toute observations appartient ainsi à deux hyperarcs, l'un décrivant ce qui la précède, et l'autre ce qui lui succède. Il y a apparition ou disparition d'une observation si celle-ci est en relation de filiation avec cette observation nulle. Un exemple d'hypergraphe de filiation construit à partir du graphe de filiation $G_f$ sur les parcelles cadastrales (voir le chapitre 5) est illustré en figure~\ref{figure:exemple_hypergraphe}.
%
\begin{table}
\caption{Modélisation des processus spatio-temporels sous la forme d'hyperarcs}
\label{table:hyperarcs}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Graphe de filiation} & \textbf{Processus spatio-temporel} & \textbf {Hyperarc} \\ 
\hline
\includegraphics[]{./tableau_process/apparition_graphe.png} & Apparition & \includegraphics[]{./tableau_process/apparition_hg.png} \\
\hline
\includegraphics[]{./tableau_process/disparition_graphe.png} & Disparition & \includegraphics[]{./tableau_process/disparition_hg.png} \\
\hline
\includegraphics[]{./tableau_process/cont_deriv_graphe.png} & Continuation, Dérivation & \includegraphics[]{./tableau_process/cont_deriv_hg.png} \\
\hline
\includegraphics[]{./tableau_process/fusion_annexion_graphe.png} & Fusion, Annexion & \includegraphics[]{./tableau_process/fusion_annexion_hg.png} \\
\hline
\includegraphics[]{./tableau_process/scission_separation_graphe.png} & Scission, Séparation & \includegraphics[]{./tableau_process/scission_separation_hg.png} \\
\hline
\includegraphics[]{./tableau_process/reallocation_graphe.png} & Réallocation & \includegraphics[]{./tableau_process/reallocation_hg.png} \\
\hline
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{exemple_hypergraphe.png}
\caption{Un hypergraphe de filiation représentant les processus spatio-temporels d'un graphe de filiation.}
\label{figure:exemple_hypergraphe}
\end{figure}


\myparagraph{Hypergraphes et graphes de filiation admissibles}
\label{paragraph:admissible}
Nous avons posé quelques contraintes sur les processus spatio-temporels dans le chapitre 5. Nous devons redéfinir ces contraintes sur l'hypergraphe de filiation. Nous introduisons également une contrainte de Berge-acyclicité\footnote{La notion de graphe acyclique sur un hypergraphe est plus complexe que pour un graphe simple et n'est pas unique. Il en existe 4 types, dont le détail peut être trouvé dans \cite{Duris2009}.} sur cet hypergraphe.
\\
Un hypergraphe de filiation $H_f(O\cup o_\emptyset,E,S,\mathcal{T})$ est admissible s'il respecte les contraintes suivantes~:
\begin{enumerate}
\item[$C_1$ :] \textbf{Processus multisource}.  Pour tout $e\in E$, toutes les observations de $Q(e)$ proviennent de la même source géohistorique $S_i$ et toutes les observations de $T(e)$ provienne d'une seconde source géohistorique $S_j$ avec $i \neq j$.
\item[$C_2$ : ] \textbf{Relations rétro-temporelles}.  Pour tout $e\in E$ avec $o_Q$ l'observation agrégée formée des observations $Q(e)$ et $o_T$ l'observation agrégée formée de $T(e)$, on a $ant(U_t(o_Q),U_t(o_T)) >0$.
\item[$C_3$ :]\textbf{Berge-acyclicité}. Un Berge-cycle dans un hypergraphe de filiation est une suite $(e_1, o_1,\dots, e_n,o_n)$ telle que~:
\begin{itemize}
\item[.] les hyperarcs $e_i$ sont distincts,
\item[.] les sommets $o_i$ sont distincts,
\item[.] pour tout $i$, $o_i$ appartient à $T(e_i)$ et à $Q(e_{i+1})$
\end{itemize}
Un hypergraphe Berge-acyclique est un hypergraphe sans Berge-cycle.
\end{enumerate}
Des exemples d'hypergraphes non admissibles sont visibles en figure~\ref{figure:gh_non_admissibles}.
Notons que la contrainte $C_1$ empêche également la création de boucles dans l'hypergraphe.
\\
Un graphe de filiation est admissible si l'hypergraphe de filiation que l'on peut construire à partir de ses relations est admissible.
\\
Dans la suite du document, nous utiliserons par souci de simplicité le terme ''arc'' pour désigner un hyperarc de $H_f$ lorsqu'il n'y a pas ambiguïté.
\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{non_admissibles_hg.png}
\caption{Exemples d'hypergraphes de filiation non admissibles.}
\label{figure:gh_non_admissibles}
\end{figure}

\myparagraph{Fonctions de vraisemblance d'un hyperarc de filiation}
Pour découvrir les relations de filiation au sein d'un ensemble d'observations, il faut être capable de déterminer si les processus spatio-temporels que l'on peut extraire à partir de ces relations correspondent à des \textbf{transformations vraisemblables des entités du monde réel sous-jacentes}.
\\
Cela revient à juger de la vraisemblance d'un hyperarc dans $H_f$. Dans ce but, nous définissons une \textbf{fonction de vraisemblance d'un hyperarc de filiation}\footnote{La vraisemblance telle que nous la définissons ici n'a pas de rapport direct avec la notion de vraisemblance statistique. } $\Phi$ définie pour tout hyperarc $e\in E$ d'un hypergraphe de filiation $H_f(O\cup o_\emptyset,  E, S, \mathcal{T})$ par~:
\begin{table}[ht]
\centering
\begin{tabular}{r c l }
$\Phi : E$& $\to$ & $[0,1]$ \\
$\Phi(e)$& $\mapsto$ &$ \phi(o_Q,o_T)$\\
\end{tabular}
\end{table}
avec~:
\begin{itemize}
\item $o_Q$ l'observation agrégée à partir des observations de $Q(e)$,
\item $o_T$ l'observation agrégée à partir des observations de $T(e)$,
\end{itemize}
Enfin, $\phi : O \times O \to [0,1]$ est une fonction qui évalue la similarité des identités des observations agrégées formant la queue et la tête de l'hyperarc et la bonne succession temporelle de ces observations. La valeur de $\phi$ résulte donc d'une fusion de plusieurs mesures.


\myparagraph{Mesures utilisées pour estimer $\phi$}
Pour pouvoir calculer $\phi$, nous introduisons deux catégories de mesures entre deux observations $o_1$ et $o_2$ de $O$, simples ou agrégées~:
\begin{itemize}
\item des \textbf{mesures de similarité d'identités} qui évaluent la ressemblance entre deux attributs de même \emph{attribute type} au sein des identités de deux observations géohistoriques,
\item des \textbf{mesures de succession} qui évaluent si $o_2$ succède à $o_1$  et lui est consécutive.
\end{itemize}
Une mesure entre deux observations $o_1$ et $o_2$, notée $c$, est de la forme suivante~:
\begin{table}[ht]
\centering
\begin{tabular}{r c l }
$c : \mathcal{U} \times \mathcal{U} \to [0,1] $\\
$c(U_i(o_1),U_i(o_2)) \mapsto s $
\end{tabular}
\end{table}
où~:
\begin{itemize}
\item $U_i$ est un \emph{attribute type} commun à $o_1$ et $o_2$,
\item $s$ est la valeur mesurée normalisée sur $[0,1]$.
\end{itemize}
\subparagraph{}
Les mesures de similarité d'identité dépendent du type des attributs constituant l'identité des observations et ne peuvent donc être définies que lorsque ces types sont connus. Nous renvoyons à la section \ref{section:simRues} pour la définition de telles mesures dans le cas des rues de Paris.
\\
Les mesures de succession portent sur les temps valides des observations et sur l'ordre de leurs sources géohistoriques. Elles sont indépendantes de la nature de ces observations, ce qui nous permet de les définir dès à présent. Dans le chapitre 4, nous avons proposé une mesure d'antécédence entre deux temps flous et une méthode d'ordonnancement de quantités floues. Nous proposons de construire deux mesures de succession à partir de ces dernières.

\myparagraph{Mesures de succession}
\label{par:succession}
La première mesure de succession, notée $c_{ant}$, évalue pour deux observations $o_1$ et $o_2$ à quel point $o_1$ est antécédente à $o_2$. Plus cette antécédence s'approche de $1$ plus il y a de chances pour que la trace à laquelle correspond $o_1$ représente une entité antérieure à l'entité représentée par $o_2$. Si ces observations sont en relation de filiation, cela signifie que $o_1$ est parente de $o_2$. Nous réutilisons directement la mesure d'antécédence choisie dans le chapitre 4 pour définir cette mesure~:
\begin{equation*}
c_{ant}(U_t(o_1),U_t(o_2)) = ant(U_t(o_1),U_t(o_2))
\end{equation*}
\subparagraph{}
La seconde mesure de succession $c_{rank}$ entre $o_1$ et $o_2$ évalue la distance -en nombre de sources- qui sépare la source de $o_1$ de celle de $o_2$. Cette mesure s'appuie sur l'indice de tri de quantités floues $Y_3$ proposée par \cite{Yager1981} et que nous avions retenu dans le chapitre 4. 
\\
Cette mesure revient à calculer le nombre de ''sauts'' qu'il faut effectuer pour passer de la source géohistorique de $o_1$ à celle de $o_2$. Elle est définie par~:
\begin{equation*}
\begin{gathered}
c_{rank}(U_s(o_1),U_s(o_2)) =e^{(-\frac{K}{\epsilon})}  \\
K = |Rang(U_s(o_1)) - Rang(U_s(o_1))|
\end{gathered}
\end{equation*} 
où~:
\begin{itemize}
\item $Rang(S_i)\in \mathbb{N}$ est le rang de la source $S_i$ après rangement à l'aide de l'indice $Y_3$ des sources décrivant les observations de $O$,
\item $\epsilon$ est un paramètre de seuil permettant d'agir sur le nombre de ''sauts'' autorisés avant de considérer la relation de filiation comme non vraisemblable. Ce paramètre ne peut être déterminé que si l'on connaît une valeur limite entre vraisemblance et non vraisemblance. La détermination de cette valeur sera présentée dans le paragraphe \ref{par:vrais_app_disp}
\end{itemize}
%Strictement parlant, la mesure du nombre de sauts ne porte non pas sur les observations mais sur leurs sources. Ces deux mesures ne sont pas redondantes mais s'expriment en réalité sur des propriétés différentes des observations. Si le temps valide d'une observation diffère de celui de sa source (i.e il est plus restreint), il est possible que les deux mesures de succession soient 
%\myparagraph{Fonction de typage d'une relation de filiation}
%Une fois les relations de filiations vraisemblables identifiées, il faut déterminer s'il s'agit de relations de continuation ou de dérivation. Dans le cas d'observations agrégées, il s'agit de connaître le type des relations de filiation qui lient les observations qui composent ces agrégats.
%\\
%Nous introduisons pour finir une fonction dont l'objectif est d'étiqueter les arcs d'un sous-graphe de filiation $G_f(\sigma, E_f,S,\mathcal{T})$ d'un graphe géohistorique (voir le paragraphe \ref{paragraph:graphe_geohisto} du chapitre 5) selon la nature exacte de cette filiation. Il peut s'agir d'une continuation ($\gamma$) ou d'une dérivation( $\delta$). Cette fonction est de la forme~:
%\begin{equation}
%{r} : E_f \to \{\gamma, \delta\}
%\end{equation}


\section{Découverte des relations de filiation}
\label{section:decouverte}
Au sein de cette section, nous présentons l'approche que nous avons mise en \oe uvre pour identifier les relations de filiation entre observations d'un même \emph{featuretype} stockées au sein d'une base de données spatiale et temporelle. Cette approche s'appuie sur une méthode d'appariement multicritère par optimisation prenant en compte les imperfections des observations géohistoriques.
\\
Dans un premier temps, nous justifions le choix de l'approche par appariement que nous étayons avec un état de l'art des principales méthode d'appariement de données géographiques. Cet état de l'art nous révèle les insuffisances de ces approches pour notre objectif mais il nous permet d'identifier certains fondements théoriques et choix de conception pouvant être mis à profit dans notre situation.
\\
Nous proposons, dans un second temps, une méthode d'appariement de données géographiques imparfaites par optimisation combinatoire permettant de découvrir les relations de filiation entre des observations géohistoriques issues d'un ensemble de sources.


\subsection{Un problème d'appariement de données géographiques multicritère}
La découverte des relations de filiation repose sur l'hypothèse selon laquelle deux observations membres d'un processus spatio-temporel ont des identités similaires. Cela signifie qu'entre deux sources géohistoriques $S_1$ et $S_2$, on recherche les relations de filiation décrivant les processus spatio-temporels les plus vraisemblables entre les différentes observations issues de $S_1$ et $S_2$. 
\\
Les observations pouvant être agrégées, cela revient à comparer les identités de chaque paire d'observations et à rechercher les observations dont les identités sont les plus proches. 
%La figure~\ref{figure:exemple_agrégation} montre comment l'agrégation de deux observations permet de réduire le problème d'identification des relations de filiation décrivant un processus de fusion à une recherche d'objets similaires. En effet, 
Les observations étant des objets géographiques représentés sous forme de \emph{features}, le problème revient donc à comparer des objets géographiques pour identifier des objets homologues au sein de différentes sources de données. La mise en correspondance de plusieurs bases de données géographiques dans le but d'identifier des objets géographiques homologues fait l'objet de recherches dans le domaine de la géomatique depuis de nombreuses années. On parle alors d'\textbf{appariement de données géographiques}. Le but des méthodes d'appariement de données géographiques est d'identifier les objets communs dans deux bases de données, parfois à des niveaux de détail différents. Cet appariement peut être réalisé pour des objectifs divers~: fusion d'informations provenant des deux bases de données, transfert d'attributs, propagation de mise à jour, versionnement, etc. L'utilisation de méthodes d'appariement pour la détection de processus spatio-temporels a déjà été proposée, par exemple par~\cite{DeRunz2008} ou \cite{Gombosi2003}. Nous faisons ici un état de l'art des principales méthodes d'appariement de données géographiques afin de discuter de leur adaptabilité à notre problème. Afin que cette discussion soit possible, nous posons d'abord les fonctionalités que doit proposer une telle méthode pour être capable de découvrir des relations de filiation entre observations géohistoriques.

\myparagraph{Fonctionnalités nécessaires pour la découverte de relations de filiation entre observations géohistoriques}
Pour permettre de découvrir les relations de filiation entre observations, une méthode d'appariement de données géographique doit proposer les fonctionnalités suivantes~:
\begin{enumerate}
\item \textbf{Critères multiples}. Le but est de comparer les observations selon plusieurs critères portant sur leurs identités et leurs temporalités, ce qui implique de disposer de plusieurs mesures permettant ces comparaisons. La méthode d'appariement doit donc être \textbf{multicritère}.
\item \textbf{Sources multiples}. Les observations composant le graphe de filiation que l'on cherche à construire sont issues d'un ensemble quelconque de sources géohistoriques. S'il est envisageable d'effectuer un appariement de proche en proche\footnote{En identifiant d'abord les relations de filiation entre les sources les plus anciennes, puis entre ce résultat et la source suivante, etc.}(cf. notamment \citep{Costes2015}), cette manière de procéder ne prend pas en compte certaines complexités des données géohistoriques. D'abord, rappelons que nous autorisons les relations de filiation à effectuer des ''sauts temporels''~: ces relations peuvent exister entre deux observations $S_1$ et $S_3$, même s'il existe une source $S_2$ dont le temps valide se situe entre ceux des deux autres sources (voir le paragraphe~\ref{par:timejump} du chapitre 5). De plus, nous autorisons également des relations de filiation ''rétro-temporelles'' (voir le paragraphe~\ref{par:retrotemp} du chapitre 5) qui ne pourraient alors pas être détectées. Nous devons donc disposer d'une méthode permettant d'apparier plusieurs sources de données à la fois.
\item \textbf{Gestion des imperfections}. Nos observations géohistoriques sont des objets géographiques imparfaits. Elles peuvent être incomplètes si certains attributs manquent, imprécises, erronées (par exemple un décalage planimétrique), etc. Il est donc crucial de disposer d'une méthode prenant en compte des données imparfaites.
\item \textbf{Cardinalité des appariements}. La majorité des méthodes d'appariement s'intéresse à identifier, pour chaque objet géographique d'une première base de données, son objet homologue dans une seconde base. Il s'agit donc de trouver uniquement des liens de cardinalité 1: 1 entre objets homologues. Cependant, il est possible qu'un objet ait plusieurs objets homologues. Ce cas se produit principalement lorsque le niveau de détail des bases de données est différent. Dans le cas des observations géohistoriques, les situations de cardinalités supérieures au cas 1: 1 sont nombreuses car nous recherchons explicitement des cas où des observations se divisent ou s'assemblent. De plus, nous devons également gérer le cas des apparitions et disparitions. Nous avons donc besoin d'une méthode capable de détecter des observations non appariées (apparitions et disparitions, de cardinalité 0: 1 ou 1: 0), des filiations simples (continuations/dérivations, de cardinalité 1: 1) et les différents cas de filiations avec des observations agrégées (fusion/annexion~: N: 1, scission/séparation~: 1: N, réallocation~:N: M).
\item \textbf{Type de géométrie}. Souvent, les méthodes d'appariement de données géographiques sont adaptées à des objets géographiques modélisés avec un type de géométrie unique (points, polylignes ou polygones par exemple). Or, les observations géohistoriques pouvant décrire n'importe quelle entité du monde réel, nous n'avons aucun a priori sur le type de géométrie qui permet de représenter leur forme et leur localisation. 
\end{enumerate}

\myparagraph{État de l'art des méthodes d'appariement de données géographiques}
Nous avons réalisé un état de l'art des principales méthodes d'appariement de données géographiques que nous avons confrontées aux fonctionalités nécessaires définies dans le paragraphe précédent.Il existe de nombreuses méthodes d'appariement de données géographiques, souvent destinées à un cas d'application particulier. Nous avons conservé ici seulement 8 propositions, sélectionnées selon les trois critères suivants~:
\begin{itemize}
\item il s'agit d'une méthode régulièrement citée dans des travaux liés à l'appariement de données géographiques,
\item la méthode est adaptée à la découverte de transformations entre objets géographiques,
\item certains auteurs ont proposés d'appliquer la méthode au cas spécifique de la découverte des transformations entre objets géographiques.
\end{itemize}
De plus, nous nous sommes restreint aux méthodes d'appariement de données géographiques vectorielles, nos observations étant des \emph{features} vecteur. Les différentes méthodes sont présentées dans le tableau~\ref{table:matchingstateoftheart}. Chaque colonne correspond à une fonctionalité nécessaire pour le cas des observations géohistoriques.
\\
Nous avons également ajouté une courte description du verrou scientifique visé par chaque proposition. De plus, la plupart des méthodes sont adaptées à des objets géographiques représentés par un certain type de géométrie (point, polyligne, polygone, etc.). Enfin, la colonne ''Type d'approche'' indique si la méthode s'appuie sur un fondement théorique spécifique. L'approche est notée comme ''algorithmique'' lorsqu'elle ne s'appuie pas sur une théorie mathématique particulière. Il s'agit généralement de méthode fondée sur la comparaison des géométries des objets traités ou leurs relations topologiques.

\begin{sidewaystable}
\caption{Tableau résumant l'adéquation des principales approches d'appariement de données géographiques aux fonctionalités nécessaires pour la découverte de relations de filiations.}
\label{table:matchingstateoftheart}
\NoAutoSpaceBeforeFDP
\begin{tabular}{|c|c|c|c|c|}
\hline
Méthode&Multicritère&Sources>2&Imperfections&Cardinalités\\
\hline
\citep{MustiereDevogele2008} & non & non & non & 0:1,1:0,N:1,1:N,N:M \\
\hline
\citep{Olteanu2008} & oui & non & oui & 0:1,1:0,N:1,1:N,(N:M) \\
\hline
\citep{Fritsch1999} & non & non & décalages planimétriques & 0:1,1:0,N:1,1:N,N:M \\
\hline
\citep{Samal2004} & oui  & oui & oui & 0:1,1:0,1:1 \\
\hline
\citep{LiGoodchild2011}  & non & non &  décalages planimétriques & 0:1,1:0,N:1,1:N \\
\hline
\citep{Gombosi2003} & non  & non & non & 0:1,1:0,1:1 \\
\hline
\citep{Beeri2004} & non  & non & non & 0:1,1:0,1:1, 1:N \\
\hline
\citep{Voltz2006} & non & non & non & 1:1,1:2 \\
\hline
%\citep{Haunert2005} & 1-n/n-1 & g & 2 & iterative& oui & réseaux linéaires \\
%\citep{Gombosi2003} & 1-1 & g & 2 & ?? & ?? & surfacique  \\
%\citep{Fritsch1999} & n-m & g & 2 & théorie de l'information & faible \\
%\citep{Samal2004} & 1-1 & g+a+p & plusieurs & iterative & faible \\
%\citep{Voltz2006} & 1-2/2-1 & g & 2 & iterative & faible \\
%\citep{LiGoodchild2011} & 1-n/n-1 & g & 2 & optimisation discrète & faible \\
%%\citep{Yang} & ? & g & 2 & iterative & ? \\
%%\citep{Zhang} & ? & ? &? &? &? \\
%%\citep{Gosseln} & ? &?&? &?&? \\
%\citep{Beeri2004}   1-1 &?&? &?&? \\
%%\citep{Daylot}   & ? &?&? &?&? \\
%\citep{Olteanu2008} & n-m & g, a & 2 & théorie des fonctions de croyance & oui \\
\end{tabular}
\end{sidewaystable}
\begin{sidewaystable}
\caption{Tableau résumant l'adéquation des principales approches d'appariement de données géographiques aux fonctionalités nécessaires pour la découverte de relations de filiation.}
\label{table:matchingstateoftheart2}
\begin{tabular}{|c|c|c|p{8cm}|}
\hline
Méthode & Géométries & Type d'approche & Verrou \\
\hline
\citep{MustiereDevogele2008} & lignes polygonales & algorithmique & Apparier deux réseaux routiers représentés à des niveaux de détail différents. \\
\hline
\citep{Olteanu2008} & tout type & théorie des croyances & Apparier deux ensembles de données à partir de connaissances imparfaites.\\
\hline
\citep{Fritsch1999} & lignes polygonales & théorie de l'information & Apparier deux réseaux routiers et évaluer automatiquement la qualité de l'appariement. \\
\hline
\citep{Samal2004} & tout type & algorithmique & Apparier $N\geq 2$ ensembles de données simultanément. \\
\hline
\citep{LiGoodchild2011}  & lignes polygonales & optimisation combinatoire & Apparier deux réseaux de rues et corriger dynamiquement les décalages planimétriques entre eux. \\
\hline
\citep{Gombosi2003} & polygones  & algorithmique & Détecter les changements entre deux ensembles de polygones. \\
\hline
\citep{Beeri2004} & points & algorithmique & Apparier efficacement deux ensembles d'objets géographiques ponctuels lorsque les ensembles se recouvrent peu\footnote{Il y a faible recouvrement entre deux ensembles de données lorsque très peu d'objets du premier ensemble possèdent un homologue dans le second ensemble.}. \\
\hline
\citep{Voltz2006} & lignes polygonales & algorithmique & Apparier deux réseaux de rues rapidement et sans perte de qualité par rapport aux méthodes existantes.  \\
\hline
\end{tabular}{}
\AutoSpaceBeforeFDP
\end{sidewaystable}

\myparagraph{Bilan et discussion sur les méthodes d'appariement de l'état de l'art}
À la lecture du tableau~\ref{table:matchingstateoftheart}, on peut constater qu'aucune méthode ne fournit toutes les fonctionalités requises. Pour la majorité des méthodes, cela est dû au fait qu'elles sont mises en place pour un problème spécifique, ce qui réduit fortement la généricité de la méthode. Ainsi, les méthodes proposées par~\cite{MustiereDevogele2008}, \cite{WalterFritsch1999}, \cite{LiGoodchild2011} et \cite{Voltz2006} s'intéressent au cas spécifique de l'appariement de réseaux de voies. De plus, toutes les méthodes à l'exception de \cite{Samal2004} et \cite{LiGoodchild2011} et surtout \cite{Olteanu2008} s'appuient uniquement sur la géométrie (et éventuellement les relations topologiques) des objets géographiques pour identifier les objets homologues. Un tel choix s'explique par le fait que la forme et la localisation des objets géographiques en constituent les attributs les plus significatifs et, souvent, les seuls fiables ou les plus renseignés dans les bases de données utilisées par les auteurs (ou, tout du moins, pour leur définition de l'identité)
\\
Parmi les méthodes présentées dans le tableau, les propositions de~\cite{Olteanu2008} et~\cite{Samal2004} sont toutefois intéressantes car elles répondent toutes deux à quatre des cinq fonctionalités dont nous avons besoin. Nous présentons ici la méthode de~\cite{Samal2004} avant de nous attarder plus longuement sur la proposition d'~\cite{Olteanu2008}. Pour apparier plusieurs ensembles d'objets géographiques~\cite{Samal2004} proposent de construire un graphe complet\footnote{Un graphe complet est un graphe dont tous les sommets sont reliés deux à deux par une arête.} entre tous les objets géographiques des différents ensembles, puis de ne conserver que les arêtes ne liant pas deux objets du même ensemble et de longueur inférieure à un seuil donné. Chaque arête est ensuite pondérée par une similarité calculée entre les deux objets géographiques liés. Cette similarité est une agrégation de plusieurs similarités s'exprimant sur divers attributs des objets géographiques, notamment la forme de leur géométrie, leur position, ainsi que des similarités entre attributs thématiques représentés sous forme de chaînes de caractères (leur nom, leur fonction, etc.). Le problème revient alors à identifier des groupes d'objets issus des différents ensembles reliés par des arêtes de poids maximal. Un aspect particulièrement intéressant de cette méthode est qu'elle intègre, parmi ses mesures de similarité, une notion de contexte local des objets géographiques~: deux objets géographiques sont d'autant plus similaires que les objet les plus significatifs\footnote{Il s'agit des objets les plus précis, ou les plus importants. Dans le cas de la mise en \oe uvre de cette méthode pour apparier des objets représentés par des polygones, il s'agit des $n$ objets de plus grande surface.} les entourant sont répartis de façon similaire. La méthode présente cependant deux faiblesses~:
\begin{itemize}
\item d'une part, le calcul est extrêmement coûteux car toutes les combinaisons d'objets géographiques en dessous du seuil de distance maximal doivent être évaluées, même si une seule combinaison est finalement conservée,
\item d'autre part, cette méthode ne permet d'identifier que des appariements de cardinalité maximale 1: 1.
\end{itemize}
Ces limitations -en particulier sur les cardinalités- nous obligent à éliminer cette méthode. En effet, la possibilité de détecter des appariements de cardinalités supérieures à 1: 1 est au c\oe ur de notre problème. La proposition de~\cite{Samal2004} pourrait être étendu au cas 1: N si l'on ajoute une étape supplémentaire consistant à agréger des objets géographiques d'une même base de données lorsque ceux-ci sont fortement similaires à un même objets d'une autre base. De cette façon, l'approche n'est pas modifiée en profondeur. Il n'est toutefois pas possible de prendre en compte de la même façon les cas N: 1 et N: M. 
\myparagraph{}
La proposition d'\cite{Olteanu2008} s'appuie sur la théorie des croyances, ou théorie de l'évidence (de l'anglais \emph{theory of evidence}), introduite par \cite{Dempster1968} et formalisée par \cite{Shafer1976}. La théorie des croyances permet de modéliser des propositions (hypothèses, évènements, décisions) sur lesquelles des sources d'informations imparfaites se prononcent. Cette théorie permet en particulier de représenter des informations incertaines ou imprécises \footnote{Nous ne présentons pas plus en détail la théorie des croyances dans ce paragraphe. Elle sera abordée plus avant dans la suite du mémoire.}. Nous présentons ici l'approche d'Olteanu de façon très générale. Pour plus de précisions, le lecteur peut se rapporter à la thèse complète citée dans la bibliographie du présent mémoire.
\\
L'approche d'\cite{Olteanu2008} appliquant cette théorie dans le cadre de l'appariement de données géographiques peut se résumer de la façon suivante~:
\begin{enumerate}
\item Un ensemble de critères mesurent la similarité entre les différents attributs des objets géographiques issus de deux jeux de données (l'un \emph{source} et l'autre \emph{cible}). Ces critères peuvent porter sur la géométrie, la topologie ou les attributs thématiques des objets.
\item Pour chaque objet géographique du premier jeu de données, un ensemble d'objets ''candidats homologues'' du second jeu de données est sélectionné selon une heuristique fondée sur une distance maximale. Sur chaque couple formé de l'objet à apparier et d'un candidat, chaque critère exprime un degré de croyance en l'une des trois hypothèses~: que les objets soient homologues, qu'ils ne le soient pas ou que la situation est totalement incertaine.
\item Pour chaque couple, une croyance globale pour chaque hypothèse est ensuite calculée en fusionnant les différentes croyances issues des critères de similarité. 
\item Enfin, le candidat ayant la plus grande croyance dans le fait qu'il soit l'homologue de l'objet à apparier est conservé comme meilleur candidat. Si aucun candidat n'est un homologue crédible, l'objet est jugé non apparié. Les appariements de cardinalité 0: 1, 1 : 0 et 1 : 1 sont détectés de cette façon.
\item Les appariement de cardinalité N : 1 sont détectés en regroupant a posteriori les appariements de cardinalité 1 : 1 lorsque un candidat a été choisi par plusieurs objets du premier jeu de données. Les appariements de cardinalité 1: N sont détectés en reproduisant le même processus mais en inversant les deux jeux de données (le jeux \emph{source} devient \emph{cible} et vice-versa). Enfin, les appariements de cardinalité N : M peuvent être trouvés en regroupant les appariements 1 :N et N : 1.
\end{enumerate}
La méthode d'\cite{Olteanu2008} présente l'avantage majeur de modéliser des connaissances imparfaites, ce qui permet de prendre en compte les différentes imperfections des données. De plus, la théorie de l'évidence fournit un cadre formel permettant d'introduire la notion d'ignorance, particulièrement pertinente lorsque les données sont incomplètes. Il s'agit finalement d'une méthode générique puisqu'elle s'appuie sur un ensemble de critères de similarité qui peuvent être définis pour un cas applicatif précis sans remettre en question le c\oe ur de la méthode.
\\
Tout comme pour \cite{Samal2004}, cette méthode présente l'inconvénient d'évaluer l'ensemble des candidats pour chaque objets géographique d'un jeu de données. Si l'on veut détecter des appariement de cardinalité N: 1 ou N: M, cette évaluation doit être faite deux fois. Le coût de calcul peut devenir prohibitif lorsque les jeux de données sont importants et denses. De plus, cette méthode ne permet d'apparier que deux jeux de données. Son utilisation nécessiterait donc une adaptation pour gérer un appariement entre plusieurs jeux de données simultanément. Le coût de calcul deviendrait alors rapidement extrêmement important.
\\
Cette approche fondée sur la théorie des croyances propose toutefois une formalisation des connaissances sur les objets géographiques homologues intéressante pour la découverte de relations de filiation entre observations. En particulier, le fait que cette approche envisage l'ignorance parmi les hypothèses de base constitue un aspect intéressant dans le cas d'observations géohistoriques pour lesquelles il est préférable d'envisager des relations de filiation incertaines. Nous proposons donc de reprendre cette approche d'appariement dans le formalisme des fonctions de croyance tout en l'adaptant pour permettre l'appariement de N (>2) sources, la détection de liens d'appariement de cardinalités complexes (1:N,N:1,N:M) et un meilleur passage à l'échelle.\\
\textbf{Nous proposons pour cela de modéliser le problème de la découverte des relations de filiation entre des observations géohistoriques issues d'un ensemble de sources sous la forme d'un problème d'optimisation discrète s'appuyant sur la théorie des croyances}. Avant de présenter notre proposition de résolution par optimisation, nous introduisons les éléments les plus importants de la théorie des croyances. Pour de plus amples détails, le lecteur peut se rapporter à \citep{Olteanu2008}, ou encore \citep{Martin2005}.

\subsection{Éléments de la théorie des croyances}
Soit $\Omega = \{\omega_1,\omega_2,\dots,\omega_n\}$ un ensemble de propositions appelé \textbf{cadre de discernement}. Une \textbf{masse de croyance, ou fonction de masse,} $m : 2^\Omega \to [0,1]$ est une fonction s'exprimant sur l'ensemble $2^\Omega = \{\emptyset,\omega_1,\omega_2, \{\omega_1,\omega_2\},\dots,\Omega \}$, nommé \emph{power set}, composé de toutes les disjonctions possibles des éléments de $\Omega$. Une fonction de masse exprime un degré de croyance dans une proposition de $\Omega$ (appelée proposition singleton) ou dans une disjonction de propositions. Notons que les propositions doivent être exclusives. Une fonction de masse doit vérifier~:
\begin{equation*}
 \sum\limits_{A \in 2^\Omega} m(A) = 1
\end{equation*}
La représentation de l'incertitude s'effectue alors en attribuant des masses de croyance à des disjonctions d'hypothèses. Un élément $A \in 2^\Omega$ pour lequel $m(A) > 0 $ est appelé \textbf{élément focal}.
\\
Une \emph{fonction de croyance, ou fonction de crédibilité} est une fonction notée $bel : 2^\Omega \to [0,1]$permettant de modéliser la certitude dans la réalité d'une proposition ou d'une disjonction de propositions. Les fonctions de croyance se définissent facilement à partir des masses de croyance. Pour toute proposition $A\in 2^\Omega$, on a~:
\begin{equation*}
 bel(A) = \sum\limits_{B \subseteq A, B \neq \emptyset} m(B)
\end{equation*}
La crédibilité d'une proposition est une mesure relativement pessimiste dans la mesure où seules les connaissances plus précises que $A$ augmentent la croyance en $A$. D'autres fonctions plus lâches existent. Ainsi, la plausibilité d'une proposition $A$ mesure la force avec laquelle on ne doute pas de $A$. Elle est définie par~:
\begin{equation*}
 pl(A) = \sum\limits_{B \cap A, B \neq \emptyset} m(B)
\end{equation*}
Enfin, une fonction intermédiaire nommée \textbf{probabilité pignistique}, introduite par~\cite{Smets1989} effectue un compromis entre crédibilité et plausibilité. Elle consiste à équi-répartir les masses de croyances des disjonctions sur les propositions singleton qui les composent. Cette probabilité s'écrit~:
\begin{equation*}
 P(A) = \sum\limits_{A \subseteq B, B \in 2^\Omega} \frac{m(B)}{|B|(1-m(\emptyset))}
\end{equation*}
où $m(\emptyset)$ désigne le conflit (voir ci-après).
\\
Plusieurs masses de croyance peuvent être définies sur $2^\Omega$. Par exemples, plusieurs sources d'informations peuvent se prononcer sur les propositions du cadre de discernement ou leurs disjonctions. Si l'on veut connaître la croyance globale dans une proposition, il faut alors fusionner les différentes masses de croyance. Le fait d'avoir plusieurs sources d'informations peut créer des situations de conflit dans lesquelles les sources proposent des masses de croyances contradictoires. Plusieurs opérateurs de fusion des masses de croyance existent et diffèrent par la stratégie adoptée pour prendre en compte les cas de conflits. Deux stratégies prédominent, nommées respectivement \emph{monde fermé} et \emph{monde ouvert}. La stratégie du monde fermé consiste à considérer que le cadre de discernement est exhaustif. Dans ce cas, le conflit est redistribué sur les éléments de $2^\Omega$. La stratégie du monde ouvert considère quant à elle un cadre de discernement non exhaustif~: la proposition la plus crédible n'est peut être pas représentée. Dans ce cas, le conflit est affecté à un élément de $2^\Omega$. Ainsi, \cite{Smets1990} propose de placer le conflit sur la proposition vide $\emptyset$, et \cite{Yager1987} sur $\Omega$.
\myparagraph{Opérateurs de fusion}
Nous présentons ici les deux opérateurs de fusion les plus utilisés. L'opérateur conjonctif de Dempster \citep{Shafer1986} considère un monde fermé. Pour un ensemble de masses de croyances $\{m_1, m_2,\dots,m_n\}$ définies par $n$ sources d'informations sur les éléments de $2^\Omega$, cet opérateur est défini par~:
\begin{equation}
\begin{aligned}
 m(A) = \frac{1}{1-k}\sum\limits_{B_1\cap B_2\cap \dots \cap B_n = A} \prod_{i=1}^{n}m_i(B_i) \text{ si A $\neq \emptyset$}\\
 m(\emptyset) = 0  \phantom{\hspace{6.9cm}}
 \end{aligned}
\end{equation}
avec $k = \sum\limits_{B_1\cap B_2\cap \dots \cap B_n = \emptyset} \prod_{i=1}^{n}m_i(B_i)$.\\
$m(A)$ désigne la masse de croyance fusionnée de $m_1,\dots, m_n$. La valeur $k$ est la \textbf{mesure de conflit} entre les diverses sources d'informations. L'opérateur de Dempster effectue une normalisation des croyances par le conflit. Cette normalisation a pour effet d'augmenter artificiellement les croyances dans les propositions pour lesquelles les sources d'informations sont concordantes.
\\
Un second opérateur, proposé par~\cite{Smets1990} reprend celui de Dempster dans le cas d'un monde ouvert. La différence avec l'opérateur conjonctif est qu'au lieu de normaliser les croyances, le conflit est déplacé sur $m(\emptyset)$. On a alors~:
\begin{equation}
\begin{aligned}
 m(A) = \sum\limits_{B_1\cap B_2\cap \dots \cap B_n = A} \prod_{i=1}^{n}m_i(B_i) \text{ si A $\neq \emptyset$}\\
 m(\emptyset) = k \phantom{\hspace{5.9cm}}
 \end{aligned}
\end{equation}
 Cet opérateur suppose que les sources d'informations sont totalement fiables. Lorsque ce n'est pas le cas, la stratégie généralement choisie est de redistribuer le conflit sur les propositions impliquées dans l'émergence de ce conflit, c'est-à-dire dont les sources d'informations sont contradictoires (voir \citep{Lefevre2002}).
\myparagraph{Affaiblissement}
La théorie des croyances permet également de modéliser la fiabilité des sources d'information lorsque cette fiabilité est quantifiable. Le manque de fiabilité d'une source résulte en un \emph{affaiblissement} de la croyance qu'elle place en une hypothèse et un accroissement de l'ignorance~\footnote{La somme des croyances de cette source est donc toujours égale à 1.}. L'affaiblissement $\alpha_i$ de la source d'information $i$ s'exprime en écrivant sa fonction de masse de la façon suivante~:
\begin{equation}
\begin{gathered}
m_i(A) = \alpha_i m_i(A),  A \in 2^\Omega \\
m_i(\Omega) = 1 - \alpha_i (1-m_i(A))
\end{gathered}
\end{equation}

\myparagraph{Décision}
La fusion des masses de croyance permet d'obtenir des masses de croyances globales résumant l'avis des différentes sources d'informations sur les différentes propositions. Il reste alors à prendre une décision, c'est-à-dire à choisir une proposition (généralement un singleton). Différents critères de décision existent, les plus classiques consistent à sélectionner la proposition de crédibilité, de plausibilité ou de probabilité pignistique maximale. 

\subsection{Approche par optimisation discrète}
Les bases de la théorie des croyances ayant été exposées, nous introduisons maintenant notre approche d'appariement multicritère fondée sur cette théorie. Pour ce faire, nous formulons ce problème d'appariement comme problème d'optimisation discrète.
\myparagraph{}
Soit $\mathcal{H}$ l'ensemble formé de tous les hypergraphes admissibles pouvant être construits à partir de tous les graphes de filiation admissibles possibles liant les observations de $O$. 
\\
Nous introduisons une fonction de \textbf{vraisemblance d'un hypergraphe de filiation} $V : \mathcal{H}  \to \mathbb{R}^+$ qui synthétise toutes les vraisemblances des arcs d'un hypergraphe $H_f\in \mathcal{H}$. Cette fonction mesure à quel point un hypergraphe admissible est une description vraisemblable des transformations ayant eu lieu entre les entités du monde réel représentées par les observations de $O$.
\\
\textbf{Le problème revient à rechercher l'hypergraphe $H_f^*$ de vraisemblance maximale au sein de tous les hypergraphes admissibles exprimables sur un ensemble d'observations.} Il s'agit donc de déterminer~:
\begin{equation}
H_f^* = \argmax \{V(H_f) : H_f \in \mathcal{H}\}
\end{equation}
Dans le reste de cette section, nous définissons précisément cette fonction de vraisemblance d'un hypergraphe de filiation. Celle-ci est intrinsèquement liée à la fonction de vraisemblance d'un hyperarc $\Phi$ que nous avons définie pour l'instant de façon générique. Nous procédons par étapes, en précisant dans un premier temps $\Phi$ puis en remontant jusqu'à l'expression de $V$ afin d'en donner l'expression détaillée.

\myparagraph{Passage à la théorie des croyances}
\label{paragraph:modcroyances}
Soit $H_f(O\cup o_\emptyset,E,S,\mathcal{T})$ un hypergraphe de filiation admissible dont on cherche à savoir s'il représente un ensemble de processus spatio-temporels vraisemblables. Nous définissons deux hypothèses sur chaque arc $e\in E$~:
\begin{itemize}
\item[$\omega_e$ : ] $e$ est l'arc décrivant le processus spatio-temporel le plus vraisemblable entre les observations de sa queue et de sa tête,
\item[$\neg \omega_e$ : ] l'hypothèse contraire.
\end{itemize}
Pour l'ensemble $\{e_1,e_2,\dots,e_n\}$ de tous les arcs de $H_f$,   nous pouvons créer le cadre de discernement $\Omega$ contenant toutes les hypothèses sur ces arcs~:
\begin{equation*}
\Omega = \{\omega_{e_1}, \neg \omega_{e_1}, \dots, \omega_{e_n}, \neg \omega_{e_n}\}
\end{equation*}
Les sources d'informations qui vont croire plus ou moins à ces hypothèses sont, pour nous, des critères portant sur l'identité et la succession temporelle des observations de la queue et de la tête de chaque hyperarc. Nous considérons que la croyance en un hyperarc ne dépend pas de celle des autres arcs de $H_f$. Autrement dit, nous traitons chaque arc comme un sous-problème du problème général de détermination des processus spatio-temporels les plus vraisemblables. Pour ce faire, nous utilisons la notion de \emph{source spécialisée}, introduite par \cite{Appriou1991} et utilisée par \citep{Olteanu2008}. L'idée est de décomposer les sources d'informations en sources se spécialisant sur chaque hypothèse singleton dans $2^\Omega$. Plus exactement, chaque source spécialisée n'a que trois éléments focaux~: une hypothèse singleton $\omega \in \Omega$, son contraire $\neg \omega$ et la disjonction totale $\Omega$. Ainsi, une source d'information peut croire qu'une hypothèse singleton est vraie, qu'elle est fausse ou bien peut être dans l'incertitude totale. Sur chaque source spécialisée est également définie un coefficient de fiabilité $\alpha$ permettant d'affaiblir les sources peu fiables. 
\\
Pour nous, les sources d'informations vont se spécialiser sur chaque hyperarc. Étant donné un nombre $M$ de sources d'informations et $n$ hyperarcs, cela implique de définir $M.n$ fonctions de masse de croyance de la forme $m_{ie_j}(A), A \in \{\omega_{e_j}, \neg \omega_{e_j}, \Omega\}$. Dans son travail, \cite{Appriou1991} s'intéresse à de l'apprentissage statistique et distribue les masses de croyances sur $\omega$ et $\neg \omega$, la croyance incertaine ($\Omega$) étant due au manque de fiabilité des sources d'informations. Il introduit les deux modèles de fonction de masses suivants~:
\\
Modèle 1 : 
\[
\begin{cases}
m_{ij}(\omega) =  \alpha_{ij} M_{ij} \\
m_{ij}(\neg \omega) =  \alpha_{ij}(1-M_{ij}) \\
m_{ij}(\Omega) = 1-\alpha_{ij} \\
\end{cases}
\]
Modèle 2 : 
\[
\begin{cases}
m_{ij}(\omega) =  0 \\
m_{ij}(\neg \omega) =  \alpha_{ij}(1-M_{ij}) \\
m_{ij}(\Omega) = 1-\alpha_{ij}(1-M_{ij}) \\
\end{cases}
\]
où $M_ij$ est la vraisemblance statistique de la proposition $\omega$ et $\alpha_ij$ un coefficient d'affaiblissement de la source focalisée $ij$. Si les sources d'informations ne sont pas fiables, toute la croyance est donc placée dans l'ignorance. 
\\
Dans notre cas, la valeur de $M_ij$ est la valeur des mesures d'identité ou de succession temporelle. De plus, nous considérons que toutes les sources focalisées du même type (antécédence, sauts, etc.) ont la même fiabilité. Enfin, les deux modèles peuvent être interprétés comme des visions plus ou moins prudentes du problème. Le premier modèle est peu prudent, les sources d'informations s'exprimant sur la validité ou la non-validité d'une proposition~: si la source ne croit pas en $\omega$, elle considère cette proposition fausse. Le second modèle est nettement plus prudent~: tout ce qui n'appuie pas la proposition $\omega$ est placé dans l'ignorance. Ainsi, si la source ne croit pas en $\omega$, elle est dans une situation d'incertitude. 
\\
Un tel comportement est particulièrement intéressant pour notre cas d'application car il permet de traiter différemment les informations sur l'identité et sur la succession temporelle. Les mesures d'identité doivent s'exprimer sur l'existence d'une filiation entre deux observations. Pour une mesure donnée, l'existence d'une filiation est crédible ou non~: elles suivent le premier modèle. Les sources d'informations sur la succession temporelle suivent quant à elles le second modèle. En effet, les mesures de succession permettent de savoir si une filiation est envisageable car logique temporellement, mais elles ne permettent pas de savoir si cette filiation est pertinente.
Finalement, les deux modèles de \cite{Appriou1991} peuvent être exprimés pour la découverte des relations de filiation de la façon suivante~:
\\
Pour les mesures d'identité~:
\[
\begin{cases}
m_{ie_j}(\omega_e) =  \alpha_{i} c \\
m_{ie_j}(\neg \omega_e) =  \alpha_{i}(1-c) \\
m_{ie_j}(\Omega) = 1-\alpha_{i} \\
\end{cases}
\]
Pour les mesures de succession temporelle~:
\[
\begin{cases}
m_{ie_j}(\omega_e) =  0 \\
m_{ie_j}(\neg \omega_e) =  \alpha_{i}(1-c) \\
m_{ie_j}(\Omega) = 1-\alpha_{i}(1-c) \\
\end{cases}
\]
où $c$ est la valeur de la mesures calculée pour l'hyperarc $e_j$.


\myparagraph{Fusion des croyances sur un hyperarc}
L'ensemble des mesures de similarité d'identité et de succession temporelle s'expriment sur chaque hyperarc de $H_f$ sous la forme de fonctions de masses. Chaque mesure donne une valeur de croyance et il faut donc fusionner l'ensemble des masses de croyances sur les hypothèses définies pour l'arc pour obtenir la fonction de masse totale $m_{e_j}$ pour chacune des hypothèses. Pour cela il faut choisir un opérateur de fusion et donc décider si l'on se place dans le cas d'un \emph{monde ouvert} ou \emph{fermé}.
\\
Évaluer la vraisemblance d'un hyperarc revient à évaluer la vraisemblance du processus spatio-temporel sous-jacent liant ses observations. Cette évaluation se fait sur les trois hypothèses définies précédemment, or d'autres relations de filiations entre les observations membres de l'hyperarc pourraient tout à fait conduire à un autre hyperarc plus vraisemblable que celui en cours d'évaluation. Nous nous trouvons donc dans le cas d'un monde ouvert, où d'autres hypothèses que celles du cadre de discernement peuvent exister.
\\
Nous utilisons donc l'opérateur conjonctif non normalisé proposé par \cite{Smets1990}~:
\[
\begin{cases}
m_{e_j}(A) = \sum\limits_{B_1\cap \dots\cap B_m=A\neq \emptyset}\prod\limits_{i=1}^{M}m_{ie_j}(B_i), \\
m_{e_j}(\emptyset) = \sum\limits_{B_1\cap \dots\cap B_m=\emptyset}\prod\limits_{i=1}^{M}m_{ie_j}(B_i),
\end{cases}
\]
$m_{e_j}(\emptyset)$ représente le conflit émergent des sources d'information contradictoires. Dans notre cas, le conflit apparaît lorsque les identités des observations agrégées de la queue et de la tête de l'hyperarc évalué sont similaires mais que la relation de filiation est soit dans le sens rétrotemporel, soit qu'il existe d'autres sources géohistoriques intercalées entre celles contenant la queue et la tête de l'arc. Autrement dit, cela signifie que les sources optimistes croient fortement dans la vraisemblance de l'arc tandis que les sources pessimistes croient dans l'hypothèse contraire.
\\
Une fois les masses de croyance fusionnées pour chaque arc $e \in E$, nous souhaitons évaluer la vraisemblance de celui-ci. Pour cela, nous pouvons évaluer la crédibilité, la plausibilité ou la probabilité pignistique de l'hypothèse $\omega_{e}$. La probabilité pignistique est dans notre cas particulièrement pertinente. En effet, c'est une mesure de compromis bien adaptée au cas où la crédibilité et la plausibilité sont très différentes, ce qui est particulièrement le cas lorsque l'ignorance est élevée. La crédibilité est alors faible tandis que la plausibilité est proche de 1. Or, les mesures optimistes et pessimistes peuvent aboutir à une forte ignorance. Pour tout hyperarc $e\in E$, la probabilité pignistique de l'hypothèse $\omega_e$ ''l'arc est vraisemblable'' est~:
\begin{equation*}
P(\omega_e) = \frac{m_e(\omega_e)}{1-m(\emptyset)} + \frac{m(\Omega)}{2(1-m(\emptyset))}
\end{equation*}
On a finalement, pour tout hyperarc $e\in E$ ~:
\begin{equation}
\Phi(e) = \phi(o_Q,o_T) = P(\omega_e)
\end{equation}

\myparagraph{Vraisemblance des hyperarcs décrivant des apparitions et disparitions}
\label{par:vrais_app_disp}
Les hyperarcs correspondant à des processus spatio-temporels d'apparition et de disparition sont de la forme $e=(o_\emptyset ,o)$ ou $e=(o,o_\emptyset )$ et connectent l'observation $o$ à l'observation nulle $o_\emptyset$. Nous venons de définir la vraisemblance de tels hyperarcs comme une mesure fusionnée à partir de critères comparant deux observations, or la comparaison avec l'observation nulle n'a pas de sens. La théorie des croyances offre en réalité un cadre permettant aisément de modéliser le cas de ces hyperarcs. En effet, lorsqu'il y a apparition ou disparition, aucune observation ne correspond à $o$~: toutes les sources d'informations sont alors dans une situation d'ignorance totale. La totalité de la masse de croyance est alors placée sur $\Omega$.
\\
La vraisemblance $\Phi$ d'hyperarcs représentant des apparitions ou disparitions est alors~:
\begin{equation}
\begin{gathered}
\forall o \in O, \forall e = (o,o_\emptyset) \text{ ou } e = (o_\emptyset,o),\\
\Phi(e) =  \frac{1}{2}
\end{gathered}
\end{equation}
En effet, dans ce cas, $m_e(\Omega) = 1$ et le conflit est nul. La probabilité pignistique de $\omega_e$ vaut alors $\frac{1}{2}$.


\myparagraph{Paramétrage des mesures}
La probabilité d'une apparition ou d'une disparition est donc indépendante des mesures ou de la nature des observations, ce qui permet un paramétrage simple des mesures de similarité d'identité et de succession temporelle. En effet, l'existence d'un processus spatio-temporel de cardinalité supérieure à 1: 0 ou 0: 1 est vraie, pour une mesure donnée, si celle-ci est au moins supérieure à $\frac{1}{2}$. En dessous, la mesure considère que les observations ne sont pas liées.
\\
Ainsi, nous avions présenté (voir le paragraphe \ref{par:succession}) la mesure de succession fondée sur le rang des sources, qui dépendait d'un paramètre $\epsilon$. Nous avions dit que la détermination de ce paramètre dépendait d'une valeur limite entre vraisemblance et non vraisemblance d'une relation de filiation. Cette valeur correspond à la vraisemblance d'une apparition ou d'une disparition. En effet, si une relation est jugée peu vraisemblable, alors il est préférable de ne pas lier les observations. Nous n'avons introduit pour l'instant un paramètre uniquement pour la mesure $c_{rank}$, mais toutes les mesures doivent être paramétrées de la même façon. En particulier, les mesures de similarité d'identités doivent être paramétrables afin qu'elles puissent s'adapter aux spécificités des observations qu'elles évaluent. La mesure $c_{ant}$ valant déjà $\frac{1}{2}$ dans les cas d'incertitude sur l'antécédence (voir le chapitre 4), elle ne nécessite pas de paramétrage supplémentaire.
 \\
La façon habituelle de déterminer une limite entre un appariement acceptable ou non pour un critère donné est de fixer un seuil au delà duquel l'appariement de deux objets est rejeté. Par exemple, si l'on cherche les homologues de deux bases de données composées d'objets ponctuels à l'aide d'une distance euclidienne, le seuil de cette distance pourra être la somme des précisions planimétriques de chaque base de données. Au delà de cette limite les objets sont arbitrairement jugés non homologues. 
\\
La détermination de $\epsilon$, pour chaque mesure, dépend de la nature de celle-ci. Cependant, il est important que cette détermination soit simple pour qu'un utilisateur construisant un graphe de filiations soit capable de maîtriser le comportement des mesures qu'il utilise. En particulier, le sens de la valeur de $\epsilon$ fixée par l'utilisateur doit être intuitif. 
\\
Pour $c_{rank}$, le paramètre $\epsilon$ peut être écrit $\epsilon = \frac{-s_{max}}{\log(0.5)}$. $s_{max}$ est alors le \textbf{nombre de sauts maximum} avant de considérer une relation comme non vraisemblable. La figure~\ref{figure:parametre} illustre le comportement de $c_{rank}$ lorsque l'on considère qu'une relation de filiation devient non vraisemblable si elle ''saute'' au dessus de deux sources géohistoriques.
\begin{figure}
\centering
\includegraphics[width=1\textwidth]{parametre.png}
\caption{Exemple de paramétrisation de la mesure de succession temporelle $c_{rank}$.}
\label{figure:parametre}
\end{figure} 

\myparagraph{Vraisemblance d'un hypergraphe de filiation}
Maintenant que nous avons défini $\Phi$, nous pouvons construire la fonction de vraisemblance $V$ d'un hypergraphe. Pour cela, nous considérons que les processus décrits par les hyperarcs sont indépendants les uns des autres \footnote{Ceci n'est vrai que parce que l'on ne traite de processus spatio-temporels que l'on a choisi de représenter comme élémentaires.}. Cela signifie que l'agrégation de deux processus spatio-temporels ne conduit pas à la création d'un nouveau processus plus vraisemblable étant donné les connaissances dont on dispose sur les observations et les transformations de l'espace. Le produit des probabilités pignistiques de tous les arcs d'un hypergraphe fournit donc la probabilité que cet hypergraphe soit vraisemblable. Le nombre d'arcs d'un hypergraphe peut être élevé, ce qui pose des problèmes de calculs de ce produit. Une façon habituelle de contourner ce problème consiste à calculer la somme des logarithmes de ces probabilités.
\\
Le fait de rechercher l'hypergraphe le plus vraisemblable par optimisation signifie qu'il faut être capable de comparer la vraisemblance de deux hypergraphes. Or, le nombre arcs variant d'un hypergraphe à l'autre, ils ne sont pas comparables. Une façon simple de permettre cette comparaison est d'attribuer la probabilité de chaque arc aux observations qui le composent. Ainsi, la vraisemblance d'un hypergraphe devient la probabilité pour que chacune de ses observations appartienne au processus spatio-temporel décrivant le plus vraisemblablement la transformation de cette observation. 
\\
Nous pouvons finalement exprimer $V(H_f)$ pour tout hypergraphe $H_f=(O,E,S,\mathcal{T})$ admissible~:
\begin{equation}
log \, V(H_f) = \sum\limits_{e\in E} log(\Phi(e))|e|
\end{equation}
$|e|$ désignant le nombre d'observations simples et non nulles  composant $e$.


\myparagraph{Vers un problème de minimisation}
Dans la section suivante, nous proposons une méthode de résolution du problème d'optimisation discrète que nous venons de présenter. Pour que cette résolution soit possible, il est nécessaire de transformer le problème de maximisation en un problème de minimisation. Le passage peut être effectué en mesurant non plus la vraisemblance d'une hyperarc $\omega_e$ mais l'hypothèse inverse. Il s'agit finalement de minimiser l'invraisemblance d'un hypergraphe. Ce passage peut être réalisé en modifiant seulement la fonction $V$ qui devient~:
\begin{equation}
log \, Iv(H_f) = \sum\limits_{e\in E} log(1-\Phi(e))|e|
\end{equation}
Le problème d'optimisation devient alors~:
\begin{equation}
\argmin \{log \,Iv(H_f) : H_f \in \mathcal{H}\}
\end{equation}
Formalisé ainsi, un hypergraphe de filiation peut être construit si l'on dispose des éléments suivants~:
\begin{itemize}
\item un ensemble d'observations $O$ dont les schémas sont alignés,
\item une définition de l'identité des observations de $O$,
\item un ensemble de mesures évaluant la similarité des attributs composant les identités des observations,
\item un coefficient de fiabilité associé à chaque mesure;
\end{itemize}
%
%\myparagraph{Remarques sur la pertinence de la modélisation par optimisation}
%On a dit que l'approche d'ana Maria posait souci lorsque l'on veut apparier plusieures bases de données et détecter des liens de cardinalité sup 1:1. En effet pour détecter ces liens elle propose deux choix : 
% - recomposer les 1:N après coup
%  - inclure des propositions ET dans le cadre de discernement
%Or, ceux 2 prop. posent pb~:
% - ds le premier cas, les observations agrégées qui correspondent au N apportent plus d'information sur la transformation que le le fait 2 1:1
%  - ds le 2e cas, le power set devient $\Omega \Omega$, sachant qu'en plus dans sont cas ce Omega n'est qu'entre 2 sources. Mm si avec les sources spécialisées seules les hypothèses singletin (qui sont donc des prop simples ou des ET) sont jugées, ça en fait 

\subsection{Un recuit simulé mono-objectif pour découvrir $H_f^*$}
Résoudre le problème de découverte des relations de filiation entre observations revient à trouver le minimum global de la fonction objectif présentée dans le paragraphe précédent. À ce minimum global correspond le graphe $H_f^*$ décrivant les processus spatio-temporels les plus vraisemblables entre les différentes observations. Afin de déterminer ce graphe il faut explorer un espace de recherche constitué de solutions possibles et à choisir la solution minimisant la fonction objectif. L'énumération complète des solutions possibles n'étant pas envisageable, l'approche usuelle consiste alors à explorer cet espace de rechercher de façon efficace. Parmi les différentes méthodes existantes, la classe des \textbf{métaheuristiques} permet de résoudre des problèmes d'optimisation pour lesquels \emph{on ne connaît pas d'algorithmes permettant de repérer un optimum global à coup sûr et en un nombre fini de calculs}\citep{Dreo2003}. Nous sommes typiquement dans un tel cas, en particulier lorsque plusieurs ordres temporels sont possibles entre observations. 

\myparagraph{Méthodes de résolution de problèmes d'optimisation combinatoire}
Résoudre un problème d'optimisation discrète pour lequel on connaît une fonction objectif consiste à trouver, au sein d'un ensemble discret et fini de solutions appelé \textbf{espace de recherche}, la \textbf{solution optimale} minimisant (ou maximisant, selon le problème) cette fonction objectif. Lorsque l'espace de recherche est très grand, la difficulté est alors de trouver la solution optimale du problème dans un temps raisonnable. L'idée générale de toutes les méthodes de résolution de problèmes d'optimisation est de parcourir l'espace de recherche du problème de façon efficiente; seule la stratégie adoptée diffère. Deux grandes classes de méthodes existent~:
\begin{enumerate}
\item Les méthodes dites exactes qui consistent à parcourir entièrement l'espace de recherche. Afin d'accélérer le parcours, ces méthodes cherchent à détecter au plus tôt les zones de l'espace de recherche ne contenant pas la solution optimale pour les éliminer. La programmation dynamique et la méthode \emph{branch and bound} sont les deux représentantes les plus connues de cette classe de méthodes. L'avantage majeur des méthodes exactes est qu'elles assurent de trouver la solution optimale du problème.
\item Les méthodes approchées sont utilisées lorsque le problème est de grande taille et que l'on peut se contenter d'une solution proche de l'optimum. Le principe des méthodes approchées est d'éviter un parcours exhaustif de l'espace de recherche à l'aide d'heuristiques diverses. Parmi les méthodes approchées, la classe des \textbf{métaheuristiques} rassemble des méthodes génériques qui  offrent une stratégie générale permettant de guider la recherche d'une solution optimale pour un problème d'optimisation spécifique. Plutôt de de méthodes ''clé en main'', il s'agit de cadres de conception de méthodes approchées pour un problème donné. Le schéma générale d'une métaheuristique consiste à choisir\footnote{Généralement de manière aléatoire} une ou plusieurs solutions dans l'espace de recherche, puis à parcourir cet espace en appliquant de légères modifications aux solutions initiales pour obtenir de nouvelles solutions, de nouveau modifiées, jusqu'à atteindre une solution jugée suffisamment proche de l'optimum global. Les métaheuristiques se répartissent selon le nombre de solutions qu'elles traitent. Ainsi, les métaheuristiques \textbf{globales} (ou distribuées) tentent de parcourir l'espace de recherche avec plusieurs solutions en parallèle, tandis que les méthode \textbf{locales} (ou de voisinage) ne s'appuient que sur une solution. S'il existe diverses métaheuristiques (voir \citep{Dreo2003}), le \textbf{recuit simulé} (local) et les \textbf{algorithmes évolutionnaires} (globaux) sont les plus généralement utilisés. Le principale avantage des méthodes approchées est leur capacité à traiter de problèmes de très grande taille. Quant aux métaheuristiques, elles fournissent un cadre de résolution générique qui leur permet d'être appliquées à des problèmes variés ou difficiles à définir formellement. La non-optimalité de ces méthodes est cependant leur défaut majeur. De plus, elles demandent de définir des heuristiques - notamment pour le parcours de l'espace de recherche- qui peuvent être difficiles à maitriser.
\end{enumerate}

\myparagraph{Choix d'une méthode}
Le premier choix à effectuer porte sur l'utilisation d'une méthode exacte ou approchée. Si l'utilisation de méthodes exactes assurerait de trouver l'optimum global, elles sont mal adaptées à notre problème. Tout d'abord, la taille de l'espace de recherche devient importante lorsque le nombre de sources et d'observations géohistorique augmente et nous ne disposons pas de critères permettant d'éviter le parcours de la majeure partie de l'espace de recherche. De plus, notre objectif est de mettre en place une méthode générique pouvant être adaptée à des observations de différentes natures. Bien que toutes les mesures (d'identité et temporelles) soient synthétisées sous la forme de croyance, la forme des graphes de filiation peut  varier selon la nature des observations\footnote{Par exemple, des bâtiments n'évoluent pas comme des parcelles. Les premiers auront tendance à apparaître et disparaître tandis que que les secondes se réorganisent.}. Par conséquent, la forme de l'espace de recherche varie également selon les observations considérées, ce qui gène d'autant plus la mise en place de critères évitant un parcours exhaustif. Enfin, l'optimalité n'est pas une nécessité absolue, d'autant plus que le résultat nécessitera le plus souvent des corrections manuelles, les observations et les mesures ne transcrivant pas toute la complexité des phénomènes représentés. Nous nous dirigeons donc vers une méthode approchée.
\\
Parmi les méthodes approchées, le cadre générique offert par les métaheuristiques est particulièrement pertinent pour les raisons que nous venons d'énoncer. De nombreux travaux ont porté sur la comparaison entre métaheuristiques globales et locales pour différentes classes de problèmes~\citep{Manikas1996, Rossi2003, Fouskakis2002, Kohonen1999} et ont montré qu'il n'y a pas de règle claire permettant de connaître a priori la méthode la mieux adaptée à un problème donné, chacune ayant des avantages et des manques dont les effets sont difficiles à estimer. \cite{Fouskakis2002} ont toutefois montré que l'hybridation de deux  métaheuristiques était une piste intéressante qui permet de combler les manques des méthodes ainsi combinées. Les principales méthodes hybrides peuvent être trouvées dans~\citep{Talbi2002}.
\\
Dans son article, \cite{Kohonen1999} rapproche les deux métaheuristiques les plus utilisées, le recuit simulé et l'algorithme génétique (membre des méthodes évolutionnaires), et montre que leurs apparentes différences cachent en réalité des comportements relativement similaires. En effet, tous deux utilisent un parcours de l'espace de recherche par voisinage, supposant ainsi que de meilleures solutions existent à proximité des solutions connues. Il montre par ailleurs que lorsque cette hypothèse se vérifie, une optimisation locale est avantageuse. La méthode du recuit simulé est donc préférable quand le problème peut être vu comme une suite d'optimisations locales.
\\
La découverte des relations de filiation est un problème d'optimisation intrinsèquement local. En effet, chaque processus spatio-temporel ne concerne qu'un petit nombre d'observations et n'occupe qu'une petite partie du graphe total. \textbf{Nous choisissons donc le recuit simulé comme méthode de résolution du problème de découverte des relations de filiations d'un graphe géohistorique.}

\myparagraph{Principe du recuit simulé}
Le recuit simulé est une métaheuristique locale dont le principe consiste à se déplacer itérativement de voisins en voisins dans l'espace de recherche en sélectionnant à chaque étape un voisin de la solution courante comme nouvelle solution selon une distribution de probabilité qui dépend de la qualité des différents voisins \citep{Kirkpatrick1983}. Le schéma du recuit simulé est illustré en figure~\ref{figure:simulatedannealing} pour le cas de la découverte des relations de filiation. Ainsi, les meilleurs voisins\footnote{Le recuit simulé étant classiquement adapté à des problèmes de minimisation, un voisin est meilleur si la valeur de la fonction objectif est plus faible.} ont une plus grande probabilité d'être choisi comme nouvelle solution courante. Afin d'éviter d'être piégé dans des minimas locaux de la fonction objectif, le recuit simulé comporte un paramètre $T$ nommé \textbf{température} dont le rôle est le suivant~:
\begin{itemize}
\item si $T$ est grand, tous les voisins d'une solution ont environ la même probabilité d'être sélectionnés comme nouvelle solution,
\item si $T$ est petit, la probabilité de choisir un voisin de mauvaise qualité est faible,
\item si $T$ est nul, seuls les voisins améliorant la fonction objectif ont une probabilité non nulle d'être choisis. 
\end{itemize}
La température d'un recuit simulé décroit avec le temps selon une loi empiriquement déterminée. Plusieurs lois de décroissance de la températures ont été proposées, les principales pouvant être trouvées dans \citep{Nourani1998}. Les lois de décroissance linéaires, géométriques et quadratiques sont les plus couramment utilisées.
\\
Si l'on note $E$ la fonction objectif (aussi appelée énergie), $x$ la solution courante, $x'$ une solution voisine et $\bigtriangleup E = E(x')-E(x)$ la différence d'énergie entre les deux solutions, la probabilité d'accepter $x'$ comme nouvelle solution courante est généralement donnée par le \textbf{critère de Metropolis}~:
\begin{equation}
p(\bigtriangleup E) = e^{(-\frac{\bigtriangleup E}{T})}
\end{equation}
La solution $x'$ est alors acceptée avec une certaine probabilité pouvant être non nulle même si elle dégrade la fonction objectif si la température est élevée.
Le recuit simulé poursuit sa recherche jusqu'à atteindre un critère d'arrêt qui peut être~:
\begin{itemize}
\item un nombre maximal d'itérations,
\item un critère de stagnation (temps ou nombre d'itérations sans amélioration significative de la solution),
\item un temps maximal.
\end{itemize}
Le recuit simulé est une méthode simple, dont la convergence théorique vers la solution optimale a été démontrée et étudiée dans de nombreux travaux (voir \citep{Henderson2003}). Cependant, plusieurs de ses paramètres doivent être fixés de manière empirique~:
\begin{itemize}
\item la loi de décroissance de température,
\item la température initiale d'une optimisation,
\item les critères d'arrêt.
\end{itemize} 
La détermination de ces paramètres est une étape cruciale car elle conditionne la convergence du recuit simulé. En effet, si la température reste trop élevé durant toute l'optimisation, le recuit simulé sera équivalent à une marche aléatoire. À l'inverse si sa décroissance est trop rapide, le risque est alors de se bloquer dans un minimum local. Une température initiale trop haute ou trop basse générera les mêmes effets. Quant aux critères d'arrêts, ils doivent être déterminés de façon à aboutir à un compromis entre temps de calcul et convergence de l'algorithme. 

\myparagraph{Application à l'identification des filiations}
Nous avons choisi d'utiliser la méthode du recuit simulé pour réaliser la phase de découverte des relations de filiations au sein d'un ensemble d'observations. Le schéma~\ref{figure:simulatedannealing} présente le déroulement général d'un algorithme de recuit simulé adapté à notre problème. Dans les paragraphes suivants, nous en détaillons les étapes principales qui doivent être crées pour mettre en place un tel algorithme. En particulier, nous nous concentrons sur les quatre éléments majeurs de la méthode~:
\begin{itemize}
\item la génération du voisinage d'une solution,
\item la détermination de la température initiale,
\item le choix d'une loi de décroissance de température,
\item la détermination de la solution initiale.
\end{itemize}
\begin{figure}
\includegraphics[width=1\textwidth]{./simulated_annealing_gh.png}
\caption{Schéma de l'algorithme de recuit simulé pour la découverte des relations de filiation.}
\label{figure:simulatedannealing}
\end{figure}

\myparagraph{Génération du voisinage d'un hypergraphe de filiations}
La stratégie de génération du voisinage d'une solution joue un rôle important dans l'efficacité d'une optimisation par recuit simulé. Pour \cite{Alizamir2008}, cette stratégie doit être efficace et efficiente. L'efficacité renvoie ici à sa capacité à générer des solutions couvrant l'intégralité de l'espace de recherche. L'efficience désigne sa capacité à se déplacer \emph{de façon adéquate} dans l'espace de recherche, c'est à dire en sortant rapidement des minimas locaux tout en atteignant rapidement les bonnes solutions. Cette efficience dépend du nombre d'étapes nécessaires pour se déplacer d'une solution à une autre (vitesse), de la taille du voisinage d'une solution, de la quantité d'information modifiée par un déplacement dans l'espace de recherche et de la difficulté de calcul des voisins. S'il est toujours préférable de minimiser le nombre de déplacements nécessaires pour aller d'une solution à un autre, les autres éléments de l'efficience sont spécifiques au problème, sont parfois contradictoires et nécessitent des compromis~\citep{Nourani1998}. 
\\
Afin de former une stratégie respectant le mieux possible ces différents aspects, nous générons le voisinage d'un hypergraphe de filiation $H_f$ en appliquant une modification élémentaire à l'un des processus spatio-temporel qu'il décrit afin de créer un nouvel hypergraphe admissible $H'_f$. Ceci revient en réalité à ajouter ou supprimer une relation de filiation du graphe $G_f$ correspondant à $H_f$ sous les contraintes d'admissibilité décrites dans le paragraphe~\ref{paragraph:admissible}. Pour transcrire ces deux actions au niveau d'un hypergraphe de filiation, les modifications doivent être effectuées sur les hyperarcs du graphe. Quatre modifications permettent de transcrire l'action d'ajout ou de suppression d'une relation de filiation~:
\begin{enumerate}
\item \textbf{l'extension} d'un hyperarc consiste à étendre un hyperarc en ajoutant une nouvelle observation dans sa queue ou sa tête.
\item \textbf{la réduction} d'un hyperarc consiste à retirer une observation de sa queue ou de sa tête.
\item \textbf{la fusion} de deux hyperarcs aboutit à la création d'un nouvel hyperarc de cardinalité n : m.
\item \textbf{la scission} d'un hyperarc aboutit à la création de deux nouveaux hyperarcs.
\end{enumerate}
Puisque nous considérons depuis le début que toute observation $o$ d'un hypergraphe de filiation est toujours connectée à une autre observation ou à l'observation nulle, elle appartient nécessairement à deux hyperarcs $e_1=(Q_1, T_1)$ et $e_2 = (Q_2,T_2)$ où $o \in T_1$ et $o \in Q_2$. Dès lors, les quatre modifications fixées peuvent s'écrire comme la fusion de deux hyperarcs voisins. Nous définissons le voisinage de deux hyperarcs de la façon suivante~: \textbf{deux hyperarcs sont voisins si les observations agrégées de leur queues (et resp. de leurs têtes) sont connectées spatialement et sont issues de la même source}. Ainsi, cela signifie que que si deux hyperarcs sont voisins, toutes les observations de leurs têtes respectives sont issues de la même source et qu'au moins deux d'entre elles sont connectés spatialement. Il en va de même pour leurs têtes. Nous considérons de plus que l'observation nulle est connectée spatialement à toutes les autres, et qu'elle appartient à toutes les sources indistinctement. La figure~\ref{figure:hypervoisins} fournit deux exemples de voisinages, l'un pour deux arcs de cardinalité 1 : 1 et l'un avec un arc comprenant l'observation nulle. Les relations spatiales sont figurées par des liens en vert.
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{./hypervoisins.png}
\caption{Deux cas d'hyperarcs voisins}
\label{figure:hypervoisins}
\end{figure}
Nous fixons alors comme contrainte sur les modifications le fait qu'elles s'appliquent uniquement sur des hyperarcs voisins. Ainsi, un arc ne peut être étendu qu'avec des observations membres d'un arc voisin de celui-ci. Il en va de même pour la modification de fusion. Nous plaçons cette contrainte de façon à éviter de créer des hyperarcs ne traduisant aucun processus spatio-temporel crédible. Par exemple, il est illogique que deux parcelles éloignées de plusieurs kilomètres se trouvent fusionnées en une seule parcelle.
\\
Exprimées de cette façon, les quatre modifications peuvent toutes être décrites comme des opérations d'union d'hyperarcs voisins ou de division d'un hyperarc en deux arcs voisins. La figure~\ref{figure:table_moves} illustre ainsi les opérations équivalentes à chaque modification sur des exemples.
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{./table_modifs.png}
\caption{Illustration des modifications d'extension, de réduction, de fusion et de scission.}
\label{figure:table_moves}
\end{figure}

\myparagraph{Création d'hypergraphes admissibles}
L'objectif étant de créer des hypergraphes admissibles, les modifications doivent être effectuée de façon à créer de nouveaux hyperarcs respectant les contraintes $C_1, C_2$ et $C_3$ fixées dans le paragraphe \ref{paragraph:admissible}. Ceci est surtout valable pour les modifications d'extension et de fusion qui créent ajoutent des observation à des arcs existants. Il est possible de vérifier si une telle modification aboutit à un  hypergraphe admissible en éliminant lors de la création des modifications d'extension et de fusion les hyperarcs voisins dont l'agrégation aboutirait à un nouvel arc violant les contraintes d'admissibilité. Il y a donc préalablement à la création de toute modification un filtrage des hyperarcs candidats, de façon à assurer la production de nouveaux hypergraphes admissibles. Ce filtrage consiste à vérifier qu'une fois la modification appliquée à un hyperarc $e$, l'hyperarc résultat $e'$ respecte les contraintes $C_1, C_2$ et $C_3$ définies sur tout hypergraphe de filiation. La procédure générale de génération d'une modification d'un hyperarc quelconque $e$ est décrite dans le schéma~\ref{figure:selection_e} et se compose des étapes suivantes~:
\begin{enumerate}
\item Une étape de filtrage des hyperarcs candidats afin de créer un nouvel hypergraphe admissible.
\item Après filtrage, un candidat est choisi aléatoirement. S'il ne reste aucun candidat, la création de la modification échoue. 
\item Enfin, la modification est créée. Une modification peut être stockée en conservant l'hyperarc modifié qu'il faudra supprimer, et en créant les nouveaux arcs résultant de la modification.
\end{enumerate}
Les quatre modifications possibles et les contraintes qui s'appliquent sur elles sont détaillés dans les paragraphes suivants.
\begin{figure}
\includegraphics[width=1\textwidth]{./selection_e.png}
\caption{Schéma général de la création d'une modification d'un hypergraphe de filiation.}
\label{figure:selection_e}
\end{figure}

\myparagraph{Extension d'un hyperarc}
Étendre un hyperarc $e$ consiste fusionner un hyperarc avec un arc voisin de cardinalité 0 :1 ou 1 : 0. Deux cas de figure peuvent se présenter~: soit l'hyperarc est un appariement de cardinalité supérieure ou égale à 1 :1, soit il décrit un apparition ou une disparition et lie donc une observation à l'observation nulle. Dans le second cas, il s'agit d'un cas particulier de l'extension qui revient à créer un nouvel appariement entre deux observations que l'on désigne comme une modifications spécifique de \textbf{naissance} d'un appariement. Dans ce cas précis, il est nécessaire de retirer l'observation nulle de l'hyperarc résultant de la fusion.
\\
L'étape de filtrage dépend du cas de figure~:
\begin{enumerate}
\item S'il s'agit d'une naissance, l'hyperarc à étendre est de la forme $e=(\{o_1\},\{o_\emptyset\})$ ou $e=(\{o_\emptyset\},\{o_1\})$. Si $e$ est de la première forme, les hyperarcs candidats sont tous ses voisins de la forme $e_2 =(\{o_\emptyset\},\{o_2\})$. La situation est symétrique si $e$ est de la seconde forme. Les hyperarcs voisins doivent ensuite être filtrés pour ne conserver que ceux qui permettent de créer un nouvel arc $e' = (\{o_1\},\{o_2\})$ ou $e' = (\{o_2\},\{o_1\})$ admissible. La contrainte $C_2$ est dors et déjà vérifiée pour tous les arcs voisins. La contrainte $C_1$ peut être aisément vérifiée en calculant l'antécédence de $o_1$ et $o_2$. Pour vérifier $C_3$, il faut vérifier qu'il n'existe pas de chemin dans le graphe allant de l'observation constituant la tête de $e'$ vers celle constituant sa queue. Un exemple de filtrage dans le cas d'une naissance est illustré en figure~\ref{figure:filtrage_naissance}. Les observations candidates valides pour l'hyperarc $e$ sont indiquées par un contour rose pointillé, celles invalides étant en rouge. Les relations spatiales existantes sont indiquées par des liens en vert.
\item Si $e$ est de cardinalité supérieure ou égale à 1 : 1, les arcs candidats sont tous ses voisins de cardinalité 1: 0 ou 0: 1. Le filtrage des candidats valides peut être effectué de la même façon que pour la naissance. Un exemple de filtrage dans le cas d'une extension est illustré en figure~\ref{figure:filtrage_extension}. La légende est similaire à l'exemple donné pour la naissance.
\end{enumerate}

\begin{figure}[ht]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{filtrage_naissance.png}
				\caption{Filtrage pour une naissance.}
                \label{figure:filtrage_naissance}
        \end{subfigure}%v
        ~\quad
        \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{filtrage_extension.png}
				\caption{Filtrage pour une extension simple.}
                \label{figure:filtrage_extension}
        \end{subfigure}
        \label{fig:filtrages_extension}
	    \caption{Exemples de l'étape de filtrage pour une modification de type ''extension''. Seul l'hyperarc concerné par la modification est montré.}
\end{figure}
Une fois le filtrage effectué, la modification peut être créée en sélectionnant de manière aléatoire un arc voisin candidat et valide $e_i$ puis en fusionnant $e$ et $e_i$. Avec $e= (Q,T)$ et $e_i=(Q_i, T_i)$, cette fusion consiste à créer un nouvel hyperarc $e'=(Q \bigcup Q_i \setminus \{o_\emptyset\}, T \bigcup T_i\setminus\{o_\emptyset\})$.
\myparagraph{Réduction d'un hyperarc}
La réduction d'un hyperarc $e$ est une modification symétrique de l'extension. Là où l'extension permettait d'ajouter une observation à un arc existant, la réduction permet de retirer une observation de $e$. Ceci revient à diviser $e$ en deux nouveaux arcs $e_1'$ et $e'_2$, le premier étant de cardinalité 0 :1 / 1: 0 et le second d'une cardinalité supérieure ou égale à 0 :1 / 1: 0. De la même façon que l'extension pouvait aboutir à une naissance, la réduction peut créer une \textbf{mort}, c'est à dire à division d'un arc en deux nouveaux arcs correspondant à une apparition et une disparition.
\\
Contrairement à l'extension, la réduction opère sur les observations composant l'arc. Les candidats ne sont donc pas des arcs voisins de $e$ mais des arcs qu'il est possible de créer à partir des observations membres de sa queue et de sa tête, ce qui revient plus simplement à choisir une observation de $e$ à lui retirer. Le fait de gérer l'extension comme une agrégation d'arcs voisins permet d'obtenir des hyperarcs dont les sous-graphe du graphe des relations spatiales $G_s$ induits par les observations composant sa tête et sa queue ne possèdent qu'une seule composante connexe\footnote{Ceci traduit une hypothèse implicite sur les processus spatio-temporels~: ils concernent des observations liées spatialement.}. Pour conserver la symétrie avec la modification d'extension, la réduction doit préserver cette propriété. Pour cela, le filtrage doit éliminer les observations de l'hyperarc qui ne sont pas dans la frontière intérieure\footnote{La frontière intérieure des sommets du sous-graphe $W$ d'un graphe $V$ désigne tous les sommets de $W$ étant connectés à au moins un sommet de $V \smallsetminus W$} des sous-graphes spatiaux induits\footnote{Un sous-graphe induit $W$ d'un graphe $V$ est un sous-graphe défini par un ensemble de sommets. Les arêtes de $W$ correspondent aux arêtes de $V$ incidentes à deux sommets de $W$.} par sa queue et  sa tête. Ainsi, nous assurons de toujours conserver une seule composante connexe. Enfin, nous interdisons le fait de retirer une observation d'un hyperarc décrivant une apparition ou une disparition, afin de ne pas créer d'hyperarcs du type $e=(\{o_\emptyset\},\{o_\emptyset\})$. La figure~\ref{figure:filtrage_reduction} illustre le filtrage des candidats dans le cas d'une réduction.
\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{./filtrage_reduction.png}
\caption{Exemples de l'étape de filtrage pour une modification de type ''réduction''.}
\label{figure:filtrage_reduction}
\end{figure}
Tout comme l'extension, la réduction peut être créée après filtrage par sélection aléatoire d'une observation candidate et valide $o_i$ puis en divisant $e$ en deux nouveaux arcs. Si $e$ est de la forme $e=(Q,T)$ et si $o_i \in Q$, les deux arcs créés sont $e_1=(\{o_i\}, \{o_\emptyset\})$ et  $e_2=(Q \setminus \{o_i\}, T)$. Si $o_i \in T$,  $e_1=(\{o_\emptyset\},\{o_i\})$ et  $e_2=(Q, T \setminus \{o_i\})$.

\myparagraph{Fusion de deux hyperarcs}
Créer une nouvelle relation de filiation entre deux observations de sources différentes peut aboutir à la fusion de deux hyperarcs. La modification de fusion consiste à agréger deux hyperarcs voisins $e_1$ et $e_2$ de cardinalités supérieures ou égales à 1 : 1. Il en résulte un nouvel arc $e'$ de cardinalité n : m, avec $n = |Q(e_1)|+|Q(e_2)|$ et $m = |T(e_1)|+|T(e_2)|$. 
\\
L'étape de filtrage consiste à éliminer les arcs candidats ne respectant pas les contraintes $C_2$ ou $C_3$. La vérification du respect de ces deux contraintes est toutefois plus complexe que dans les cas précédents. Pour $C_3$, il est nécessaire de vérifier qu'il n'existe aucun chemin menant de la tête de $e_2$ vers la queue de $e_1$ ou de la tête de $e_1$ vers la queue de $e_2$. Il y a donc au plus $n+m$ explorations à effectuer. Toutefois, sauf dans le cas de graphes dont les observations présentent des transformations particulièrement complexes, le nombre de chemins existant à partir d'une observation est relativement faible. Pour $C_2$, une méthode simple consiste à calculer l'intersection des temporalités des observations de $Q(e_1) \bigcup Q(e_2)$ puis celle des temporalités de  $T(e_1) \bigcup T(e_2)$. Si l'une des deux est vide, la contrainte n'est pas respectée.
\\
La figure~\ref{figure:filtrage_fusion} présente un exemple de filtrage d'une modification de fusion. Les arcs $e_1$ et $e_2$ ne peuvent être fusionnés car ils ne sont pas voisins. Les arcs $e_1$ et $e_3$ sont pour leur par des candidats valides êtres fusionnés.
\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{./filtrage_merge.png}
\caption{Exemples de l'étape de filtrage pour une modification de type ''fusion''.}
\label{figure:filtrage_fusion}
\end{figure}
À partir d'un hyperarc $e_1$ fixé, la création d'une modification de fusion est effectuée en sélectionnant un hyperagraphe voisin valide de $e_2$ puis en créant un nouvel hyperarc $e'$ dont la tête et la queue sont formées de l'union des têtes et queues de $e_1$ et $e_2$. 
\\
Cet arc est de la forme $e'=(Q(e_1)\bigcup Q(e_2), T(e_1 \bigcup T(e_2))  $


\myparagraph{Scission d'un hyperarc}
La dernière modification possible est la scission d'un hyperarc $e$ en deux autres hyperarcs de cardinalité supérieure ou égale à 1 : 1. Puisque nous nous assurons de toujours manipuler des hypegraphes admissibles, aucune contrainte ne doit être vérifiée pour diviser un arc. Cependant, si l'on veut obtenir deux nouveaux arcs n'étant pas des apparitions ou des disparitions, l'arc $e$ doit nécessairement être de cardinalité n : m. Pour la scission, l'étape de filtrage consiste donc simplement à vérifier la cardinalité de $e$~: si elle ne convient pas, l'arc ne peux être scindé.
\\
La génération des candidats pour une scission est particulière puisque cette modification consiste à diviser un arc en deux nouveaux arcs de cardinalité moindre. Il faut donc scinder la queue de $e$ en deux sous-ensembles non vides $Q_1$ et  $Q_2$ puis faire de même pour sa tête pour obtenir $T_1$ et $T_2$. Deux nouveaux arcs $e'_1$ et $e'_2$ peuvent être alors crées à partir de ces quatre sous ensembles en associant ceux-ci deux à deux. Deux problèmes se posent cependant. D'une part, nous souhaitons conserver des hyperarcs dont les observations sont connectées spatialement de façon à former de nouveaux arcs cohérents avec la façon de traiter les autres modifications. De plus, plusieurs couples d'arcs peuvent être formés une fois la tête et la queue scindée. Il faut donc effectuer un choix sur la façon d'associer ces sous-ensembles en nouveaux arcs.
\\
Un sous-ensemble d'observations respecte la contrainte de connexion spatiale si le sous-graphe spatial induit par ces observations est connexe. Diviser un ensemble d'observations en deux sous-ensembles dont les éléments sont connectés revient donc a effectuer une coupe dans le sous-graphe spatial induit de façon à obtenir exactement deux composantes connexes. Une façon naïve de réaliser cette opération consiste à retirer du sous-graphe des arcs jusqu'à obtenir deux composantes connexes\footnote{Il s'agit de fait d'une coupe aléatoire.}. De cette façon, nous obtenons des sous-ensembles $Q_1$,$Q_2$, $T_1$ et $T_2$ dont les sous-graphes spatiaux induits sont connexes. Afin d'associer les ensembles d'observations ainsi créés en nouveau arcs, nous proposons de créer aléatoirement deux couples $(Q,T)$, ce qui permet de créer une scission aléatoire d'un hyperarc. La figure~\ref{figure:random_cut} présente un exemple du découpage d'un hyperarc $e$ en deux nouveaux arcs en effectuant une coupe aléatoire des sous-graphes spatiaux induits par $Q(e)$ et $T(e)$. Une autre possibilité serait d'associer les ensembles $Q_.$ et $T_.$ dont les observation sont les plus proches spatialement. Cependant, le comportement stochastique de la modification serait partiellement perdu. 
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{./random_cut.png}
\caption{Exemples de la scission d'un hyperarc en utilisant une coupe aléatoire dans les sous-graphes spatiaux induits par sa tête et sa queue.}
\label{figure:random_cut}
\end{figure}

\myparagraph{Génération d'un hypergraphe voisin du graphe courant}
Dans les paragraphes précédents nous avons expliqué comment pouvaient être créées des modifications sur un hyperarc donné d'un hypergraphe de filiation de façon à générer un nouvel hypergraphe admissible. Le voisinage d'un hypergraphe est donc formé de tous les hypergraphes admissibles résultants de l'application d'une modification à l'un de ses hyperarcs. Au cours du recuit simulé, il n'est pas nécessaire de produire l'ensemble du voisinage de l'hypergraphe courant\footnote{Ce qui ne serait de toute façon pas raisonnable en termes de temps de calcul.} mais seulement de tirer aléatoirement un hypergraphe voisin qui sera évalué par le critère de Metropolis. La manière de sélectionner un hypergraphe voisin d'un hypergraphe donné est illustré dans le schéma~\ref{figure:gen_voisin}.
\begin{figure}
\includegraphics[width=1\textwidth]{./selection_gh.png}
\caption{Génération et sélection d'une solution du voisinage de la solution courante.}
\label{figure:gen_voisin}
\end{figure}

\myparagraph{Évaluation de la solution voisine choisie}
L'étape d'évaluation d'un hypergraphe $H'_f$ sélectionné aléatoirement comme nouvelle solution du recuit simulé consiste à calculer la  fonction $Iv(H'_f)$. Ce calcul a été détaillé dans la section précédente et nécessite d'agréger les observations constituant les extrémités des arcs de $H'_f$. Le critère de Metropolis peut ensuite être appliqué en comparant la valeur de $Iv(H'_f)$ avec $Iv(H_f)$ , $H_f$ étant le graphe courant. La probabilité d'accepter $H'_f$ comme solution courante de l'algorithme est donc~:
\begin{equation}
p(\bigtriangleup Iv) = e^{(-\frac{Iv(H'_f) - Iv(H_f)}{T})}
\end{equation}

\myparagraph{Loi de décroissance de la température}
La loi de décroissance de la température du recuit $f(T)$ est un élément crucial de la conception d'un algorithme efficace. Elle doit permettre une décroissance de la température au fil du temps suffisamment lente pour permettre à l'algorithme de converger vers une solution proche de l'optimum, tout en permettant cette convergence dans un temps raisonnable. Le choix d'une loi de décroissance repose sur un compromis entre qualité de la solution finale et rapidité de l'algorithme et dépend donc du type de problème, de sa complexité et des contraintes sur son temps de réponse. Dans notre cas, le temps de calcul nécessaire pour la découverte des relations de filiation n'est pas un critère bloquant puisqu'il peut être effectué une seule fois pour plusieurs interprétations de l'identité. De plus, nous visons des solutions proches de l'optimum pour réduire les corrections manuelles et, surtout, éviter les cas de sous-appariement. Nous choisissons pour cela une loi de décroissance géométrique proposée par \cite{Kirkpatrick1983}, de la forme~:
\begin{equation}
f(T) = T_0 \mu^i
\end{equation}
où $i$ est l'itération courante et $\mu$ est un facteur de décroissance dont la valeur est définie entre 0 et 1. Il a été montré expérimentalement que la décroissance géométrique a un comportement optimal si $\mu$ est proche de $1$ \citep{Kirkpatrick1983}.
\\
Dans un processus de recuit simulé classique, la température ne décroit que par paliers et reste fixe pour un certain nombre d'itérations. Ce nombre d'itérations étant un paramètre empirique, \cite{Lundy1986} montrent qu'il peut être supprimé à condition de faire décroître la température lentement à chaque itération. Notons enfin que le nombre d'itérations à chaque palier de température peut être modifié durant le déroulement de l'algorithme. L'idée est alors d'effectuer de descendre rapidement la température lorsque celle-ci est élevée, puis d'allonger petit à petit la durée de chaque palier au fur et à mesure que la température diminue. De cette façon, le parcours de l'espace de recherche est dans un premier temps proche d'une marche aléatoire, puis le rayon de recherche va petit-à-petit se resserrer autour d'une zone contenant de bonnes solutions.

\myparagraph{Température initiale $T_0$}
La température initiale d'un recuit simulé est également un paramètre critique qui conditionne la convergence vers des bonnes solutions. Si celle-ci est trop faible, le risque est d'être rapidement bloqué dans un minimum local de l'espace de recherche. Trop élevée, elle empêchera l'algorithme de converger. Ce paramètre peut être fixé de manière empirique par expérimentation, où être calculé a priori lors d'une phase consistant à explorer l'espace de recherche afin de déterminer une température initiale assurant un certain taux d'acceptation de mauvaises solutions. Différents travaux se sont penchés sur cette questions. \cite{Rayward1996} proposent ainsi d'effectuer un premier recuit avec une température initiale très élevée et décroissant rapidement jusqu'à ce qu'environ 60\% de solutions dégradant la fonction objectif soient acceptés. La température correspondant à cet instant est ensuite utilisée comme température initiale d'un second recuit plus lent. \cite{Dowsland2012} proposent un processus similaire consistant à effectuer un premier recuit en augmentant rapidement la température jusqu'à ce qu'une certaine proportion de mauvaises solutions soient acceptées, puis de décroître lentement la température afin de trouver la solution optimale. Citons également~\cite{BenAmeur2004} qui introduit une méthode permettant d'estimer une température initiale assurant un taux d'acceptation de 80\% d'une solution de l'espace de recherche. Comme les autres, cette méthode consiste à parcourir l'espace de recherche à haute température, puis à faire décroître celle-ci jusqu'à obtenir un taux d'acceptation moyen de 0.8. Dans notre cas, nous choisissons de nous appuyer sur la proposition de \cite{BenAmeur2004} afin de fixer la température initiale.


\myparagraph{Génération du graphe initial $H_{f0}$}
La solution initiale utilisée pour amorcer l'optimisation peut être fixée -par exemple un graphe vide- ou choisie de manière aléatoire. Cette seconde possibilité assure une meilleure couverture de l'espace de recherche sur plusieurs lancements de l'algorithme. Pour générer un hypergraphe de filiation initiale aléatoire, une solution est d'effectuer une marche aléatoire dans l'espace de recherche en réalisant une première phase de recuit à haute température. En particulier, la solution initiale peut être celle crée par l'étape d'évaluation de la température initiale. 

\myparagraph{Conclusion sur la découverte des relations de filiation}
Nous avons proposé dans cette section une méthode automatique s'appuyant sur un recuit simulé mono-objectif afin de réaliser la phase de découverte des relations de filiations. Cette phase est une étape d'appariement de données géographiques qui s'appuie sur une connaissance des processus spatio-temporels entre observations géohistoriques et sont modélisés à l'aide d'un hypergraphe de filiations\footnote{Les méthodes d'appariement de données géographiques s'appuient sur différentes mesures de similarité entre objets. Celles-ci dépendant des données, elles seront abordée ultérieurement lors de l'application de l'approche aux observations sur Paris. Seules les mesures portant sur le temps sont génériques et sont données dans la section suivante pour tout type d'objet géographique.}. La résolution de cette phase d'appariement par un processus de recuit simulé nous permet de créer un hypergraphe finale $H_f$ optimal ou proche de l'hypergraphe optimal $H*_f$. Cependant, l'hypergraphe produit ne décrit pas réellement les processus spatio-temporels à l'\oe uvre car il n'a  pas connaissance du type des relations de filiations. En effet, la phase d'appariement permet de détecter les observations liées par une relation de filiation mais elle ne détermine pas si ces relations sont des continuations ou des dérivations. Dans ce but, nous introduisons une étape de typage des relations de filiations qui ont été identifiées lors de la phase d'appariement. 

\section{Typage des relations de filiations}
\label{section:tagging}
Une fois l'étape de découverte des relations de filiation effectuée, nous disposons d'un hypergraphe représentant des groupes d'observations affiliées. Le type de ces relations de filiations dépend d'une définition précise de l'identité des observations qui n'a pas été nécessairement définie au moment de l'appariement. Ceci permet, à partir d'un seul appariement, de produire plusieurs graphes géohistoriques selon une certaine vision de l'identité des observations par un utilisateur. Dans cette section, nous présentons une approche permettant d'instancier un graphe de filiations à partir d'un hypergraphe de filiation à l'aide d'une comparaison entre les identités des observations qui ont été liées. De façon à être cohérente avec la phase d'appariement et pour prendre en compte les incertitudes liées aux observations, cette comparaison s'appuie également sur la théorie des croyances.
\\
Le typage des relations de filiations découvertes est un processus en 3 étapes, résumé par le schéma~\ref{figure:schema_typage}. Nous présentons dans cette section les différentes étapes de ce processus. Dans un premier temps, nous présentons la création d'un graphe de filiation  non typé $G$ à partir d'un hypergraphe de filiations. Ensuite, nous proposons une approche permettant d'étiqueter chaque arc de ce graphe selon le type de filiation qu'elle représente à l'aide de la théorie des croyances et d'un ensembles de mesures sur l'identité des observations. Enfin, une dernière étape permet de résoudre les conflits générés par cette phase de typage de façon à créer un graphe de filiation valide. Une fois ces étapes réalisées, nous disposons d'un graphe géohistorique complet compatible avec le modèle de \cite{DelMondo2011}.
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{./schema_typage.png}
\caption{Schéma général de la phase de typage des filiations.}
\label{figure:schema_typage}
\end{figure}

\subsection{Décomposition d'un hypergraphe $H_f$}
La première étape du processus de typage des relations de filiations consister à décomposer l'hypergraphe de filiation construit $H_f=(O,E_H)$ par optimisation en un graphe orienté simple $G(O, A)$ composé d'arcs correspondant à des relations de filiations entre observations. 
\\
Cette décomposition, illustrée en figure~\ref{figure:decomposition}, peut être effectuée en deux étapes~:
\begin{enumerate}
\item Un graphe vide $G(O,\emptyset)$ est créé.
\item Pour tout hyperarc $e=(Q,T)$ de $H_f$, le produit cartésien $Q \times T$ produit $|Q|.|T|$ couples d'observations. Pour chacun de ces couples est ajouté dans $G$ un arc liant les deux observations membres du couple.
\end{enumerate}
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{./decomposition.png}
\caption{Décomposition d'un hypergraphe de filiation en graphe simple.}
\label{figure:decomposition}
\end{figure}
Ainsi, nous obtenons un premier graphe $G$ non étiqueté qui peut servir de support à la création d'un graphe de filiation.

\subsection{Étiquetage des filiations par fusion d'informations incertaines}
Le graphe $G$ produit décrit une ensemble relations de filiations dont la nature doit être précisée. Cette nature dépend de l'identité des observation -et donc de leur nature-, mais également d'une certaine vision de la transmission de l'identité par un utilisateur. Si l'on prend le cas de rues, un utilisateur s'intéressant à l'évolution de leur nommage considérera le nom comme membre de l'identité, un changement de nom entre deux rues résultera en une dérivation. Si seule la morphogenèse du réseau viaire est étudiée, un changement de nom n'aura aucun impact, la filiation résultante sera une continuation.  Dans les deux cas, le graphe $G$ support est le même, seule la définition de l'identité change.
\\
Enfin, le graphe créé par décomposition de $H_f$ contient un certain nombre d'arcs qu'il n'est pas toujours souhaitable de conserver. Le graphe $G$ est une structure générale qui doit être raffinée. Ceci est particulièrement vrai pour les hyperarcs de cardinalité n : m dont la décomposition crée artificiellement des arcs n'ayant pas toujours de sens. Enfin, le processus d'appariement par optimisation peut aboutir à des situations de sur-appariement, en particulier avec des arcs n :m lorsque la transformation des observations est ambiguë (en raison des imprécisions, du manque de connaissances, etc.). Nous introduisons pour ces deux raisons la possibilité d'étiqueter une arc de $G$ comme une relation erronée devant être supprimée du graphe de filiation final.
\\
L'étape de découverte des relations de filiations s'appuie sur la théorie des croyances afin d'identifier les observations liées. Cette théorie offrant l'avantage de modéliser l'incertitude et l'imprécision des informations, elles est également pertinente pour étiqueter les arcs de $G$. Dans cette section, nous présentons une procédure d'étiquetage des filiations utilisant la théorie des croyances.

\myparagraph{Fonction d'étiquetage}
Nous définissons trois étiquettes $\{\delta, \gamma, \epsilon \}$ pouvant être assignées à toute arc du graphe $G$, avec la signification suivante~:
\begin{itemize}
\item Une arête portant l'étiquette $\delta$ est relation de type 
dérivation,
\item Une arête portant l'étiquette $\gamma$ est relation de continuation,
\item Une arête portant l'étiquette $\epsilon$ est relation erronée en représentant pas de réelle filiation.
\end{itemize}
Nous introduisons une fonction d'étiquetage d'un arc $a \in A$ de $G=(O,A)$
\begin{equation}
{r} : A \to \{\gamma, \delta, \epsilon\}
\end{equation}
Nous devons maintenant détailler cette fonction afin de pouvoir étiqueter tout graphe $G$ construit à partir d'un hypergraphe de filiation.

\myparagraph{Sources d'informations pour l'étiquetage}
Tout comme pour l'étape de découverte, l'étiquetage repose donc sur des mesures de similarité d'identité. Ces mesures sont spécifiques à la nature des observations et donc au cas applicatif. Celles-ci peuvent être différentes des mesures utilisées dans l'étape de découverte des filiations. En effet, si elles comparent également des éléments de l'identité de deux observations, elles visent un objectif plus précis que lors de l'appariement. Il est cependant nécessaire que tous les éléments de l'identité soient comparés pour que le typage des arcs soit cohérent avec la définition de l'identité des observations spécifiée en amont de la découverte des relations de filiation.

\myparagraph{Cadre de discernement}
Identifier la nature exacte d'une relation de filiation revient à comparer les identités des observations de l'arc de $G$ décrivant cette relation. Tout comme pour la phase de découverte, cette comparaison peut s'appuyer sur un ensemble de mesures entre les membres de l'identité des observations en jeu. Nous avons alors pour chaque arc de $G$ un problème de décision multicritère, que nous souhaitons résoudre à l'aide de la théorie des croyances. Notons que nous sommes cette fois dans un cas de monde fermé, la décision du type de chaque arc ne dépendant que des observations qui constituent ses extrémités. L'étiquetage étant de la même forme pour tous les arcs du graphe, nous pouvons réduire le problème de décision à un arc $a \in A$ seul.
\\
Nous définissons pour $a$ un cadre de discernement $\Omega  = \{\gamma,\delta,\epsilon\}$ correspondant aux étiquettes qui peuvent lui être assignées. Le problème est donc de choisir une étiquette maximisant la croyance d'un ensemble de sources d'informations sur le type de $a$.

\myparagraph{Masses de croyance}
Nous ne définissons cette fois pas de modèle spécifique pour les fonctions de masse. Ainsi, il est possible que des sources d'information sur l'identité placent une croyance sur des disjonctions de propositions dans $\Omega$. Considérons par exemple le cas de deux parcelles cadastrales dont l'identité est formée de leur géométrie et de leur numéro, liées par une relation de filiation dont le type est inconnu. Un changement de numéro de parcelle ne signifie pas nécessairement que la relation est une dérivation~: cela dépend en réalité de ce que l'on étudie. Par contre, un numéro identique est un indice tendant à dire qu'il s'agit de la même parcelle et que la relation est une continuation. Dans ce cas, une source d'information  pourrait avoir le comportement suivant~:
\begin{itemize}
\item si les numéros sont identiques, la source croit fortement en la proposition$\{\gamma\}$.
\item si les numéros sont différents, la source est incapable de dire s'il s'agit d'une continuation ou d'une dérivation. Sa croyance serait alors placée sur la disjonction $\{\gamma,\delta\}$.
\end{itemize}
Comme les fonctions de masse pour le typage ne suivent aucun modèle précis, leur définition dépend totalement du type d'observation et de la nature de la source d'information. Ces fonctions de masse doivent donc être définies pour une application précise. Ainsi, les fonctions de masse pour le typage des relations dans le cas des rues de Paris sont présentées dans la section~\ref{section:massetypage}.

\myparagraph{Fusion des masses et décision}
Comme nous l'avons dit, nous sommes cette fois dans le cas d'un monde fermé. La fusion de l'ensemble des masses de croyances définies sur les éléments de $2^\Omega$ doit être effectuée à l'aide de l'opérateur conjonctif de Dempster défini précédemment.
\\
La dernière étape après fusion des croyances consiste à décider de l'étiquette assigner à l'arc, et donc du type de relation. Afin de prendre en compte les masses de croyance placées sur les disjonctions de propositions, nous proposons d'utiliser le principe du maximum de probabilité pignistique. Ce mode de prise de décision consiste à choisir la proposition de $\Omega$ dont la probabilité pignistique est la plus grande. 

\subsection{Construction d'un graphe géohistorique valide}
L'étiquetage des arcs de $G$ ne suffit pas pour produire un graphe de filiations $G_f$ valide. En effet, chaque arc ayant été considéré comme un problème de fusion d'information indépendant, nous pouvons obtenir des graphes dans lesquels une observation est en relation de continuation avec plusieurs autres observations. Ceci a pourtant été interdit (voir le chapitre 5) pour conserver une cohérence du graphe vis-à-vis de la notion d'identité. En effet, un tel cas de figure signifierait que plusieurs observations d'une même sources géohistorique partagent la même identité.
Nous proposons ici une étape de post-traitement visant à résoudre ce type de conflit.
 
\myparagraph{Résolution des conflits liés aux continuations multiples}
Une façon simple de résoudre les conflits dans le graphe créé consiste à identifier les sous-ensembles d'arcs de $G$ en conflit. Pour chaque sous-ensemble, il ne faut conserver qu'un seul arc étiqueté comme continuation. Pour déterminer cet arc, il suffit de ré-appliquer le principe de maximum de probabilité pignistique. Tous les autres arcs sont alors étiquetés comme dérivations.

\myparagraph{Niveau de détail}
Cette méthode simple est toutefois naïve car elle ne permet pas de détecter les situations où les conflits correspondent en réalité à des changements de niveau de détail entre deux sources géohistoriques. Nous n'avons toutefois pas exploré plus en profondeur la phase de résolution des conflits afin de corriger ce type de cas. Une possibilité serait d'analyser les écarts entre probabilités pignistiques des arcs en conflit et, en dessous d'un certain seuil, considérer qu'il s'agit d'un cas de changement de niveau de détail.


\section{Mise en \oe uvre et résultats~: application aux rues de Paris}
\label{section:application}
Dans cette section, nous appliquons l'approche de construction de graphe géohistoriques aux réseaux des rues de Paris extrait des différents atlas parisiens traités dans les chapitres précédents. 

\subsection{Implémentation de l'approche de construction de graphes géohistoriques}
Un prototype expérimental de l'approche proposée a été implémentée au sein de la plateforme SIG libre Geoxygene~\footnote{\url{http://oxygene-project.sourceforge.net/}}. Cette plateforme est développée en Java au sein du laboratoire COGIT de l'Institut National de l'Information Géographique et Forestière~\footnote{\url{http://recherche.ign.fr/labos/cogit/accueilCOGIT.php}}
. Nous avons mis en place les deux modules principaux de découverte des relations de filiation et de création d'un graphe de filiation par typage. Ces deux modules interagissent avec la base de données spatiale et temporelle stockant les \emph{snapshots} sur les rues de Paris. De plus, nous stockons les graphes créés à l'issue des deux modules également dans une base de données relationnelle. Ces deux bases de données ont été mises en place à l'aide du SGBD Postgresql muni de l'extension Postgis pour les données géographiques et du module développé par \cite{VanDaele2014} pour la représentation de sous-ensembles flous.
\\
Notre implémentation utilise de plus certaines bibliothèques externes à Geoxygene afin d'intégrer la théorie des croyances, de manipuler efficacement des hypergraphes et enfin de manipuler des sous-ensembles flous. Concernant la théorie des croyances, nous nous appuyons sur la bilbiothèque libre Java Evidence4j\footnote{\url{https://github.com/IGNF/evidence4j}}\footnote{Elle même fondée sur la bibliothèque C++ Evidenz\citep{evidenz}.} intégrant toutes les fonctionnalités nécessaires. La modélisation de l'hypergraphe de filiation et des graphes créés lors de l'étape de typage s'appuient sur la bibliothèque opensource Grph\footnote{\url{http://www.i3s.unice.fr/~hogie/grph/}} développée au laboratoire I3S de l'université de Sophia Antipolis. Enfin, pour manipuler des sous-ensembles temporels flous, nous avons utilisé la bibliothèque également libre FuzzyJ\footnote{\url{http://rorchard.github.io/FuzzyJ/}} développée par le \emph{National Research Council Canada}.
\\
L'architecture générale du prototype développé est illustré en figure~\ref{figure:implementation}.
\\
Rajoutons également que le code de ce prototype devrait être déposé en OpenSource rapidement.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{./implementation.png}
\caption{Schéma général du prototype implémenté.}
\label{figure:implementation}
\end{figure}

\subsection{Données utilisées}
Les quatre atlas dont les rues ont été vectorisés sont les suivants~:
\begin{itemize}
\item l'atlas de Verniquet, dont le temps valide est le sous-ensemble flou (1783, 1785, 1791, 1799),
\item l'atlas par îlots de Vasserot, entièrement vectorisé par \cite{ALPAGE}, dont nous avons fixé le temps valide (1808, 1810, 1836, 1854),
\item l'atlas de Jacoubet, de temps valide (1825, 1827, 1836, 1837),
\item l'atlas municipal de Paris de l'année 1888, de temps valide (1887, 1888, 1889),
\end{itemize}
Le temps valide désigne la période pendant laquelle les observations des entités du monde réel sont effectuées puis transcrites dans l'atlas. Par exemple, nous considérons que l'atlas de Verniquet décrit des entités du monde réel ayant existé entre 1783 et 1799. La certitude de cette hypothèse n'est pas toujours totale, ce qui est exprimé par l'utilisation des sous-ensembles temporels flous. Ainsi, nous sommes sûrs que l'atlas de Verniquet ait pu permettre l'observation d'entités du monde réel entre 1785 et 1791 c'est à dire la période de levé topographique et de dessin de l'atlas. Avant 1785 et après 1791, il est de moins en moins certain que des relevés ou des corrections aient été effectuées.
\\
La figure~\ref{figure:temps_valides_tous_plans} illustre la localisation temporelle de ces différents atlas. On peut remarquer que le temps valide de l'atlas de Jacoubet est totalement inclus dans celui de l'atlas de Vasserot. Cela signifie que la méthode de construction du graphe considérera comme possible l'existence de relations de filiations allant de Vasserot à Jacoubet ou de Jacoubet vers Vasserot.
\\
Nous n'avons pas l'information détaillée des temps valide pour chaque observations vectorisée des rues de Paris. Bien que ces informations puissent être extraites de sources telles que les dictionnaires de rues (voir \citep{Lazare1844, Lazare1855}), leur saisie dépasse de la cadre de cette thèse. Pour cette raison, \textbf{nous avons considéré que les observations des rues de Paris ont le même temps valide que leur source géohistorique}.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{./ftime_all_rappel.png}
\caption{Temps valides des différents atlas.}
\label{figure:temps_valides_tous_plans}
\end{figure}

\myparagraph{Représentation des réseaux viaires dans la base de données spatiale et temporelle}
Les réseaux de rues vectorisés et présentés en fin du chapitre 3 sont des graphes planaires topologiquement justes. Chaque réseau de rue vectorisé a été stocké comme un \emph{snapshot} vecteur de notre base de données géohistorique. Les observations géohistoriques de ces \emph{snapshots} sont des tronçons de rues, et non des rues au sens commun du terme (c'est-à dire un ensemble contigu de tronçons portant le même nom). Dans la base de données spatiale et temporelle, ces observations sont modélisées sous la forme de \emph{feature} linéaires\footnote{\emph{Linestrings} du schéma simple feature access}. La nature de ces observations est importante à garder en tête, en particulier pour l'étape de typage des filiations. En effet, ce ne seront alors pas les relations de filiations entre rues qui seront identifiées, mais les relations de filiation entre tronçons. Les transformations de l'espace qui en résultent sont donc également entre tronçons. La figure~\ref{fig:troncons} illustre ainsi le découpage du réseau viaire en tronçons de rues représentés sous la forme de \emph{features} linéaires. Cette figure présente tout d'abord le schéma de donnée aligné des tronçons de rue issus des différents \emph{snapshots}que nous utilisons pour tester notre méthode (\ref{figure:schema_aligne_troncons}), puis illustre le découpage du réseau en tronçons pour les \emph{snapshots} des tronçons de Verniquet, Vasserot et Jacoubet (\ref{fig:extract_tronc}). Enfin, un extrait du snapshot des tronçons de l'atlas de Verniquet est présenté en figure \ref{fig:extract_snap}. Les champs \emph{vtime} (temps valide) et \emph{the\_geom} (géométrie) sont figurés sous forme textuelle pour la lecture. Rajoutons que l'alignement des schémas des réseaux viaires a été réalisé manuellement au moment du versement des réseaux dans la base de données.
\begin{figure}[ht!]
        \centering
        \begin{subfigure}[b]{0.28\textwidth}
                \includegraphics[width=\textwidth]{./schema_bd_tronc.png}
				\caption{Schéma commun des tronçons des réseaux viaires de Paris.}
                \label{figure:schema_aligne_troncons}
        \end{subfigure}%v
        \\
        \begin{subfigure}[b]{1\textwidth}
                \includegraphics[width=\textwidth]{./extract_tronc_3src.png}
				\caption{Extraits des réseaux viaires stockés.}
                \label{fig:extract_tronc}
        \end{subfigure}
        \\
        \begin{subfigure}[b]{1\textwidth}
                \includegraphics[width=\textwidth]{./extract_bd_vern.png}
                \caption{Extrait de la base de données spatiale et temporelle.}
                \label{fig:extract_snap}
        \end{subfigure}
	    \caption{Modélisation des réseaux viaires de Paris dans la base de données spatiale et temporelle}
	            \label{fig:troncons}
\end{figure}

\myparagraph{Décalages planimétriques}
En raison des erreurs de levé topographique et des erreurs résiduelles dues au géoréférencement des planches des atlas, les réseaux de rue présentent des décalages. L'amplitude de ces erreurs peut varier fortement selon les zones de l'espace parisien, allant de quelques mètres dans le centre parisien de la rive droite à plus de 10 mètres pour les impasses, passages et cul-de sacs souvent présents dans les îlots anciens. Ces décalages peuvent être également dû aux élargissements des voies. En effet, puisque seuls les axes centraux des rues ont été vectorisés, un élargissement à droite ou à gauche d'une rue décale son axe. Deux situations sont présentées en figure \ref{fig:extrait_decalage}, avec tous les réseaux superposés. À gauche, la partie nord de l'île Saint-Louis, presque inchangée, présente des décalages faibles. À l'inverse, la vignette de droite montre les décalages de rues et passages à l'intérieur d'un îlot, où les décalages planimétriques approchent 10 mètres. L'erreur est particulièrement importante pour les tronçons issus de l'atlas de Jacoubet (en orange).

\begin{figure}
\includegraphics[width=1\textwidth]{./ex_decalages.png}
\caption{Illustration des décalages planimétriques entre réseaux viaires sur l'île saint Louis et l'îlot de la trinité.}
\label{fig:extrait_decalage}
\end{figure}

\myparagraph{Identité des tronçons de rues }
Nous utilisons dans cette section la géométrie et le nom de la rue comme éléments de l'identité d'un tronçon. Les attributs sont 'nom\_entier' et 'the\_geom' sont les deux membres de l'identité de tout tronçon de rue de Paris. L'attribut 'nom\_entier' contient le nommage complet de la rue, avec sa nature et sa dénomination (par exemple ''rue Neuve Saint-Martin''. La géométrie du tronçon décrit la forme de son axe central et sa localisation.

\myparagraph{Espaces parisiens étudiés}
Pour présenter et discuter les résultats obtenus par notre approche sur les réseaux de rues de Paris, nous nous focalisons sur deux zones de Paris ayant subit des transformations différentes. De plus, nous souhaitons comparer les résultats avec une vérité terrain créée manuellement dont nous ne pouvons disposer que pour des espaces réduits en raison du volume de données. En effet, créer une vérité terrain consiste à créer dans la base de données spatio-temporelle un graphe de filiation pour les quatre plans.  De plus, cette vérité terrain doit s'appuyer sur des connaissances historiques des transformations issues d'experts historiens.
\\
Nous avons choisi de traiter deux zones de Paris de taille différente. Tout d'abord, nous nous intéresserons aux transformations de l'intérieur de l'îlot parisien de la trinité décrit dans~\citep{Gribaudi2009}. Les différents états de l'îlot au travers de nos plans sont visibles en figure~\ref{figure:trinité} présentant une transformation typique des anciens enclos religieux du centre de Paris. Nous traiterons plus particulièrement l'intérieur de cet îlot, dont les réseaux vectorisés dans la partie inférieure de la figure. Cet îlot est un espace occupé pendant l'Ancien Régime par l'enclos de l'Hôpital de la Trinité, supprimé en 1790. L'hôpital lui-même est détruit en 1817 et laisse place à des immeubles d'habitation et des ateliers d'artisans. La structure viaire de l?îlot change peu jusqu'aux interventions haussmanniennes au cours desquelles il est presque intégralement détruit en raison du percement du boulevard Sébastopol. Seule la partie Ouest de l'îlot échappe à la destruction, ce qui épargne des deux passages permettant d'atteindre ses rues internes.
\begin{figure}
\includegraphics[width=1\textwidth]{./trinite2.png}
\caption{Les états successifs de l'îlot de la trinité et les réseaux vectorisés correspondants.}
\label{figure:trinité}
\end{figure}
Le second espace que nous traiterons est le quartier des Arts et Métiers, entourant immédiatement l'actuel Conservatoire National des Arts et Métiers situé dans le 3\textsuperscript{eme} arrondissement. Les différents états du quartier sont illustrés en figure~\ref{figure:stmartin}. Il s'agit ici aussi d'un ancien enclos religieux appartenant à l'abbaye de Saint-Martin-des-Champs et de son abord immédiat. Le quartier présente cette fois des transformations plus variées. Tout d'abord, malgré l'ouverture de l'enclos vers l'extérieur avec la Révolution, les bâtiments de l'abbaye ont été pour la plupart conservés pour devenir le Conservatoire National. Une partie des rues est restée presque inchangée, en particulier dans la partie Nord du quartier. La partie Sud est à l'inverse presque entièrement détruite par le percement de la rue Réaumur entre 1854 et 1858. Seule la rue Aumaire, au Sud, est partiellement épargnée et existe encore aujourd'hui. La partie Est subit des transformations plus progressives, avec l'ouverture de rues et d'un marché à la place de l'ancien enclos dans un premier temps, puis des opérations d'alignement et de fusion des rues lors du Second Empire. À l'Ouest, la rue Saint-Martin est seulement alignée.
\begin{figure}
\includegraphics[width=1\textwidth]{./stmartin.png}
\caption{Les états successifs du quartier des Arts et Métiers et les réseaux vectorisés correspondants.}
\label{figure:stmartin}
\end{figure}


\subsection{Fonctions d'agrégation des membres de l'identité des tronçons}
Lors de l'évaluation de la vraisemblance d'un hyperarc durant l'étape de découverte des filiations, il est nécessaire d'agréger les observations membres composant la queue et la tête de l'hyperarc. De cette façon, l'estimation de la vraisemblance d'un arc revient à un problème classique de recherche d'objets homologues. Cette vraisemblance étant calculée seulement à partir des attributs constituant l'identité des observations membres de l'hyperarc, il est seulement nécessaire d'agréger ces attributs ainsi que le temps valide des observations. Dans le cas précis des réseaux viaires de Paris, tous les tronçons ont le temps valide de leur source, il n'y a donc pas besoin d'agréger ces temporalités\footnote{Ceci est vrai car les hyperarcs ne peuvent être multi-sources.}. Nous n'avons donc besoin de définir que deux fonctions d'agrégations, l'une pour les géométries des tronçons et l'autre pour les noms des rues. Nous présentons ici les deux méthodes choisies.

\myparagraph{Fonction d'agrégation des géométries des tronçons de rue}
Une première fonction d'agrégation doit permettre de fusionner les géométries des tronçons de rue constituant une extrémité d'un hyperarc de filiation. La façon dont nous parcourons l'espace de recherche des hypergraphes de filiation nous assure que chaque extrémité d'un arc est constituée de tronçons connectés spatialement. Cependant, la configuration de ces tronçons dans le réseau peut être quelconque. Il n'est donc pas toujours possible d'effectuer une simple union des géométries en une unique polyligne (c'est par exemple le cas dans une configuration en 'T').
\\
Afin de pouvoir agréger toute configuration de tronçons, nous utilisons la méthode de création de \emph{routes naturelles} introduit par \cite{Jiang2008}. Une route naturelle est, dans un graphe routier, le chemin emprunté par un individu partant d'un point donné et parcourant le réseau en cherchant à conserver un trajet le plus rectiligne possible. Il s'agit donc d'agréger des tronçons consécutifs de façon à former un ensemble de lignes polygonales correspondant à des routes naturelles. L'algorithme proposé consiste à former ces chemins de manière itérative en agrégeant de proche en proche des tronçons connectés. Pour choisir à chaque intersection quels tronçons seront agrégés, l'auteur propose trois stratégies. La première, \emph{best fit}, consiste choisir le tronçon à agréger aléatoirement parmi ceux formant l'intersection. La stratégie \emph{self fit} consiste à conserver pour la route naturelle en cours de formation le tronçon formant un angle minimal. Enfin, la stratégie \emph{every-best-fit} consiste à calculer le meilleur compromis à chaque intersection. Nous illustrons en figure~\ref{figure:ebf} le résultat de la formation des routes naturelles sur un ensemble de tronçons en utilisant la stratégie \emph{every-best-fit}.
\begin{figure}
        \centering
        \begin{subfigure}[b]{0.33\textwidth}
                \includegraphics[width=\textwidth]{jiang1.png}
				\caption{Tronçons initiaux}
                \label{figure:noebf}
        \end{subfigure}%v
        ~
        \begin{subfigure}[b]{0.33\textwidth}
                \includegraphics[width=\textwidth]{jiang2.png}
				\caption{Routes naturelles (\emph{every best fig})}
                \label{figure:ebf}
        \end{subfigure}
        \label{fig:ebf}
	    \caption{Agrégation des tronçons de rues par création de routes naturelles. Chaque couleur correspond à une polyligne.}
\end{figure}
Cette méthode d'agrégation peut aboutir à la création de plusieurs polylignes agrégées lorsque la configuration des tronçons agrégés ne forme pas un chemin unique. Les mesures de similarité d'identité portant sur la géométrie doivent donc être capables de comparer des observations agrégées dont les géométries sont multiples.


\myparagraph{Fonction d'agrégation des noms de rues}
Le second membre de l'identité des tronçons de rue est le nom de la rue à laquelle ils appartiennent. Lorsque les noms de rue des tronçons à agréger sont les mêmes, il suffit d'éliminer les doublons. Lors de la formation d'une observation agrégée, il est possible que des tronçons de rues de noms différents doivent être agrégés. Dans ce cas, la fonction d'agrégation des noms de rue renvoie un l'ensemble des noms de rues différents portés par les tronçons agrégés. Tout comme pour l'agrégation géométrique, les mesures de similarité portant sur ces noms doivent pouvoir comparer des ensembles de noms.


\subsection{Mesures de similarité d'identité utilisées pour la découverte des relations de filiation}
\label{section:simRues}
Nous avons défini le schéma des observations sur les tronçons de rue de Paris que nous utilisons dans cette section. Pour qu'il soit possible de découvrir les relations de filiations qui existent entre elles, nous devons définir des mesures de similarité d'identité permettant de comparer les attributs composant leur identité. Puisque nous avons défini l'identité d'un tronçon de rue comme étant formé du nom de rue et de sa géométrie, nous devons disposer de mesures de similarité pouvant comparer des géométries linéaires ainsi que des noms de rues représentés sous forme de chaînes de caractères. 
\\
Une fois ces mesures déterminées, elles doivent être intégrées dans des fonctions de masses de croyance respectant le modèle défini dans le paragraphe~\ref{paragraph:modcroyances}.
\\
Concernant les mesures de succession temporelle, elles ne dépendent pas du cadre applicatif, ce qui nous permet de réutiliser les deux mesures définies pour le cadre général (voir le paragraphe~\ref{par:succession}).

\subsubsection{Mesure de similarité entre géométries linéaires et fonction de masse associée}
L'objectif est de définir des mesures capables d'estimer si les géométries des observations agrégées d'un hyperarc sont suffisamment similaires pour que l'on puisse considérer que les observations simples qui le compose sont affiliées. Au niveau des observations agrégées, cela revient à une mesure de similarité classique. Pour comparer deux lignes polygonales, la plupart des méthodes d'appariement de données géographiques s'appuient sur la distance de Hausdorff ou de Fréchet \citep{Alt1995}. La distance de Hausdorff est une mesure du plus grand écart entre deux positions au sein des polylignes. Pour expliquer le comportement de la distance de Fréchet entre deux courbes, l'analogie suivante est souvent proposée \citep{Devogele2000, Mascret2006}~: un homme promène son chien en laisse, chacun suivant l'une des courbe sans pouvoir reculer mais en avançant ou en s'arrêtant chacun librement. La distance de Fréchet est alors la longueur minimale de la laisse qui permet au maître et au chien de parcourir leur courbe. La distance de Fréchet est définie dans un cadre continu, mais une discrétisation en a été proposée par \citep{Alt1995} puis  \citep{Devogele2000}. \cite{Devogele2000} a par ailleurs montré que cette distance était mieux adaptée pour comparer des lignes sinueuses, et correspondant davantage à l'intuition humaine. La figure~\ref{figure:frechet} illustre graphiquement le calcul de la distance de Fréchet $d_f$ entre deux polylignes.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{/frechet.png}
\caption{Exemple de calcul de la distance de Fréchet entre deux lignes polygonales.}
\label{figure:frechet}
\end{figure}
À partir de cette distance, nous pouvons construire une mesure de similarité d'identité entre géométries linéaire. Cette \textbf{mesure de Fréchet}, notée $c_f$ est donnée entre deux observations $o_1$ et $o_2$ par 
\begin{equation}
\begin{gathered}
c_f(o_1, o_2) = e^{- \frac{K}{\epsilon_f}}\\
K = sup \{d_f(U_g(o_1),U_g(o_2))\}
\end{gathered}
\end{equation}
où $\epsilon_f$ est un paramètre de seuil permettant de fixer la distance à partir de laquelle la filiation entre $o_1$ et $o_2$ ne sera plus jugée vraisemblable d'après cette mesure. La limite entre vraisemblance et non vraisemblance étant fixée à $0.5$ par la probabilité pignisitique des hyeperarcs décrivant des apparitions et des disparitions d'observations, ce paramètre peut être déterminé si l'on connaît la distance $df_{max}$ à partir de laquelle une filiation n'est plus jugée vraisemblable. On a en effet $\epsilon_f = -df_{max} / ln(0.5)$. La mesure de Fréchet peut donc être paramétrée par $df_{max}$ dont le sens est plus intuitif.
\\
$K$ est la plus grande distance de Fréchet calculée entre les lignes polygonales composant les géométries des observations agrégées $o_1$ et $o_2$.
\\
La mesure de Fréchet étant une mesure de similarité d'identité, la fonction de masse qui lui est associée suit le modèle 1 de \cite{Appriou1991}. De plus, nous considérons cette mesure totalement fiable pour notre cas applicatif ($\alpha = 1$). Pour tout hyperarc $e$  d'un hypergraphe de filiation, cette fonction de masse est don ~:
\[
\begin{cases}
m_{e_i}(\omega_e) =  c_f(o_1,o_2) \\
m_{e_i}(\neg \omega_e) =  1-c_f(o_1,o_2) \\
m_{e_i}(\Omega) = 0 \\
\end{cases}
\]
où $o_1$ est l'observation agrégée de $Q(e_i)$ et $o_2$ celle de $T(e_i)$. 

\subsubsection{Mesure de similarité entre noms de rues}
Pour mesurer une similarité entre noms de rues, nous réutilisons la similarité de chaînes de caractères proposée par \cite{Samal2004} fondée sur une distance de Damerau-Levenshtein. Cette dernière est une extension de la distance d'édition de Levenshtein prenant également en compte l'inversion de deux caractères adjacents. La similarité proposée , que l'on note $s_r$, consiste pour deux chaînes de caractère à découper chacune d'elle en mot, puis à calculer une matrice de similarité entre mots des deux chaînes à partir de la distance de Damerau-Levenshtein\footnote{La distance entre deux mots est transformée en une similarité par normalisation par le nombre de caractère du plus long mot.}. La similarité finale est la moyenne des valeurs des cellules maximisant les similarités inter-mots, sans réutiliser deux fois le même mot\footnote{Une seule cellule sélectionnée par ligne et par colonne}.
\\
La mesure de similarité entre noms de rues $c_r$ est alors~:
\begin{equation}
\begin{gathered}
c_{r}(o_1, o_2) = e^{- \frac{K}{\epsilon_r}}\\
K = sup \{s_r(U_{nom\_entier}(o_1),U_{nom\_entier}(o_2))\}
\end{gathered}
\end{equation}
De la même manière que pour la mesure de Fréchet, $\epsilon_r$ est un paramètre de la mesure indiquant un seuil de vraisemblance. Il peut également être déterminé à l'aide d'une distance d'édition maximale $dr_{max}$.
\\
Les noms de rues sont cruciaux pour savoir si une relation de filiation est une continuation ou de dérivation. Ils doivent impérativement être pris en compte au moment du typage des filiations. Lors de la phase de découverte, ces noms n'ajoutent toutefois que peu d'information à celle fournie par la mesure de Fréchet. Le fait que deux tronçons aient des noms différents ne fournit aucune information quant à leur liaison\footnote{Même si la rue à changé de nom, des tronçons peuvent tout de même entretenir des relations de dérivation.}. Lorsque le nom est le même, cela permet seulement de supposer qu'ils sont membres de la même rue. La mesure de nom de rue est utile pour qualifier une relation, mais elle n'est pas suffisamment spécifique pour apporter une information significative par rapport à ce que peut exprimer la mesure géométrique. Il aura même tendance à dégrader localement l'efficacité du recuit simulé en augmentant les croyances en des relations concernant des tronçons d'une même rue mais éloignés. 
Pour cette raison, nous considérons que cette mesure n'est pas fiable pour la phase de découverte des relations ($\alpha = 0$). Toutefois, elle reste cruciale dans la phase d'étiquetage des relations découvertes. Dans les cas d'application présentés ici, l'appariement comporte donc seulement trois critères~: deux temporels et un géométrique.
\\
La fonction de masse résultante est alors~:
\[
\begin{cases}
m_{e_i}(\omega_e) =  0 \\
m_{e_i}(\neg \omega_e) =  0 \\
m_{e_i}(\Omega) = 1 \\
\end{cases}
\]
La mesure n'étant pas fiable, la totalité de la croyance est placée dans l'ignorance. La mesure de noms de rues est un élément neutre du calcul de la vraisemblance d'un hypergraphe de filiation.

\subsection{Fonctions de masse de croyance utilisées pour le typage des filiations}
\label{section:massetypage}
Nous définissons ici les fonctions de masse utilisées pour typer les relations de filiation issues de première étape du processus. Ces fonctions de masse peuvent s'exprimer sur le cadre de discernement $\Omega=\{\gamma,\delta,\epsilon\}$ défini pour chaque tronçon ou sur toute disjonction de $2^\Omega$. Nous avons choisi d'utiliser trois sources d'informations s'exprimant sur les deux attributs composant l'identité des tronçons~:
\begin{enumerate}
\item une source $a$ fondée sur une distance de Fréchet entre tronçon dont le but est de déterminer si la filiation est un cas d'erreur.
\item une source $b$ comparant la longueur de deux tronçons de manière à distinguer une dérivation d'une continuation. L'hypothèse est la suivante~: si deux tronçons affiliés sont de longueur très différentes, il s'agit d'une relation de dérivation.
\item une dernière source $c$ 'information doit juger de la ressemblance des noms de rue des tronçons. Il y a continuation entre deux tronçons si la rue n'a pas changé de nom.
\end{enumerate}
Les fonctions de masse associées à ces trois sources d'information sont données dans les paragraphes suivants.

\myparagraph{Détection des relations erronées~: $a$}
Le rôle de la source d'information $a$ est de détecter les relations de filiations erronées. Celles-ci sont filtrées par un critère de distance de Fréchet entre tronçons~: au delà d'un seuil, la filiation est jugée erronée. Afin que les résultats soient cohérents avec la phase de découverte des filiations, cette distance doit égale à $df_{max}$. Pour deux tronçons $o_1$ et $o_2$, la fonction de masse de croyance associée à la source $a$ est ~:
\[
\begin{cases}
m_a(\{\delta,\gamma\}) = 1 \text{ si } d_f(o_1,o_2) \leq df_{max}, 0 \text{ sinon} \\
m_a(\epsilon) = 1-m_a(\{\delta,\gamma\})
\end{cases}
\]

\myparagraph{Mesure des longueurs relatives des tronçons liés~: $b$}
Étant donné deux tronçons $o_1$ et $o_2$ de longueur respective $l_1$ et $l_2$, la fonction de masse associée à la sources d'information $b$ est~:
\[
\begin{cases}
m_b(\gamma) = \alpha e^{\frac{-(1-min(l_1,l_2) / max(l_1, l_2)}{\epsilon_b}} \\
m_b(\delta) = 1-m_a(\gamma)\\
m_b(\Omega) = 1-\alpha
\end{cases}
\]
$\epsilon_b$ est un paramètre de seuil déterminé comme pour les autres fonctions de masse par $\epsilon_b = -d_b/ln(0.5)$. Si un des tronçons est plus petit que l'autre d'un facteur $1/d_b$, la filiation sera considérée par la source d'information $b$ comme une dérivation. $\alpha$ est un facteur de fiabilité fixé arbitrairement à $0.95$. Abaisser légèrement la fiabilité de cette mesure permet de prendre en compte la croyance de la source d'information $c$ même si les tronçons $o_1$ et $o_2$ sont identiques.

\myparagraph{Source d'information sur les noms de rue~: $c$}
Le but de la source d'information $c$ est de typer une relation de filiation entre deux tronçons $o_1$ et $o_2$ en s'appuyant sur leur nom de rue. La fonction de masse associée est~:
\[
\begin{cases}
m_c(\gamma) = \alpha  \text{ si } c_r(o_1, o_2) \leq dr_{max}\\
m_c(\{\\delta,\epsilon\}) = 1-m_c(\gamma)\\
m_c(\Omega) = 1-\alpha
\end{cases}
\]



\subsection{Résultats sur l'îlot de la Trinité}
\myparagraph{Paramètres utilisés pour la phase de découverte des relations}
L'approche que nous proposons repose sur un nombre important de paramètres empiriques. Nous donnons ici ceux utilisés pour la construction du graphe de filiation sur l'îlot de la trinité.
\\
Un premier ensemble de paramètres concerne le recuit simulé. Les températures initiales ainsi que la solution initiales sont déterminées d'après la proposition de~\cite{BenAmeur2004}. Le nombre d'itérations maximale a été fixé à 50000, avec un critère d'arrêt si aucune meilleure solution n'est trouvée avant 10000 itérations. Enfin, le paramètre de vitesse de décroissance de la température a été fixé arbitrairement à 0.9999 dans le but d'atteindre une température proche de 0 à 50000 itérations sans pour autant décroître trop rapidement.
\\
Un second ensemble de paramètres concerne les critères de similarité d'identité et de succession temporelle pour la phase de découverte des filiations. Nous avons fixé empiriquement les paramètres suivants~:
\begin{itemize}
\item seuil de la mesure de Fréchet~: $df_{max} = 20$ mètres. 
\item seuil de la mesure de rang temporel~: $drank_{max} = 1$. Au delà d'un saut au dessus d'une source géohistorique, une filiation sera considérée non vraisemblable.
\end{itemize}


\myparagraph{Découverte des relations de filiation : visualisation des résultats d'appariement}
Nous présentons ici le résultat de la phase de découverte des relations de filiation pour les tronçons de rues de l'îlot de la trinité pour les quatre sources cartographiques simultanément après 20000 itérations du recuit simulé. Un première figure (\ref{figure:matchinggreneta}) présente le résultat des appariements créés par l'algorithme. Pour des questions de lisibilité, le résultat est présenté source à source. La couleur des différents réseaux correspond au code couleur fixé en début de section. Les flèches en rose correspondent à au résultat de la transformation de l'hypergraphe de filiation créé en graphe simple mais non étiqueté. Il est ici géolocalisé pour permettre de visualiser les liens entre les observations géohistoriques dans l'espace géographique. Enfin, les numéros correspondent aux identifiants (gid) des tronçons.
\begin{figure}
\includegraphics[angle = 90, width=1\textwidth]{/illus_graphs/graphe_troncons_greneta.png}
\caption{Appariement résultant de la découverte des relations de filiations entre tronçons de l'îlot de la trinité.}
\label{figure:matchinggreneta}
\end{figure}

Nous constatons tout d'abord que l'identification des relations de filiation est satisfaisante, aucun lien incohérent n'ayant été créé. Cependant, la solution contient plusieurs cas de liens N : M qui apparaissent en raison des décalages planimétriques des données. Plus exactement, ce type de liens apparaît lorsque les tronçons des deux sources ne sont pas parfaitement face à face. Dans ces cas, l'agrégation des tronçons et la formation d'hyperarcs de cardinalité N: M permet de diminuer la valeur de la mesure de Fréchet, ce qui augmente alors la vraisemblance de l'hyperarc.
\\
La méthode a donc tendance à sur-apparier les réseaux en présence de décalages planimétriques faibles\footnote{Lorsque ceux-ci sont trop important, les observations ne seront pas appariées.}. Ce sur-appariement n'est pas nécessairement problématique car la phase de typage des relations permet d'éliminer au moins partiellement ce type de configuration.
\\
Les tronçons non appariés sont également intéressants. Ainsi, les tronçons 2295 et 2494 du réseau de l'atlas de Verniquet ne sont liés à aucun autre tronçon. Le premier est situé à moins de 10 mètres du tronçon 1885 de l'atlas de Vasserot, et, d'après la mesure de Fréchet, sa liaison avec ce dernier est plus crédible qu'une disparition. Cependant, son non-appariement est dû au fait qu'il est préférable de lier le tronçon 2296 uniquement au tronçon 1885 de l'atlas  de Vasserot. Notre méthode permet donc également de prendre en compte de façon implicite le voisinage des observations. Le tronçon 2494 n'est quand à lui pas apparié en raison de sa forme particulière. En effet, la distance de Fréchet entre ce tronçon et le tronçon 1877 est élevée~: la croyance pour la distance de Fréchet de Verniquet à Vasserot est donc principalement placée sur la proposition de non-vraisemblance de la filiation. Enfin, notons que seuls deux tronçons de rues de l'atlas de Jacoubet ont été appariés, la structure viaire ayant été totalement modifiée.
\\
En réalité, le graphe présenté dans cette figure est la solution optimale du recuit simulé\footnote{Sur 5 exécutions du recuit simulé, la solution optimale est systématiquement atteinte en moyenne à l'itération 6920. Pour 20 000 itérations, le temps de calcul moyen est de 12.8 secondes.}. La tendance au sur-appariement est intrinsèquement liée à la façon dont a été modélisée la mesure de Fréchet.
\\
Pour éviter de tels sur-appariements, une possibilité est d'ajouter un coût aux hyperarcs de cardinalité n :m en affaiblissant la croyance des mesures de similarité d'identité dans la proposition $\omega$. De cette façon, de tels hyperarcs ne peuvent être créés que s'ils apportent augmentent significativement la vraisemblance de l'hypergraphe. Dans la figure~\ref{figure:nms_vs_nonms}, nous donnons en exemple le résultat obtenu pour la découverte des relations de filiations entre l'atlas de Verniquet et de Vasserot avec un coefficient d'affaiblissement de  $0.90$ pour les arcs de cardinalité n: m pour la mesure de Fréchet\footnote{On a donc $m(\omega) = 0.90c_{frechet}$}. Les deux cas de sur-appariement présents dans la première version (à gauche) ne sont plus présents dans la solution optimale avec affaiblissement des arcs n: m. Ce paramètre est toutefois empirique et peut gêner la convergence de l'algorithme en empêchant la création d'arcs complexes dégradant momentanément la solution. De plus, nous avons dit que le sur-appariement n'était pas problématique car, à l'inverse du sous-appariement, il peut être corrigé par la phase de typage. Un comportement prudent consiste ici à ne pas pénaliser les hyperarcs de cardinalité n :m afin d'améliorer la convergence du recuit simulé.
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{/illus_graphs/penalite_nm.png}
\caption{Découverte des relations de filiation avec pénalité des hyperarcs de cardinalité n : m.}
\label{figure:nms_vs_nonms}
\end{figure}


\myparagraph{Découverte des relations de filiation : confrontation avec un algorithme d'appariement de réseaux classique}
L'approche que nous avons proposé pour créer l'hypergraphe de filiation est une approche d'appariement de données géographiques. Il est alors intéressant de confronter les résultats que nous obtenons avec ceux d'une méthode d'appariement classique de réseaux. En effet, nous avons opté pour la généricité, mais une méthode spécifiquement dédiée à un type de données géographiques peut être plus efficace. Pour cela, nous avons créé une vérité-terrain pour l'îlot de la Trinité à partir des connaissances sur les transformations de ses rues issues de \citep{Gribaudi2009} et \citep{Lazare1844}. Nous avons ensuite choisi de comparer notre approche avec la méthode d'appariement de données géographiques proposée par \cite{MustiereDevogele2008}. Celle-ci est en effet spécifiquement dédiée à l'appariement de réseaux de rues dont les niveaux de détail diffèrent. Elle a de plus prouvé son efficacité (voir \citep{Olteanu2008}) et est disponible dans la plateforme Geoxygene. Cette méthode d'appariement ne permet toutefois que des appariements entre deux sources de données. Nous l'avons donc exécutée pour chacun des couple de \emph{snapshots} (Verniquet, Vasserot), (Vasserot, Jacoubet) et (Jacoubet, Municipal). Nous méthode est quand à elle appliquée à tous les \emph{snapshots} simultanément. Pour comparer les deux méthodes, nous nous appuyons sur les mesures classiques de précision, rappel et f-mesure, définies par~:
\begin{equation*}
 \text{precision} =  \frac{\text{nombre de liens d'appariement corrects trouvés}}{\text{nombre de liens d'appariement trouvés}}
 \end{equation*}
 \begin{equation*}
 \text{rappel} =  \frac{\text{nombre de liens d'appariement corrects trouvés}}{\text{nombre de liens existants dans la vérité-terrain}}
 \end{equation*}
 \begin{equation*}
  \text{f-mesure} =  2\times \frac{\text{precision}\times \text{rappel}}{\text{precision}+ \text{rappel}} 
\end{equation*}
La précision est une mesure du bruit présent dans le résultat. Le rappel indique quant à lui la pertinence du résultat~: il est d'autant plus élevé que les liens de la vérité terrain ont été identifiés. Enfin, la f-mesure est une synthèse donnant une qualité globale de la solution.
\\
Le tableau~\ref{table:vs_mustiere} présente le résultat de cette évaluation. Nous avons également donné le résultat des mesures pour la solution intégrant la pénalité sur les hyperarcs de cardinalité n : m.
L'approche de \cite{MustiereDevogele2008} est noté ''M\&D''.
\begin{table}
\caption{Qualité de l'appariement sur l'îlot de la trinité}
\label{table:vs_mustiere}
\begin{tabular}{|c|c|c|c|c|}
\hline
Méthode & Snapshots & Rappel & Précision & Fmesure \\ \hline
\hline
M\&D & Verniquet $\to$ Vasserot &  0.93 & 1 & 0.963 \\ \hline
- & Vasserot $\to$ Jacoubet &  1 & 1 & 1 \\ \hline
- & Jacoubet $\to$ Poubelle &  1 & 0.67 & 0.8 \\ \hline
\hline
Notre méthode ($\alpha =1$) & Verniquet $\to$ Vasserot &  1 & 0.77 & 0.875 \\ \hline
- & Vasserot $\to$ Jacoubet &  1 & 0.63 & 0.77 \\ \hline
- & Jacoubet $\to$ Poubelle &  1 & 1 & 1 \\ \hline
\hline
($\alpha =0.9$) & Verniquet $\to$ Vasserot &  1 & 1 & 1 \\ \hline
- & Vasserot $\to$ Jacoubet &  1 & 1 & 1 \\ \hline
- & Jacoubet $\to$ Poubelle &  1 & 1 & 1 \\ \hline
\end{tabular}
\end{table}

Nous retrouvons dans les résultats la tendance de notre méthode au sur-appariement. En effet, la précision est relativement faible en raison des hyperarcs de cardinalité n : m créés. Cet effet est entièrement corrigé lorsque l'on pénalise ce type d'hyperarcs. Le rappel est quant à lui systématiquement supérieur ou égal à celui obtenu avec la l'approche d'apparition de réseaux. Le nombre de liens d'appariement existants dans la vérité terrain est toutefois particulièrement faible (15 liens) tout comme le nombre de tronçons. L'utilisation d'une méthode automatique pour des cas de cette taille n'est pas pertinent. Il nous faut donc effectuer ces mesures sur un espace de plus grande taille.

\myparagraph{Graphe de filiations non étiqueté et typage}
Nous avons présenté jusqu'ici le résultat de la phase de découverte des relations de filiation selon l'angle de l'appariement de données géographiques. Le véritable objectif de cette phase est de produire un graphe $G$ en décomposant le meilleur hypergraphe de filiation construit par le recuit simulé. Ce graphe, dont les arcs ne sont pas encore étiquetés, est présenté dans la figure~\ref{figure:graphe_noetiq}. Notons que le graphe que nous présentons ici est celui obtenu en pénalisant les hyperarcs de cardinalité n: m afin d'éliminer les cas de sur-appariement.
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{/illus_graphs/graphe_untagged_greneta.png}
\caption{Graphe $G$ non étiqueté crée par la phase de découverte des relations de filiation pour l'îlot de la Trinité.}
\label{figure:graphe_noetiq}
\end{figure}
Chaque tronçon est représenté par un cadre indiquant son identifiant et le nom de la rue à laquelle il appartient. Ce graphe étant une autre représentation du résultat illustré par les figures précédentes, nous retrouvons exactement les mêmes arcs. Nous pouvons déjà voir apparaître quelques phénomènes~: la brisure haussmannienne qui détruit le quartier (lisible entre Jacoubet et l'atlas Municipal) ainsi que le changement presque intégral des noms des rues qui traduit le changement de statut de l'îlot après la Révolution.
\\
Nous appliquons ensuite le processus de typage des relations de filiation en utilisant les paramètres et mesures définies pour cet îlot. Ainsi, nous considérons qu'il y a dérivation entre deux tronçons si le nom de la rue change ou si la longueur des tronçons liés varie fortement, témoignant d'un découpage de la rue (en général par l'apparition d'une intersection). Enfin, les arcs sont considérés erronés s'ils sont situés à plus de 20 mètres les uns des autres. Le graphe de filiation $G_f$ résultant de l'étape de typage est visible dans la figure~\ref{figure:typagegreneta1}. Les relations de dérivation sont marquées par un trait pointillé et les relations de continuation par un trait en plein. La probabilité pignistique du type choisi est indiquée par un dégradé de couleur et est de plus indiquée au dessus de chaque arc. Cette probabilité pignistique traduit certitude que nous avons sur le type de chaque lien. 
 \begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{/illus_graphs/gf_greneta_1.png}
\caption{Graphe de filiation $G_f$ après typage des relations de filiations découvertes pour l'îlot de la trinité.}
\label{figure:typagegreneta1}
\end{figure}
Le graphe formé fait apparaître les mutations des tronçons des rues de l'îlot. Ainsi, la totalité des tronçons de l'atlas de Verniquet sont en relation de dérivation avec ceux de l'atlas de Vasserot puisque le nom des rues changent. La probabilité pignistique de ces dérivations est toutefois peu élevée car les tronçons sont géométriquement très similaires. Entre les atlas de Vasserot et de Jacoubet, l'état de l'îlot ne varie plus. Les seules relations de dérivation identifiées sont dues à des données manquantes. Enfin, tous les tronçons disparaissent à l'exception du tronçon 2136 et 2147. Le premier existe toujours dans l'atlas municipal, mais une dérivation a été identifiée en raison du nom partiellement manquant dans l'atlas de Jacoubet. Concernant le tronçon 2147, la dérivation est due au fait que la rue de Palestro occupe l'espace de la Cour du Commerce. 
\\
Pour illustrer les différents graphes de filiation qui peuvent être créés à partir d'un même hypergraphe de filiation, nous avons ré-appliqué le processus de typage en considérant cette fois seulement la morphologie du réseau. Pour que le nom des rues n'intervienne plus dans le choix du type, nous avons modifié la mesure associée afin qu'elle place toute sa croyance systématiquement dans l'ignorance totale. Le résultat est visible dans la figure~\ref{figure:typagegreneta2}.
 \begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{/illus_graphs/gf_greneta_2.png}
\caption{Graphe de filiation $G_f$ après typage des relations de filiations découvertes pour l'îlot de la trinité, sans considérer le nom des rues.}
\label{figure:typagegreneta2}
\end{figure}
Cette fois, la majorité des relations sont des continuations, la géométrie des tronçons variant peu jusqu'à leur destruction (sauf décalages planimétriques). Seuls les tronçons de la rue Saint-Guillaume sont en relation de dérivation car la disparition du tronçon 2494 (visible en figure~\ref{figure:matchinggreneta}) provoque leur fusion. Notons que le nom de la rue n'intervenant plus dans le typage, la relation entre la Cour du Commerce et la rue de Palestro est cette fois identifiée comme une continuation. 


\subsection{Résultats sur le quartier des Arts et Métiers}
Bien que les résultats sur l'îlot de Greneta sont satisfaisant, il s'agit d'un petit espace composé de peu de tronçons. Nous appliquons ici notre approche à un espace de plus grande taille  (251 tronçons répartis dans 4 \emph{snapshots}) dans le but d'évaluer sa qualité dans des configurations plus complexes. Les résultats présentés ici ont été obtenus après $100000$ itérations du recuit simulé, avec une température initiale $T0=1.4$  et un facteur de décroissance de température $\mu=0.9999$. Le reste des paramètres est identique à l'îlot de la trinité.

\myparagraph{Visualisation des résultats d'appariement}
 Le résultat de la phase de découverte des filiations est présenté sous la forme d'un résultat d'appariement en figure~\ref{figure:stmartinfull}. Les relations de filiations sont indiquées par des flèches en rose. Afin de discuter les résultats, nous allons effectuer quelques zooms sur certaines zones pour lesquelles l'algorithme de découverte des relations ne permet pas de trouver de bonne solution.
\begin{figure}
\includegraphics[angle = 90, width=1\textwidth]{/illus_graphs/graphe_troncons_stmartin.png}
\caption{Appariement résultant de la découverte des relations de filiations entre tronçons pour le quartier des Arts et Métiers.}
\label{figure:stmartinfull}
\end{figure}
La figure~\ref{figure:zoomfailstmartin} pointe trois situations entre les atlas de Jacoubet et Municipal dans lesquelles l'algorithme n'a pas permis de détecter les relations de filiation. Le détail de ces trois zones et la vérité terrain correspondante est visible en figure~\ref{figure:detailfails}. La décomposition des hyperarcs de la vérité terrain sont tracés en noir, ceux de la solution trouvée en rose. Les cas $a$ et $c$ correspondent à des processus spatio-temporels de réallocation. Dans le premier cas, la rue (il s'agit de la rue Saint-Martin) reste inchangée entre les deux atlas, mais plusieurs nouvelles rues apparaissant avec les percées haussmanniennes viennent créer de nouvelles intersections et ainsi diviser le tronçon 1202 de l'atlas de Jacoubet en 3 tronçons. Dans le même temps, le tronçon 1191 de Jacoubet disparaît.  Dans le cas $c$, la situation est du même type~: les tronçons 3665, 3667, 148, 137 et 138 de Jacoubet, formant initialement deux rues (rue de Frépillon et rue de la Croix) sont alignées et rassemblées en une rue unique (rue Volta) composée des tronçons 10199, 9858 et 9860.  Enfin, le cas $b$ correspond au replacement d'une ancienne rue (rue Royale) par un tronçon de l'actuelle rue Réaumur. 
 \begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{/illus_graphs/zoom_fail_stmartin.png}
\caption{Cas d'échecs de l'algorithme de découverte des relations de filiation.}
\label{figure:zoomfailstmartin}
\end{figure}
L'échec de l'algorithme de découverte des filiations est ici dû à un problème de convergence. Dans les trois cas, les tronçons à apparier sont de taille très différentes, avec de petits tronçons devant être affiliés à des tronçons de grande taille (par exemple les tronçons 143 et 9871 du cas $b$). Les hyperagraphes étant construits pas à pas pendant le parcours de l'espace de recherche en commençant par la création d'un hyperarc de cardinalité 1: 1, les hyperarcs des cas $a$, $b$ et $c$ ne peuvent être atteints que par la complexification d'un hyperarc créé initialement entre deux tronçons. Puisque ceux-ci sont très différents, la mesure de Fréchet va considérer la filiation de ces deux tronçons comme non vraisemblable et il sera alors momentanément préférable de ne pas les affilier. Seule l'extension de l'hyperarc initial vers un arc de cardinalité n: m permettra de trouver la bonne solution. L'absence de filiation est donc ici un cas de minimum local qui devrait être surmonté par l'algorithme. Or, celui-ci ne parvient pas à sortir de ce minimum local pour deux raisons~:
\begin{itemize}
\item plus les tronçons initialement liés sont éloignés et de taille différente, plus la dégradation de la fonction initiale sera importante. Plus encore, une dégradation -de moins en moins importante toutefois- aura lieu pour toutes les étapes intermédiaires menant à l'hyperarc n: m. Cet hyperarc ne peut donc être trouvé que lorsque la température du recuit est élevée,
\item lorsque la température est élevée, l'algorithme effectue une marche aléatoire dans l'espace de recherche. Il devient donc improbable de créer l'hyperarc souhaité~\footnote{La probabilité est d'autant plus faible que la taille du problème augmente.}. Même si cet hyperarc est trouvé, le parcours de l'espace de recherche à température élevée peut aboutir à sa destruction.
\end{itemize}
Ces deux effets se cumulent donc et gênent fortement la convergence de l'algorithme sur les cas de processus spatio-temporels entre tronçons de géométries très différentes.
\begin{figure}
        \centering
        \begin{subfigure}[b]{0.44\textwidth}
                \includegraphics[width=\textwidth]{/illus_graphs/fail_a.png}
				\caption{Cas d'échec $a$}
                \label{figure:faila}
        \end{subfigure}%v
        \\
        \begin{subfigure}[b]{0.44\textwidth}
                \includegraphics[width=\textwidth]{/illus_graphs/fail_b.png}
				\caption{Cas d'échec $b$}
                \label{figure:failb}
        \end{subfigure}
        \\
        \begin{subfigure}[b]{0.44\textwidth}
                \includegraphics[width=\textwidth]{/illus_graphs/fail_c.png}
				\caption{Cas d'échec $c$}
                \label{figure:failc}
        \end{subfigure}
	    \caption{Détail des situations d'échec de la phase de découverte des relations de filiation pour le quartier des Arts et Métiers.}
        \label{figure:detailfails}
\end{figure}

Un second type de cas d'échec est visible en figure~\ref{figure:fail2}, lié cette fois aux décalages planimétriques des réseaux viaires. Nous nous situons cette fois dans la partie centrale de l'ancien enclos de Saint-Martin des Champs. Le réseau issu de l'atlas de Jacoubet, figuré en orange, est décalé en moyenne de 10 mètres sur l'ensemble de cet espace, créant un chevauchement avec le réseau issu de l'atlas de Vasserot (en vert). Le décalage est tel que les tronçons les plus proches dans les deux atlas ne sont pas ceux affiliés dans la vérité terrain. La localisation des tronçons de l'atlas de Jacoubet corrigée manuellement, visible dans l'encart de la même figure, permet de se rendre compte de l'effet de chevauchement des deux réseaux. 
Dans notre approche de découverte des filiations, nous n'avons pas pris en compte les décalages planimétriques entre observations issues de différents \emph{snapshots}. La filiation étant fondée sur des distances, les tronçons les plus proches sont choisis, ce qui peut être faux lorsqu'il y a des décalages importants. De plus, comme nous ne prenons pas en compte le nom de la rue à laquelle appartient le tronçon, seule la géométrie apporte une information sur l'existence d'une relation de filiation entre deux tronçons. Les erreurs visibles sur la figure \ref{figure:fail2} sont dues à cet effet. Par exemple, le tronçon 3220 encadré en vert devrait être affilié au tronçon 155 (encadré en orange), et non au tronçon 159 pourtant plus proche en raison du décalage du réseau. Afin de corriger ce type d'erreur, une possibilité serait d'intégrer au sein même de la découverte des filiations une étape de recalage planimétrique.
% Nous discutons plus longuement de cette possibilité dans le paragraphe~\ref{paragraph:recalage}.
 \begin{figure}
\centering
\includegraphics[width=1\textwidth]{/illus_graphs/fail_center.png}
\caption{Faux positifs lors de la découverte des filiations, dus au décalage planimétrique des données. À gauche, les décalages planimétriques de l'atlas de Jacoubet sont réduits manuellement par rapport à celui de Vasserot.}
\label{figure:fail2}
\end{figure}
\newpage
\myparagraph{Comparaison avec la méthode d'appariement de \cite{MustiereDevogele2008}}
De la même façon que pour l'îlot de la trinité, nous avons comparé les résultats obtenus pour le quartiers des Arts et Métiers avec la méthode d'appariement de réseaux de \cite{MustiereDevogele2008}. Les mesures sont présentées dans le tableau~\ref{table:compstmartin}.
Tout comme pour l'îlot de la trinité, nous obtenons une valeur de rappel systématiquement meilleur que l'approche spécialisée. Le rappel décroit cependant fortement entre les atlas de Jacoubet et Municipal en raison des problèmes présentés dans les paragraphes précédents. Il en résulte un sous-appariement qui empêchera la création d'un graphe de filiation satisfaisant et demande une correction manuelle.  L'approche de Mustière et Devogèle est peu efficace pour détecter les relations de filiation entre ces deux derniers atlas car elle n'est pas adaptée à des réseaux dont la structure change. En effet, l'objectif de cette approche étant d'apparier des réseaux à différents niveaux de détail mais représentant les mêmes entités du monde réel, elle suppose que les topologies des réseaux appariés sont proches et utilise cette topologie pour effectuer l'appariement. Or, nous sommes dans un cas différent où les réseaux ne représentent pas nécessairement les mêmes entités~: la topologie des réseaux n'est donc pas toujours d'un grand secours et peut même nuire à l'appariement. Concernant la précision, notre approche a toujours tendance à sur-apparier les réseaux en créant des hypergraphes complexes. 
\begin{table}
\caption{Qualité de l'appariement sur le quartier Saint Martin}
\label{table:compstmartin}
\begin{tabular}{|c|c|c|c|c|}
\hline
Méthode & Snapshots & Rappel & Précision & Fmesure \\ \hline
M\&D & Verniquet $\to$ Vasserot &  0.836 & 0.98 & 0.90 \\ \hline
- & Vasserot $\to$ Jacoubet &  0.75 & 0.86 & 0.8 \\ \hline
- & Jacoubet $\to$ Municipal &  0.30 & 0.90 & 0.45 \\ \hline
\hline
Notre méthode ($\alpha = 1$)& Verniquet $\to$ Vasserot &  0.95 & 0.90 & 0.93 \\ \hline
- & Vasserot $\to$ Jacoubet &  0,76 & 0.71 & 0.736 \\ \hline
- & Jacoubet $\to$ Municipal &  0.52 & 0.74 & 0.61 \\ \hline
\hline
\end{tabular}
\end{table}
%
%\myparagraph{Création du graphe de filiation par typage des relations découvertes}
%Nous avons appliqué la procédure de typage des relations de filiation découvertes en utilisant les mêmes paramètre que pour l'îlot de la trinité. Étant donné le nombre de tronçons liés, la visualisation du graphe dans son entièreté n'est pas pertinente. Nous proposons ici d'effectuer un zoom sur quelques rues des quatre réseaux liés de façon satisfaisante afin d'illustrer le graphe de filiation construit. 

%
%\subsection{Discussion sur les résultats obtenus}
%\paragraph{Approche pessimiste versus optimiste}
%\paragraph{Intégrer le recalage}
%\paragraph{Améliorer la convergence de la méthode}
%Problème induit par les NMS ~: pourquoi ça se passe, comment serait-il possible de le régler? Noter que çe problème là arrive égakelement dans les cas de 1-N et N-1 lorsque les tronçons de la première base sont très petits et ceux de la seconde très grands (ou vice versa). Pour régler le problème,une possibilité est d'opter pour un mode bcp plus optimiste dans lequel Frechet va placer dans l'ignorance plutôt que dans l'hypothèse "non app". En faisant ça, la probalitié pignistique d'un hyperarc est toujours supérieur à 0.5. Cela signifie que la mort et naissance est le pire des cas , ou, autrement dit, une observation disparait (resp apparait) lorsqu'aucune autre observation n'est disponible pour être son étant suivant (resp. précédent).
%\paragraph{Paramétrisation de la phase de typage (apprentissage?)}
%\paragraph{Visualisation des graphes créés}

\clearpage
\section{Conclusion}
\label{section:cclchap6}
Dans ce dernier chapitre nous avons présenté une approche générique permettant de construire des graphes de filiations à partir d'observations géohistoriques structurées au sein d'une base de données spatio-temporelle.
Cette approche s'appuie sur un processus en deux temps.
Tout d'abord, les observations géohistoriques sont appariées afin de détecter les observations dont les identités sont liées.
Nous proposons pour cela une nouvelle méthode d'appariement de données géographiques par optimisation discrète entre un nombre quelconque de sources de données.
Cette méthode s'appuie sur la théorie des croyances introduite par \cite{Dempster1968} et utilisée dans le cadre de l'appariement de données géographiques par \cite{Olteanu2008}.
Dans un second temps, un graphe de filiation est créé à partir du résultat de l'appariement à l'aide de connaissances imparfaites sur la nature des relations de filiation entre observations.
Ceci nous permet de générer des graphes de filiation automatiquement à partir d'une définition de l'identité d'une observation fournie par l'utilisateur.
\\
Nous avons développé un prototype expérimental nous permettant de tester l'approche sur les réseaux de rues de Paris vectorisés et présentés dans le chapitre 3 puis stockés dans une base de données spatiale et temporelle dont le schéma a été explicité au cours du chapitre 4.
Les premières expérimentations de notre approche démontre sa capacité à identifier des relations de filiation entre observations géohistoriques vectorielles issues d'un nombre quelconque de sources. Les résultats sont ainsi encourageants sur les données de réseau de rues utilisés. La méthode recherchant des agencements d'objets géographiques décrivant des transformations (ou des persistences), elle se montre plus adaptée qu'une approche d'appariement spécialisée dédiée à l'identification d'objets homologues choisie dans la littérature, tout en traitant simultanément plusieurs sources de données. Notre approche est également fortement générique, lui permettant d'être appliquée à des observations de nature différentes. L'étape d'étiquetage permet quant à elle une maîtrise relativement fine de la notion d'identité d'une observation géohistorique. Elle offre en particulier la possibilité de créer plusieurs graphes géohistoriques à partir d'un même résultat d'appariement correspondants à une conception différente de l'identité des observations en jeu. 
\\
Les expérimentations mettent également en évidence certaines limites. Tout d'abord, le processus proposé est relativement complexe et s'appuie sur plusieurs de paramètres empiriques, ce qui questionne réutilisabilité, ou, tout du moins, à sa robustesse. De plus, nous avons pointé des failles dans la convergence de la méthode qui gênent le passage à l'échelle. Ces limites ouvrent plusieurs pistes d'amélioration de l'approche de construction de graphes géohistoriques.
\\
La première concerne le passage à l'échelle de l'approche. Les cas d'application sont ciblés sur des zones restreintes de Paris, en raison de la complexité de la représentation visuelle des résultats mais également de difficultés de convergences du recuit simulé\footnote{Ces difficultés ont été abordées au cours des expérimentations.}. Le parcours du voisinage d'une solution au cours du processus d'optimisation est un point critique conditionnant à la fois la rapidité de l'optimisation et la qualité du résultat. À une itération donnée, le nombre de modifications applicables au graphe en cours de construction est élevé mais seules quelques modifications sont réellement pertinentes~: seule une petite portion de l'espace de recherche est réellement intéressante. Actuellement, aucune notion de pertinence n'est inclue dans l'étape de génération et de choix d'une nouvelle solution, la nouvelle solution étant tirée selon une loi uniforme. Le risque est alors de passer un temps important dans des zones non pertinentes de l'espace de recherche. Pour pallier ce problème, plusieurs pistes sont envisageables. Une façon de guider le parcours de cet espace serait de remplacer la loi uniforme par une loi ad-hoc privilégiant les solutions décrivant des processus spatio-temporels dont la structure est crédible. Cette crédibilité dépend bien sur des données et peut varier selon le lieu et l'époque des entités du mode réel considérées. La division d'une parcelle en quelques parcelles plus petits est un processus crédible, tandis que l'existence d'un large processus N: M mêlant plusieurs dizaines de parcelles est relativement peu probable. Pourtant, ces deux solutions ont actuellement la même chance d'être atteintes. Une loi de probabilité adaptée à un espace, un temps et un type d'entités du monde réel doit être construite à partir de connaissances d'experts, ce qui pourrait être obtenu par une étape d'apprentissage supervisé permettant d'entraîner la méthode pour un usage donné.
\\
Le recuit simulé étant une méthode d'optimisation locale et le problème d'appariement étant en réalité composé d'une multitude de sous-problèmes d'identification de processus spatio-temporels, l'application de l'approche à des espaces de plus grande taille (Paris, île de France, France entière,etc.) pose clairement des questions sur le temps de calcul nécessaire à l'obtention d'une solution de bonne qualité. Il est utile de s'interroger sur la possibilité de parallélisation de l'approche pour la construction de graphes de grande taille, lorsqu'il s'agit d'apparier un nombre important d'observations. Ce problème se pose tant dans la largeur (traiter des portions d'espace géographique de grande taille) que dans la profondeur (lorsque de nombreuses sources sont disponibles sur une portion d'espace donnée). Indirectement, la question de la parallélisation de l'approche pose la question du découpage du graphe à construire en sous-problèmes concernant une étendue spatiale et temporelle et la gestion des frontières entre ces sous-problèmes. Il s'agit ici d'une question ouverte qui nécessitera un travail ultérieur.
\\
Une seconde perspective concerne l'amélioration de la qualité des solutions proposées. Nous avons vu que, bien que l'approche soit robuste aux imperfections de faible ampleur (décalages planimétriques notamment), la qualité des résultats baisse fortement lorsque celles-ci sont importantes. Concernant le cas précis des décalages planimétriques, il serait particulièrement intéressant d'inclure pendant l'optimisation des phases de recalage s'appuyant sur les relations de filiation déjà découvertes. Ainsi, il s'agirait de procéder à la fois à la création d'un graphe géohistorique et au recalage géométrique des données. Ce type de procédé a d'ailleurs déjà été appliqué pour l'appariement de deux réseaux par \cite{LiGoodchild2011}. Ceci nécessite toutefois de déterminer le moment adéquat de l'application du recalage pendant l'optimisation. On peut par exemple imaginer d'attendre que la qualité des solutions ne stagne ou bien de l'appliquer à itérations fixes. Il faut également déterminer s'il est alors préférable de poursuivre la descente de température ou bien de commencer une nouvelle phase de descente rapide à haute température\footnote{en utilisant par exemple une stratégie de \emph{reannealing}. Pour cela, voir notamment~\citep{Ingber1989}}. Le choix d'une stratégie doit être guidé par des expérimentations. 
\\
L'extension de l'approche à des observations de nature diverse est également une perspective intéressante. En effet, nous avons pour l'instant considéré seulement le cas où les schémas de toutes les observations traitées sont alignés et que celles-ci sont de même nature. Or, des processus spatio-temporels peuvent tout à fait concerner des observations de nature différentes. Il est par exemple courant, durant la première moitié du XIX\up{e}, que des propriétaires parisiens fassent ouvrir de nouvelles rues au sein d'îlots bâtis, remplaçant une partie de l'espace bâti par le tracé d'une rue. Se trouvent donc mêlés bâtiments, parcelles et voies dans un processus global de percement de rues. Or, celui-ci n'est représentable au sein d'un graphe géohistorique qu'à condition que celui-ci comporte des observations de différentes natures. Ceci implique d'étendre l'approche pour considérer des processus spatio-temporels plus complexes porteurs d'une sémantique particulière (le percement d'une rue par exemple). Cela oblige également à étendre la définition des mesures pour qu'elles soient capables de comparer les identités d'observations de natures différentes.
\\
Une dernière perspective porte sur la détermination de l'identité des observations et, par là, des paramètres et fonctions de masses de croyances nécessaires à l'étape d'étiquetage. En effet, cette définition est difficile car elle nécessite une connaissance précise de l'identité des observations, notion pourtant régulièrement floue qu'il n'est pas possible de décomposer en attributs comme nous le faisons ici. Pour aider à cette définition, une première possibilité consiste à procéder par apprentissage supervisé des paramètres des fonctions de masse de croyance à partir d'exemples de relations de continuation et de dérivation soumis à des experts historiens. Même appris, il est tout à fait possible que ces paramètres ne soient valides que pour une portion d'espace donnée ou seulement pour un usage précis.


%La création d'un nouvel hyperarc de filiation est effectué selon l'algorithme 
%
%\myparagraph{Recuit simulé mono-objectif}
%
%\section{Dissimilarité d'un hypergraphe de filiations existant}
%Cette section sert a montrer comment évaluer la vraisemblance d'un graphe géohistorique.
%Chaque sous section est une étape du processus.
%C'est ss doute le bon moment pour parler de Strokes!
%\subsection{Processus d'évaluation}
%\myparagraph{Agregation des tetes et queues : process général}
%\myparagraph{Agrégation spatiale}
%\myparagraph{Agrégation temporelle}
%\myparagraph{Agrégation attributaire}
%\myparagraph{Dissimilarité d'un hyperarc}
%\begin{equation}
%\forall h \in E_{H_p}, V(h) = \frac{\alpha g(h) + [\beta_1 a_1(h) +\beta_1 a_2(h)+...+\beta_n a_n(h) ] + \gamma order(h)+\theta ant(h)}{\alpha + \beta_1 +...+ \beta_n + \gamma + \theta }
%\end{equation}
%
%\subsection{Vérification des contraintes}
%Il s'agit juste de donner comment on vérifie l'ensemble des contraintes qui s'appliquent sur un graphe.
%Est-ce vraiment utile? Non pas tellement en fait.
%
%\subsection{Evaluation de la vraisemblance d'un procesus}
%Ici on explique comment est calculé la  vraisemblance d'un hyperarc.
%\begin{enumerate}
% \item Similarité
% \item Ordre temporel
% \item Antécédance temporelle
%\end{enumerate}
%
%
%\subsection{Evaluation de la vraisemblance du graphe géohistorique}
%Ici on présente l'ajout des morts et naissances, ainsi que la répartition sur les noeuds (car ça permet de comparer différents graphes sur le même ensemble d'observations).
%\begin{enumerate}
% \item Prise en compte des naissance et mort : recherche du meilleur graphe non 11
% \item Distribution de l'énergie aux noeuds.
%\end{enumerate}
%
%
%\section{Transition d'état d'un hypergraphe de filiations}
%La méthode du recuit simulé consruit une 
%
%Ici on va présenter comment, à partir d'un hypergraphe de filiations quelconque on peut créér un autre hypergraphe.
%\subsection{Transitions}
%La progression dans l'espace de recherche découle d'un mouvement extrèmement simple~: dans l'idée, il s'agit simplement de créér une nouvelle filiation et de voir si ça fait progresser le graphe. Sur l'hypergraphe, ceci se traduit par ajouter un noeud à un hyperarc, ou en enlever. Cette opération peut avoir les conséquences suivantes~:
%\begin{itemize}
%\item Etendre ou réduire un hyperarc de fusion/scission/réallocation.
%\item Fusionner deux hyperarcs
%\item séparer un hyperarc en deux.
%\end{itemize}
%Enfin on ajoute le fait de pouvoir créér un supprimer un hyperarc dès lors qu'il est une filiation.
%On a donc une progression souple entre les processus.
%L'avantage c'est entre autre qu'on a pas d'opératiion "nulle" d'éffet comme on pourrait avoir sur les NM avec des arcs simples.
%\\
%Il faut détailler la façon dont on crée les split/merge d'hyperarcs~: en effet, ces deux opérations sont un peu plus complexes. Le merge est simple mais le split pose clairement le problème de la coupe. On détermine une coupe aléatoire de chaque coté de l'hyperarc.
%
%
%
%\section{Parcourir l'espace de recherche d'un probleme de construction}
%\subsection{Etat initial}
%Comment on construit un graphe au début.
%\subsection{Heuristiques de réduction de l'espace de recherche}
%Cylindre spatio-temporel.
%\subsection{Paramètres du recuit simulé}
%Décroissance de température, température initiale, stratégie de sampling.
%
%\subsection{Acceptation d'une solution}
%\begin{itemize}
%\item A Energie égale, on préfère une solution avec des arcs.
%\item A Energie égale, on préfère une solution avec les structures les moins complexes.
%\end{itemize}
%\subsection{Conditions d'arrêt}
%
%
%\subsection{Post-traitements}
%Là il faut expliquer qu'il faut recréer les arcs 1-1 qui n'ont pas encore été créés.
%
%
%
%\begin{enumerate}
%\item \'Etape 1 : transformation / initialisation : Déjà traité ?
%\item \'Etape 2 : appariement : Construction de l'hypergraphe des filiations.
%\item \'Etape 3 : décision : Classification des 
%\end{enumerate}
%Donner le schéma global du chapitre.
%
%La prémière étape est un traitement des données en entrées...
%
%\section{Un problème d'appariement de données géographiques}
%\subsection{Formulation comme appariement spatio-temporel}
%\subsubsection{L'appariement de données géographiques}
%\subsubsection{Un appariement fondé sur la vraisemblance}
%\subsection{Méthodes d'appariement géographiques existantes}
%
%L'appariement de données est la première étape du processus de fusion d'informations géographiques. Il s'agit de reconnaitre des objets homologues dans différentes bases de données, c'est à dire la représentation du même objet. Ce problème n'est pas trivial à cause de l'hétérogénéité des bases~: les spécifications de saisie, le niveau de généralisation et la précision géométrique des données rendent difficile l'identification de deux objets équivalents.
%Différentes méthodes ont été proposées.
%
%\subsubsection{Mustière et Devogèle~: appariement de réseaux à des niveaux de détails différents}
%Définition de ''Niveau de détails différents'' : 
%\begin{itemize}
% \item Certains éléments ne sont représentés que dans un seul réseau
%\item Un élément du réseau le moins détaillé peut correspondre à un élément du réseau le plus détaillé
%\item La différence de détail induit des décalages géométriques importants, donc la distance entre objet n'est pas suffisante. Il faut s'appuyer sur la topologie.
%\end{itemize}
%Constatation : l'organisation topologique est au moins aussi importante que la comparaison des positions absolues des objets. \\
%Particularité de la méthode : allier un appariement des noeuds à un appariement des arcs du réseau. \\
%Déroulement de la méthode : 
%\begin{itemize}
% \item Prétraitement des données : Création d'un réseau topologique pour chaque base à apparier 
%\item Préappariement des noeuds : recherche pour chaque noeud tous noeuds candidats. Ceci est fait avec un critère de distance (euclidienne). La distance peut être adaptative et varier en fonction de conaissances sur les objets (ex : pour un rond point, tous ses noeuds sont candidats même s'ils sont ''trop loin'') (+1 parama + x en fonction des conaissances supplémentaires ) Cette étape n'est pas forcément obligatoire.
%\item Préappariement des arcs : Utilisation de la demi distance de Haussdorff  (+2 params)
%\item Appariement des noeuds : le but est de trouver l'homologue / les homologues de chaque noeud.  Etape cruciale. On classe chaque candidat selon si les arcs connectés à lui sont préappariés entre eux, et à quel point ils le sont totalement (comlplet/incomplet/impossible). Il y a 2 critères supplémentaires : l'orientation des arcs et l'ordre atour du noeud (pour distinguer les isomorphismes).
%Finalement,on donne une note a chaque matching ou non-matching : certain, incertain, 
%\item Appariement des arcs : Pour chaque arc  A, on cherche le ou les arcs de B qui sont tous des candidats à l'appariement avec A,  relie des neouds appariés aux extremités de A et qui sont proches de A.1
%\end{itemize}
%
%
%Problèmes : 
%\begin{itemize}
%\item seulement 2 bases de données
%\item nécessite de connaitre la base la moins détaillée et la plus détaillée (pour le préappariement des arcs en particulier)
%\item très sensible aux seuils.
%\item pas de cas N-M
%\end{itemize}
%
%Avantages :  
%\begin{itemize}
%\item Peut prendre en compte la sémantique des données
%\item très adapté à la fusion puisque les noeuds sont appariés.
%\item prend en compte l'agencement topologique.
%\end{itemize}
%
%\subsubsection{Haunert 2005~: appariement de réseaux à des niveaux de détails différents}
%La méthode consiste, grosso modo, à projeter les points de chaque linestring sur l'autre, puis calculer l'écart entre les deux.
%Une méthode de rubber sheetign est ensuite appliquée pour distordre le réseau à apparier.
%Ça ressemble pas mal a la méthode mustière
%
%Avantages: 
%c'est simple et ça recale les réseaux
%
%Désavantages : 
%Très sensible à la distance. En fait, ça ne peut bien fonctionner qu'avec des réseaux peu déformés. C'est prévu pour des réseaux a des niveaux de détails différents, mais il ne faut pas que la différence soit trop forte.
%
%
%\subsubsection{Thierry Badard et Cécile Lemarié~: appariement de réseaux à des dates différentes (détection de MAJ)}
%C'est fait pour fusionner les différences entre deux versions d'une base de données.
%A finir de lire
%C'est tout pourri.
%
%\subsubsection{Gombosi et al~: appariement de réseaux à des dates différentes (détection de MAJ)}
%%http://www.tandfonline.com/doi/pdf/10.1080/1365881031000072627
%Ca détecte les découpages de polygones. C'est interessant car ça permet de récupérer les
%frontières de polygones qui sont la cause du changement. Par contre, ça ne marche que si les polygones sont parfaitement calés entre eux.
%
%\subsubsection*{ Fritsch 1999~: appariement de réseaux à des niveaux de détails proches}
%%http://www.tandfonline.com/doi/pdf/10.1080/136588199241157
%Utilise la théorie de l'information : l'objectif est de trouver le meilleur ``channel'' entre l'emitter (BDA) et le receiver (BD2).
%Ca identifie des 11,1n,n1,nm et aussi des matchings m+n (genre, plusieurs voies qui correspondent à une route)
%Ca prend aussi en compte les voisins : ''2 objets A et B matchent d'autant plus que leurs voisins matchent aussi entre eux''
%C'est pas mal du tout comme approche, le vrai problème c'est la lourdeur car tous les matchings potentiels doivent être précalculés (sauf les ''très improbables'' dont la quantité dépend largement de l'hétérogénéité des 2 bases)
%
%\subsubsection{Samal et al 2004~: appariement de réseaux à des niveaux de détails proches}
%%http://www.tandfonline.com/doi/pdf/10.1080/13658810410001658076
%ça match plusieurs bases de données d'un coup, dont des rasters 
%Ca inclue le voisinage avec la ''similarité dépendante du contexte' (c'est à dire la distance et la direction des features qui entourent un objet : ça revient à construire des graphes en étoile qui décrivent la position et la distance relative des objets autour d'un objet choisi. Lorsque l'on calcule la similarité entre 2 objets, on compare alors également la ressemblance de leur graphe de proximité.
%Pour faire du matching de N DB, on calcule des 'similarity' sets 2 à 2, qui sont simplement des ensembles dont la similarité des éléments est élevée.
%On fusionne ces ensembles pour les N sources, ce qui donne un graphe. Finalement, on calcule la plus grande clique du graphe, ce qui donne le matching final.
%
%\subsubsection{Voltz 2006~: appariement de réseaux à des niveaux de détails proches}
%C'est du matching entre 2 bases de données.
%Processus iteratif : 
%\begin{enumerate}
%\item rubber sheeting pour réduire le décalage géométrique entre les couches de données (preprocessing)
%\item Découpage des objets pour faciliter le matching 1-1.
%\item ensuite matching 1-1 ou 1-2
%\end{enumerate}
%En fait, c'est assez proche de ce que fait Mustiere.
%C'est surtout completement ad-hoc... avec des seuils dans tous les sens et des traitements très spécifiques aux données considérées.\\
%Pas de NM non plus car Walter et Fritsch on dit que c'était trop dur (en terme de combinatoire) : du coup ils découpent pour décomposer le 1-1 en N:M.
%Cette façon de procéder pour éliminer les cas NM se retrouve chez Mustiere également. Est-ce que c'est problématique? Non si on ne s'appuie que sur les géométries, et oui potentiellement si les attributs servent aussi. Anéfé, en découpant on perd l'éventuelle ajout sémantique résultant de l'union de plusieurs objets (c'est hyper clair comme explication...).
%
%
%\subsubsection{Linna Li et Goodchild : matching de réseaux par optimisation}
%%Il y a 2 papiers : 
%%http://www.isprs.org/proceedings/xxxviii/part2/papers/51_paper.pdf
%%et 
%%http://www.geog.ucsb.edu/~good/papers/510.pdf
%Pas de NM car c'est inutile pour eux.
%Modèle par optimisation : prog linéaire
%Basé sur un critère de distance (haussdorff dirigé)
%Recalage par un modèle de déformation affine
%
%Problème : c'est assez long + ça n'est que sur 2 DB
%
%\subsubsection{W Yang : Pattern Based Feature Matching for Geospatial Data Conflation}
%%file:///home/BDumenieu/ntfs_drive/T%C3%A9l%C3%A9chargements/geoprocessing_2014_3_30_30146.pdf
%Recherche de mathcing de réseraux avec des patterns de forme (U, L, cul-de-sac, etc)
%On prend un ensemble de ''patterns de forme''. Il y a des patterns atomiques et des patterns composites.
%Pour chaque pattern identifié dans la DB 1, on cherche tous les patterns de DB2 qui sont proches du pattern, et on les trie par complexité croissante.
%On fait pareil dans le sesn DB2->DB1
%A la fin on retrouve les features simples qui matchent ensemble et on les projettent les unes sur les autres.
%
%\subsubsection{M Zhang : Delimited stroke oriented algorithm working principle and implementation for the matching of road networks}
%Pas lu
%
%
%\subsubsection{Gosseln and Sesters 2004 : Integration of Geoscientific data sets and the german digital map using a matching approach}
%C'est pour 2 bases surfaciques.
%
%\subsubsection{Daylot Sesters and al 2013 : Integrating network structures of different
%geometric representations}
%Intégration (matching?) de deux bases de données de réseaux avec des représentations différentes. En particulier, l'une surfacique et l'autre filaire.
%
%
%\subsection{Appariement de données incertaines}
%Lorsque les données à mettre en correspondance sont imparfaites, les méthodes classiques ne sont plus efficaces. Les théories de l'incertain offrent une base pour ça. En particulier, \cite{Olteanu2008} a utilisé la théorie de Dempster Shafer pour apparier des données géographiques
%\subsubsection{Ana Maria : matching avec fonctions de croyances}
%Un papier qui donne des algos pour réduire la complexité de dempster shafer : %http://ac.els-cdn.com/S0888613X97000133/1-s2.0-S0888613X97000133-main.pdf?_tid=fb8a8ef0-6f45-11e4-9f27-00000aacb35e&acdnat=1416330889_10fcf2eacf4f2ac8a70fe245585f681b
%
%\subsection{Passer d'un appariement entre 2 bases à un appariement entre N bases}
%
%Recomposer 'tout en un' la généalogie des objets historiques
%
%Pourquoi ne pas faire du 'l'un après l'autre' : 
%Les résurrections /différence de niveau de détails : une rue de A ne trouve pas d'équivalent dans B mais un dans C.
%Pour trouver ce cas, on doit faire A-B, B-C, A-C et ensuite filtrer. Soit 3 fois l'appariement donc une fois pour un seul objet.
%Si on le fait en direct, c'est plus rapide.
%
%Incohérences temporelles : les plans sont parfois incohérents entre eux 
%Il y a déjà le cas des rues représentées ''trop tôt'' -> ca on le détecte comme une différence de niveau de détails.
%Ensuite ça permet d'ordonner les plans et de définir leur temps valide.
%
%Enfin ça permet de recomposer la généalogie d'un objet ''en une fois''.
%   
%   
%(Li et Goodchild) : N:M ne sont pas très courants dans le cas des réseaux. Dans notre cas, on cherche des groupes d'objets qui correspondent, et non pas spécialement
%les correspondant de chaque arc. C a D que si on a le choix de créér un NM et que ça améliore la situation ou que ça ne change rien, alors on l'ajoute.
%Ca permet aussi d'avoir les 1:1 à la fin. AUtrement dit : des tronçons d'une même rue doivent être groupés.
%
%\subsection{Conclusion~: besoin d'un appariement adapté ST + complexité du problème}
%
%\section{Construction du graphe des transformations par optimisation stochastique}
%\subsection{Métaheuristiques pour l'optimisation}
%\subsection{Recuit simulé}
%\subsubsection{Mono-objectif vs Multi-Objectif}
%\subsection{Formulation énergétique}
%Énergie d'une transformation $g$ ~:
%\begin{equation}
%E(g) = \sum (1+X_g(a,b))(1+X_s(a,b)+X_t(a,b))
%\end{equation}
%Energie entrante d'un noeud déconnecté $a$:
%\begin{equation}
%E^-(a) =  inf \underline{} (E(b,a)) \text{si la relation b a est possible}
%\end{equation}
%Traduction : la plus petite énergie parmis tous les noeuds atteignables.
%Energie entrante d'un noeud déconnecté $a$:
%\begin{equation}
%E^+(a) =  E^-(a) =  inf(E(b,a)) \text{si la relation a b est possible}
%\end{equation}
%Traduction : la plus petite énergie parmis tous les noeuds atteignables.
%TODO : A RAJOUTER QQ PART comment on appelle l'ensemble des parties du graphe géohistorique.
%Energie d'un graphe $G_H$: 
%\begin{equation}
%E(G) = \sum \limits_{}^{g \in \mathcal{G}} E(g)\times \text{ordre}(g) + \sum \limits_{}^{a \in D^+} E^+(a)  + \sum \limits_{}^{a \in D^-} E^-(a)
%\end{equation}
%\myparagraph{L'énergie est relative à la seule grandeur non normalisée}
%Du coup c'est très pratique car il continue d'y avoir une relation de préférence meme avec des transformations éloignées.
%\myparagraph{Valeur plancher}
%Meme avec un critère à 0 on peut encore avoir un appariement qui a un sens. Le plus critique c'est la géométrie, mais avec cette énergie on a une une valeur plancher de 1 pour le critere géométrieque
%\myparagraph{Valeurs neutres}
%Si le critère vaut 0 alors il devient neutre, ce sont les autres qui se pronocent
%\myparagraph{Principe de l'énergie déconnecté}
%Avantage : on ne cherche en fait qu'a améliorer un matching 1-1. Du coup on peut se concentrer sur les cas plus complexes. Le fait de conserver les 1 1 à la fin dépend de la décision. Mais du coup on laisse un choix même dans le cas de données pourries.
%
%\subsection{Stratégie de décision}
%\myparagraph{Contrainte sup : bad is better than nothing}
%A énergie égale, on conserve la solution avec le plus d'arc
%
%\subsection{Sampling}
%
%\subsubsection{Critères d'appariement}
%\paragraph{Spatiaux}
%\paragraph{Sémantiques}
%\paragraph{Temporels}
%\subsubsection{Fonction objectif}
%
%\subsection{Généralisation d'un problème d'appariement géographique}
%On peut aussi faire de l'appariement non temporel avec l'algo proposé car c'est une généralisation d'un processus d'appariement. 
%\paragraph{Cas de deux bases de données atemporelles}
%\paragraph{Cas de n bases de données atemporelles}
%\paragraph{Cas de n bases de données sans incertitude temporelle}
%Le test effectif pour montrer que ça marche effectivement bien fait partie des perspectives.
%
%\subsection{Contraintes et réduction de l'espace de recherche}
%On ajoute des heuristiques pour accélérer le processus.
%\subsubsection{Constats sur la taille de l'espace de recherche}
%Voisinage spatio-temporel
%\subsubsection{Contraintes transformationelles}
%\'A terme, on pourrait ajouter des contraintes spatio-temporelles plus subtiles mais ce sera pour plus tard.
%
%\subsection{Algo}
%Là il faut reprendre le code STAIPE BAILLE STAIPE.
%
%\myparagraph{Mise à jour d'un arc}
%Vrif des contraintes 
%
%
%\section{Décision multicritère pour la construction du graphe spatio-temporel}
%
%\subsection{Décision multicritère en présence d'intertitude : Théorie des fonctions de croyance}
%\subsection{Détermination des types de filiations}
%Si on a déjà présenté le fait que c'était un problème de décision
%\subsubsection{Algorithme global}
%\subsubsection{Formulation dans le cadre de la théorie de Dempster Shafer}
%\subsubsection{Construction des fonctions de masse de croyance}
%\subsubsection{Résolution des conflits}
%\subsubsection{Résolution des conflits}
%

